#!/usr/bin/env python
"""
ç®€å•çš„NLPæ¨¡å—æµ‹è¯•
éªŒè¯åŸºç¡€NLPåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.abspath('.'))

def test_nlp_components():
    """æµ‹è¯•NLPç»„ä»¶"""
    print("ğŸ§  æµ‹è¯•NLPæ¨¡å—...")
    
    try:
        # å¯¼å…¥æ¨¡å—
        from app.core.schemas import QueryContextFactory
        from app.services.nlp import (
            LanguageDetector, 
            Tokenizer, 
            EntityExtractor, 
            IntentClassifier,
            NLPPipeline
        )
        print("âœ… NLPæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # åˆ›å»ºå„ä¸ªç»„ä»¶
        print("\nğŸ“ åˆå§‹åŒ–NLPç»„ä»¶...")
        language_detector = LanguageDetector()
        tokenizer = Tokenizer()
        entity_extractor = EntityExtractor() 
        intent_classifier = IntentClassifier()
        
        # åˆå§‹åŒ–ç»„ä»¶
        components = [
            ("è¯­è¨€æ£€æµ‹å™¨", language_detector),
            ("åˆ†è¯å™¨", tokenizer),
            ("å®ä½“æå–å™¨", entity_extractor),
            ("æ„å›¾åˆ†ç±»å™¨", intent_classifier)
        ]
        
        for name, component in components:
            print(f"åˆå§‹åŒ– {name}...")
            if component.initialize():
                print(f"âœ… {name} åˆå§‹åŒ–æˆåŠŸ")
            else:
                print(f"âŒ {name} åˆå§‹åŒ–å¤±è´¥")
                return
        
        # åˆ›å»ºNLPç®¡é“
        print("\nğŸ”„ åˆ›å»ºNLPå¤„ç†ç®¡é“...")
        nlp_pipeline = NLPPipeline(
            language_detector=language_detector,
            tokenizer=tokenizer,
            entity_extractor=entity_extractor,
            intent_classifier=intent_classifier
        )
        
        if nlp_pipeline.initialize():
            print("âœ… NLPç®¡é“åˆå§‹åŒ–æˆåŠŸ")
        else:
            print("âŒ NLPç®¡é“åˆå§‹åŒ–å¤±è´¥")
            return
        
        # æµ‹è¯•æŸ¥è¯¢
        test_query = "Who was in the Rockets, Kobe or Yao Ming? ?"
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: '{test_query}'")
        print("-" * 50)
        
        # åˆ›å»ºQueryContext
        context = QueryContextFactory.create(test_query)
        print(f"ğŸ“„ åŸå§‹æŸ¥è¯¢: {context.original_query}")
        
        # é€æ­¥å¤„ç†
        print("\nğŸŒ æ­¥éª¤1: è¯­è¨€æ£€æµ‹...")
        context = language_detector.process(context)
        if context.language_info:
            print(f"   æ£€æµ‹è¯­è¨€: {context.language_info.original_language}")
            print(f"   ç½®ä¿¡åº¦: {context.language_info.detected_confidence:.2f}")
        
        print("\nğŸ”¤ æ­¥éª¤2: åˆ†è¯...")
        context = tokenizer.process(context)
        if hasattr(context, 'tokens') and context.tokens:
            print(f"   åˆ†è¯ç»“æœ: {len(context.tokens)} ä¸ªtoken")
            
            # ğŸ†• æ˜¾ç¤ºæ‰€æœ‰é‡è¦tokenï¼ˆå‰5ä¸ª + æœ‰å®ä½“ç±»å‹çš„ï¼‰
            shown_indices = set()
            
            # æ˜¾ç¤ºå‰5ä¸ª
            for i, token in enumerate(context.tokens[:5]):
                print(f"   {i+1}. '{token.text}' ({token.pos}) [ent_type: '{token.ent_type}']")
                shown_indices.add(i)
            
            # æ˜¾ç¤ºå‰©ä½™çš„æœ‰å®ä½“ç±»å‹çš„token
            for i, token in enumerate(context.tokens[5:], 5):
                if token.ent_type or token.pos == "PROPN":
                    print(f"   {i+1}. '{token.text}' ({token.pos}) [ent_type: '{token.ent_type}']")
        
        print("\nğŸ‘¤ æ­¥éª¤3: å®ä½“æå–...")
        context = entity_extractor.process(context)
        if context.entity_info:
            print(f"   çƒå‘˜: {context.entity_info.players}")
            print(f"   é˜Ÿä¼: {context.entity_info.teams}")
            print(f"   æ‰€æœ‰ç›®æ ‡å®ä½“: {context.entity_info.target_entities}")
            print(f"   å‘åå…¼å®¹ç›®æ ‡: {context.entity_info.target_entity}")
        
        print("\nğŸ¯ æ­¥éª¤4: æ„å›¾åˆ†ç±»...")
        context = intent_classifier.process(context)
        if context.intent_info:
            print(f"   æ„å›¾: {context.intent_info.intent}")
            print(f"   ç½®ä¿¡åº¦: {context.intent_info.confidence:.2f}")
            print(f"   å±æ€§ç±»å‹: {context.intent_info.attribute_type}")
        
        print("\nğŸ”„ æµ‹è¯•å®Œæ•´ç®¡é“...")
        # é‡æ–°åˆ›å»ºcontextæµ‹è¯•å®Œæ•´ç®¡é“
        context2 = QueryContextFactory.create(test_query)
        result = nlp_pipeline.process(context2)
        
        print(f"âœ… ç®¡é“å¤„ç†å®Œæˆ!")
        print(f"   æœ€ç»ˆæ„å›¾: {result.intent_info.intent if result.intent_info else 'None'}")
        print(f"   ç›®æ ‡å®ä½“: {result.entity_info.target_entity if result.entity_info else 'None'}")
        
        print("\nğŸ‰ NLPæ¨¡å—æµ‹è¯•æˆåŠŸ!")
        
        # ğŸ†• æ¸…ç†èµ„æº
        print("\nğŸ”„ æ¸…ç†èµ„æº...")
        try:
            entity_extractor.close()  # å…³é—­NebulaGraphè¿æ¥
            print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ èµ„æºæ¸…ç†è­¦å‘Š: {e}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_nlp_components()
