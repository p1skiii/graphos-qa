"""
åˆ†è¯å™¨
å¯¹è‹±æ–‡æŸ¥è¯¢è¿›è¡Œåˆ†è¯ã€è¯æ€§æ ‡æ³¨
"""
import spacy
from typing import List, Dict, Any
from dataclasses import dataclass
from app.core.schemas import QueryContext
from .base_processor import BaseNLPProcessor
import logging

logger = logging.getLogger(__name__)

@dataclass
class Token:
    """åˆ†è¯ç»“æœ"""
    text: str           # åŸæ–‡
    lemma: str          # è¯æ ¹
    pos: str            # è¯æ€§
    tag: str            # è¯¦ç»†æ ‡ç­¾
    is_alpha: bool      # æ˜¯å¦ä¸ºå­—æ¯
    is_stop: bool       # æ˜¯å¦ä¸ºåœç”¨è¯
    is_punct: bool      # æ˜¯å¦ä¸ºæ ‡ç‚¹
    ent_type: str       # å®ä½“ç±»å‹ï¼ˆå¦‚æœæœ‰ï¼‰

class Tokenizer(BaseNLPProcessor):
    """è‹±æ–‡åˆ†è¯å™¨ - ä½¿ç”¨spaCyè¿›è¡Œåˆ†è¯å’Œè¯æ€§æ ‡æ³¨"""
    
    def __init__(self, model_name: str = "en_core_web_sm"):
        super().__init__("tokenizer")
        self.model_name = model_name
        self.nlp = None
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–spaCyæ¨¡å‹"""
        try:
            self.nlp = spacy.load(self.model_name)
            self.initialized = True
            logger.info(f"âœ… {self.name} åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {self.model_name})")
            return True
        except OSError as e:
            logger.error(f"âŒ {self.name} åˆå§‹åŒ–å¤±è´¥: æ‰¾ä¸åˆ°æ¨¡å‹ {self.model_name}")
            logger.error(f"è¯·è¿è¡Œ: python -m spacy download {self.model_name}")
            return False
        except Exception as e:
            logger.error(f"âŒ {self.name} åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def process(self, context: QueryContext) -> QueryContext:
        """
        åˆ†è¯å¹¶å¡«å……tokenså­—æ®µï¼Œåˆå¹¶å‘½åå®ä½“
        
        Args:
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡
            
        Returns:
            QueryContext: å¡«å……äº†tokensçš„ä¸Šä¸‹æ–‡
        """
        self._add_trace(context, "start_tokenization")
        
        try:
            # è·å–è¦å¤„ç†çš„æ–‡æœ¬ï¼ˆä¼˜å…ˆä½¿ç”¨ç¿»è¯‘åçš„ï¼Œå¦åˆ™ä½¿ç”¨åŸæ–‡ï¼‰
            text = getattr(context, 'translated_query', None) or context.original_query
            
            # ä½¿ç”¨spaCyå¤„ç†
            doc = self.nlp(text)
            
            # ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºåŸºç¡€tokenåˆ—è¡¨
            base_tokens = []
            for token in doc:
                base_tokens.append(Token(
                    text=token.text,
                    lemma=token.lemma_,
                    pos=token.pos_,
                    tag=token.tag_,
                    is_alpha=token.is_alpha,
                    is_stop=token.is_stop,
                    is_punct=token.is_punct,
                    ent_type=token.ent_type_ if token.ent_type_ else ""
                ))
            
            # ç¬¬äºŒæ­¥ï¼šæ™ºèƒ½åˆå¹¶å‘½åå®ä½“
            merged_tokens = self._merge_named_entities(doc, base_tokens)
            
            # æ·»åŠ åˆ°context
            context.tokens = merged_tokens
            
            # æå–ä¸€äº›æœ‰ç”¨çš„ç»Ÿè®¡ä¿¡æ¯
            content_tokens = [t for t in merged_tokens if t.is_alpha and not t.is_stop]
            question_words = [t for t in merged_tokens if t.lemma.lower() in ['who', 'what', 'when', 'where', 'why', 'how']]
            person_entities = [t for t in merged_tokens if t.ent_type == 'PERSON']
            
            # æ·»åŠ è¿½è¸ªä¿¡æ¯
            self._add_trace(context, "tokenization_complete", {
                "total_tokens": len(merged_tokens),
                "content_tokens": len(content_tokens),
                "question_words": [t.text for t in question_words],
                "person_entities": [t.text for t in person_entities],
                "text_processed": text[:100] + "..." if len(text) > 100 else text
            })
            
            logger.debug(f"ğŸ”¤ åˆ†è¯å®Œæˆ: {len(merged_tokens)} tokens, {len(content_tokens)} content tokens, {len(person_entities)} person entities")
            
        except Exception as e:
            logger.error(f"âŒ åˆ†è¯å¤±è´¥: {e}")
            context.tokens = []
            self._add_trace(context, "tokenization_error", {"error": str(e)})
        
        return context
    
    def _merge_named_entities(self, doc, base_tokens: List[Token]) -> List[Token]:
        """
        æ™ºèƒ½åˆå¹¶spaCyè¯†åˆ«çš„å‘½åå®ä½“ï¼Œç‰¹åˆ«æ˜¯äººåå’Œç»„ç»‡å
        
        Args:
            doc: spaCyæ–‡æ¡£å¯¹è±¡
            base_tokens: åŸºç¡€tokenåˆ—è¡¨
            
        Returns:
            List[Token]: åˆå¹¶åçš„tokenåˆ—è¡¨
        """
        if not doc.ents:
            return base_tokens
        
        merged_tokens = []
        processed_positions = set()
        
        # å¤„ç†æ‰€æœ‰å®ä½“ï¼Œåˆ›å»ºå®ä½“æ˜ å°„
        entity_map = {}
        for ent in doc.ents:
            # åªåˆå¹¶äººå(PERSON)å’Œç»„ç»‡(ORG)å®ä½“ï¼Œè¿‡æ»¤æ‰é”™è¯¯çš„è¯†åˆ«
            if ent.label_ in ["PERSON", "ORG"]:
                # æ¸…ç†å®ä½“æ–‡æœ¬ï¼Œç§»é™¤ä¸åˆç†çš„å‰ç¼€
                clean_text = self._clean_entity_text(ent.text, ent.label_)
                if clean_text:
                    entity_map[ent.start] = {
                        'end': ent.end,
                        'text': clean_text,
                        'label': ent.label_
                    }
                    # æ ‡è®°æ‰€æœ‰è¢«åˆå¹¶çš„ä½ç½®
                    for i in range(ent.start, ent.end):
                        processed_positions.add(i)
        
        # é‡å»ºtokenåˆ—è¡¨
        i = 0
        while i < len(base_tokens):
            if i in entity_map:
                # åˆ›å»ºåˆå¹¶çš„å®ä½“token
                entity_info = entity_map[i]
                merged_token = Token(
                    text=entity_info['text'],
                    lemma=entity_info['text'].lower(),
                    pos="PROPN",  # ä¸“æœ‰åè¯
                    tag="NNP",    # ä¸“æœ‰åè¯å•æ•°
                    is_alpha=True,
                    is_stop=False,
                    is_punct=False,
                    ent_type=entity_info['label']
                )
                merged_tokens.append(merged_token)
                # è·³åˆ°å®ä½“ç»“æŸä½ç½®
                i = entity_info['end']
            elif i not in processed_positions:
                # æ·»åŠ æ™®é€štoken
                merged_tokens.append(base_tokens[i])
                i += 1
            else:
                # è·³è¿‡è¢«åˆå¹¶çš„token
                i += 1
        
        return merged_tokens
    
    def _clean_entity_text(self, text: str, label: str) -> str:
        """
        æ¸…ç†å®ä½“æ–‡æœ¬ï¼Œç§»é™¤ä¸åˆç†çš„éƒ¨åˆ†
        
        Args:
            text: åŸå§‹å®ä½“æ–‡æœ¬
            label: å®ä½“æ ‡ç­¾
            
        Returns:
            str: æ¸…ç†åçš„æ–‡æœ¬ï¼Œå¦‚æœä¸åˆç†åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        text = text.strip()
        
        if label == "PERSON":
            # å¯¹äºäººåï¼Œç§»é™¤åŠ¨è¯å‰ç¼€
            unwanted_prefixes = ["compare", "tell", "show", "find"]
            text_lower = text.lower()
            
            for prefix in unwanted_prefixes:
                if text_lower.startswith(prefix + " "):
                    text = text[len(prefix):].strip()
                    break
            
            # æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯åˆç†çš„äººåï¼ˆè‡³å°‘åŒ…å«ä¸€ä¸ªå¤§å†™å­—æ¯å¼€å¤´çš„å•è¯ï¼‰
            words = text.split()
            valid_words = [w for w in words if w and w[0].isupper() and w.isalpha()]
            if len(valid_words) >= 1:
                return " ".join(valid_words)
            else:
                return ""
        
        elif label == "ORG":
            # å¯¹äºç»„ç»‡åï¼Œç®€å•æ¸…ç†
            if text and any(c.isupper() for c in text):
                return text
            else:
                return ""
        
        return text