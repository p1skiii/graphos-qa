"""
GNNå¤„ç†å™¨ (GNN Processor)
è´Ÿè´£ç¼–æ’æ•´ä¸ªGNNå¤„ç†æµç¨‹ï¼š
1. è°ƒç”¨æ£€ç´¢å™¨è·å–ç§å­èŠ‚ç‚¹
2. è°ƒç”¨GNNæ•°æ®æ„å»ºå™¨å‡†å¤‡torch_geometricæ•°æ®
3. è°ƒç”¨GNNæ¨¡å‹è¿›è¡Œæ¨ç†
4. å¤„ç†ç»“æœå¹¶è¿”å›
"""
import torch
import logging
from typing import Dict, Any, List, Optional
from app.rag.components import component_factory, ProcessorConfig, ComponentConfig, ProcessorDefaultConfigs
from app.rag.processors.base_processor import BaseProcessor

try:
    import torch.nn.functional as F
    from torch_geometric.nn import GCNConv, global_mean_pool
    HAS_TORCH_GEOMETRIC = True
except ImportError:
    HAS_TORCH_GEOMETRIC = False

logger = logging.getLogger(__name__)

class SimpleGNN(torch.nn.Module):
    """ç®€å•çš„GNNæ¨¡å‹ç”¨äºæ¼”ç¤º"""
    
    def __init__(self, input_dim: int = 768, hidden_dim: int = 256, output_dim: int = 128):
        super(SimpleGNN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim) if HAS_TORCH_GEOMETRIC else None
        self.conv2 = GCNConv(hidden_dim, output_dim) if HAS_TORCH_GEOMETRIC else None
        self.dropout = torch.nn.Dropout(0.2)
        
    def forward(self, data):
        if not HAS_TORCH_GEOMETRIC:
            # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥è¿”å›èŠ‚ç‚¹ç‰¹å¾çš„å‡å€¼
            return torch.mean(data.x, dim=1) if data.x.size(0) > 0 else torch.zeros(128)
        
        x, edge_index = data.x, data.edge_index
        
        # ç¬¬ä¸€å±‚GCN
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        
        # ç¬¬äºŒå±‚GCN
        x = self.conv2(x, edge_index)
        
        # å…¨å±€æ± åŒ–ï¼ˆå›¾çº§åˆ«è¡¨ç¤ºï¼‰
        batch = torch.zeros(x.size(0), dtype=torch.long)  # å•å›¾æƒ…å†µ
        graph_embedding = global_mean_pool(x, batch)
        
        return graph_embedding

class GNNProcessor(BaseProcessor):
    """GNNå¤„ç†å™¨ - ç¼–æ’GNNå¤„ç†æµç¨‹"""
    
    def __init__(self, config: Optional[ProcessorConfig] = None):
        """åˆå§‹åŒ–GNNå¤„ç†å™¨"""
        if config is None:
            config = ProcessorDefaultConfigs.get_gnn_processor_config()
        
        super().__init__(config)
        
        # GNNç‰¹æœ‰é…ç½® - ä»é»˜è®¤é…ç½®è·å–
        self.model_config = self._get_default_model_config()
        
        # GNNæ¨¡å‹
        self.gnn_model = None
        
        if not HAS_TORCH_GEOMETRIC:
            logger.warning("âš ï¸ torch_geometricæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆGNNæ¨¡å‹")
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–GNNå¤„ç†å™¨"""
        try:
            # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
            if not super().initialize():
                return False
            
            logger.info("ğŸ”„ åˆå§‹åŒ–GNNç‰¹æœ‰ç»„ä»¶...")
            
            # åˆ›å»ºGNNæ¨¡å‹
            self.gnn_model = SimpleGNN(
                input_dim=self.model_config['input_dim'],
                hidden_dim=self.model_config['hidden_dim'],
                output_dim=self.model_config['output_dim']
            )
            self.gnn_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            
            logger.info("âœ… GNNå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ GNNå¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def _process_impl(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """å…·ä½“å¤„ç†é€»è¾‘"""
        logger.info(f"ğŸ§  GNNå¤„ç†å™¨å¼€å§‹å¤„ç†æŸ¥è¯¢: {query[:50]}...")
        
        try:
            # 1. æ£€ç´¢ç§å­èŠ‚ç‚¹
            logger.info("ğŸ” æ­¥éª¤1: æ£€ç´¢ç§å­èŠ‚ç‚¹")
            retrieval_result = self.retriever.retrieve(query, top_k=5)
            seed_nodes = [item['node_id'] for item in retrieval_result]
            
            if not seed_nodes:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ç§å­èŠ‚ç‚¹")
                return self._create_empty_result(query)
            
            logger.info(f"âœ… æ‰¾åˆ° {len(seed_nodes)} ä¸ªç§å­èŠ‚ç‚¹")
            
            # 2. æ„å»ºGNNæ•°æ® - ä½¿ç”¨GNNæ•°æ®æ„å»ºå™¨
            logger.info("ğŸ—ï¸ æ­¥éª¤2: æ„å»ºGNNæ•°æ®")
            gnn_data_result = self.graph_builder.build_subgraph(seed_nodes, query)
            
            if gnn_data_result['num_nodes'] == 0:
                logger.warning("âš ï¸ æ„å»ºçš„å­å›¾ä¸ºç©º")
                return self._create_empty_result(query)
            
            logger.info(f"âœ… æ„å»ºGNNæ•°æ®å®Œæˆï¼ŒèŠ‚ç‚¹: {gnn_data_result['num_nodes']}, è¾¹: {gnn_data_result['num_edges']}")
            
            # 3. GNNæ¨¡å‹æ¨ç†
            logger.info("ğŸ§  æ­¥éª¤3: GNNæ¨¡å‹æ¨ç†")
            graph_embedding = self._run_gnn_inference(gnn_data_result['data'])
            
            # 4. å¤„ç†ç»“æœ
            result = self._process_gnn_output(
                graph_embedding, 
                gnn_data_result, 
                retrieval_result, 
                query
            )
            
            logger.info("âœ… GNNå¤„ç†å®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ GNNå¤„ç†å¤±è´¥: {str(e)}")
            return self._create_error_result(query, str(e))
    
    def _run_gnn_inference(self, data) -> torch.Tensor:
        """è¿è¡ŒGNNæ¨ç†"""
        with torch.no_grad():
            embedding = self.gnn_model(data)
            return embedding
    
    def _process_gnn_output(self, graph_embedding: torch.Tensor, 
                          gnn_data_result: Dict[str, Any],
                          retrieval_result: List[Dict[str, Any]], 
                          query: str) -> Dict[str, Any]:
        """å¤„ç†GNNè¾“å‡º"""
        
        # è½¬æ¢embeddingä¸ºåˆ—è¡¨ï¼ˆç”¨äºJSONåºåˆ—åŒ–ï¼‰
        embedding_list = graph_embedding.squeeze().tolist()
        
        # æ„å»ºèŠ‚ç‚¹ä¿¡æ¯
        nodes_info = []
        for i, node_id in enumerate(gnn_data_result['node_ids']):
            node_info = {
                'node_id': node_id,
                'index': i,
                'type': node_id.split(':')[0] if ':' in node_id else 'unknown'
            }
            
            # æ·»åŠ æ£€ç´¢ç›¸å…³ä¿¡æ¯
            for item in retrieval_result:
                if item['node_id'] == node_id:
                    node_info.update({
                        'similarity_score': item.get('similarity_score', 0.0),
                        'description': item.get('description', ''),
                        'is_seed': True
                    })
                    break
            else:
                node_info['is_seed'] = False
            
            nodes_info.append(node_info)
        
        return {
            'success': True,
            'query': query,
            'processing_type': 'gnn',
            'graph_embedding': embedding_list,
            'embedding_dim': len(embedding_list),
            'subgraph_info': {
                'num_nodes': gnn_data_result['num_nodes'],
                'num_edges': gnn_data_result['num_edges'],
                'feature_dim': gnn_data_result['feature_dim'],
                'nodes': nodes_info
            },
            'seed_nodes': [item['node_id'] for item in retrieval_result],
            'metadata': {
                'retriever_type': self.config.retriever_config.component_name,
                'gnn_builder_type': self.config.graph_builder_config.component_name,
                'model_type': 'SimpleGNN'
            }
        }
    
    def _create_empty_result(self, query: str) -> Dict[str, Any]:
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            'success': True,
            'query': query,
            'processing_type': 'gnn',
            'graph_embedding': [],
            'embedding_dim': 0,
            'subgraph_info': {
                'num_nodes': 0,
                'num_edges': 0,
                'feature_dim': 0,
                'nodes': []
            },
            'seed_nodes': [],
            'metadata': {
                'message': 'No relevant data found'
            }
        }
    
    def _create_error_result(self, query: str, error_msg: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        return {
            'success': False,
            'query': query,
            'processing_type': 'gnn',
            'error': error_msg,
            'graph_embedding': [],
            'embedding_dim': 0,
            'subgraph_info': {
                'num_nodes': 0,
                'num_edges': 0,
                'feature_dim': 0,
                'nodes': []
            },
            'seed_nodes': [],
            'metadata': {}
        }
    
    def _get_default_model_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤æ¨¡å‹é…ç½®"""
        return {
            "input_dim": 768,
            "hidden_dim": 256,
            "output_dim": 128
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = super().get_stats()
        stats.update({
            'model_params': {
                'input_dim': self.model_config.get('input_dim', 0),
                'hidden_dim': self.model_config.get('hidden_dim', 0),
                'output_dim': self.model_config.get('output_dim', 0)
            },
            'torch_geometric_available': HAS_TORCH_GEOMETRIC
        })
        return stats

# =============================================================================
# å·¥å‚å‡½æ•°
# =============================================================================

def create_gnn_processor(custom_config: Optional[Dict[str, Any]] = None) -> GNNProcessor:
    """åˆ›å»ºGNNå¤„ç†å™¨å®ä¾‹"""
    if custom_config:
        config = ProcessorDefaultConfigs.get_gnn_processor_config()
        
        # æ›´æ–°é…ç½®
        if 'retriever' in custom_config:
            config.retriever_config.config.update(custom_config['retriever'])
        if 'graph_builder' in custom_config:
            config.graph_builder_config.config.update(custom_config['graph_builder'])
        if 'textualizer' in custom_config:
            config.textualizer_config.config.update(custom_config['textualizer'])
        
        for key in ['cache_enabled', 'cache_ttl', 'max_tokens']:
            if key in custom_config:
                setattr(config, key, custom_config[key])
        
        return GNNProcessor(config)
    else:
        return GNNProcessor()

# =============================================================================
# æ³¨å†ŒGNNå¤„ç†å™¨
# =============================================================================

def register_gnn_processor():
    """æ³¨å†ŒGNNå¤„ç†å™¨åˆ°å·¥å‚"""
    try:
        component_factory.register_processor('gnn', GNNProcessor)
        logger.info("âœ… GNNå¤„ç†å™¨å·²æ³¨å†Œåˆ°ç»„ä»¶å·¥å‚")
        
    except Exception as e:
        logger.error(f"âŒ GNNå¤„ç†å™¨æ³¨å†Œå¤±è´¥: {str(e)}")

# è‡ªåŠ¨æ³¨å†Œ
register_gnn_processor()
