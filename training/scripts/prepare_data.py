#!/usr/bin/env python3
"""
æ•°æ®é¢„å¤„ç†è„šæœ¬
å°†åŸå§‹CSVæ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
"""
import os
import sys
import yaml
import json
import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split
from typing import Dict, Tuple, List
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataPreprocessor:
    def __init__(self, data_config_path: str, bert_config_path: str):
        """åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨"""
        with open(data_config_path, 'r') as f:
            self.data_config = yaml.safe_load(f)
        
        with open(bert_config_path, 'r') as f:
            self.bert_config = yaml.safe_load(f)
        
        self.raw_data_path = Path(self.data_config['data_sources']['raw_data_path'])
        self.processed_train_path = Path(self.data_config['data_sources']['processed_train_path'])
        self.processed_val_path = Path(self.data_config['data_sources']['processed_val_path'])
        
    def load_raw_data(self) -> pd.DataFrame:
        """åŠ è½½åŸå§‹æ•°æ®"""
        logger.info(f"ğŸ“‚ åŠ è½½åŸå§‹æ•°æ®: {self.raw_data_path}")
        
        if not self.raw_data_path.exists():
            raise FileNotFoundError(f"åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.raw_data_path}")
        
        df = pd.read_csv(self.raw_data_path)
        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
        logger.info(f"ğŸ“Š æ•°æ®åˆ—: {list(df.columns)}")
        
        return df
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æ•°æ®"""
        logger.info("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        
        # è·å–é…ç½®
        text_col = self.data_config['field_mapping']['text_column']
        label_col = self.data_config['field_mapping']['label_column']
        preprocessing = self.data_config['preprocessing']
        
        initial_size = len(df)
        
        # åˆ é™¤ç©ºå€¼
        df = df.dropna(subset=[text_col, label_col])
        logger.info(f"ğŸ“Š åˆ é™¤ç©ºå€¼å: {len(df)} è¡Œ (åˆ é™¤äº† {initial_size - len(df)} è¡Œ)")
        
        # æ–‡æœ¬é•¿åº¦è¿‡æ»¤
        min_len = preprocessing['min_text_length']
        max_len = preprocessing['max_text_length']
        df = df[df[text_col].str.len().between(min_len, max_len)]
        logger.info(f"ğŸ“Š æ–‡æœ¬é•¿åº¦è¿‡æ»¤å: {len(df)} è¡Œ")
        
        # åˆ é™¤é‡å¤é¡¹
        if preprocessing['remove_duplicates']:
            df = df.drop_duplicates(subset=[text_col])
            logger.info(f"ğŸ“Š åˆ é™¤é‡å¤é¡¹å: {len(df)} è¡Œ")
        
        return df
    
    def map_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ˜ å°„æ ‡ç­¾"""
        logger.info("ğŸ·ï¸ å¼€å§‹æ ‡ç­¾æ˜ å°„...")
        
        label_col = self.data_config['field_mapping']['label_column']
        label_mapping = self.data_config['label_mapping']
        bert_labels = self.bert_config['labels']
        
        # åº”ç”¨æ ‡ç­¾æ˜ å°„
        df['mapped_label'] = df[label_col].map(label_mapping)
        
        # æ£€æŸ¥æœªæ˜ å°„çš„æ ‡ç­¾
        unmapped = df[df['mapped_label'].isna()]
        if len(unmapped) > 0:
            logger.warning(f"âš ï¸ å‘ç° {len(unmapped)} ä¸ªæœªæ˜ å°„çš„æ ‡ç­¾:")
            logger.warning(f"   {unmapped[label_col].unique()}")
            # å°†æœªæ˜ å°„çš„æ ‡ç­¾è®¾ä¸ºOUT_OF_DOMAIN
            df['mapped_label'] = df['mapped_label'].fillna('OUT_OF_DOMAIN')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        df['label_id'] = df['mapped_label'].map(bert_labels)
        
        # æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡
        label_counts = df['mapped_label'].value_counts()
        logger.info("ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
        for label, count in label_counts.items():
            logger.info(f"   {label}: {count} ({count/len(df)*100:.1f}%)")
        
        return df
    
    def split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""
        logger.info("âœ‚ï¸ åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®...")
        
        train_split = self.bert_config['data']['train_split']
        random_seed = self.bert_config['data']['random_seed']
        
        train_df, val_df = train_test_split(
            df,
            test_size=1-train_split,
            stratify=df['label_id'],
            random_state=random_seed
        )
        
        logger.info(f"ğŸ“Š è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬")
        logger.info(f"ğŸ“Š éªŒè¯é›†: {len(val_df)} æ ·æœ¬")
        
        return train_df, val_df
    
    def save_processed_data(self, train_df: pd.DataFrame, val_df: pd.DataFrame):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        logger.info("ğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
        
        text_col = self.data_config['field_mapping']['text_column']
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.processed_train_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        train_data = []
        for _, row in train_df.iterrows():
            train_data.append({
                "text": row[text_col],
                "label": row['mapped_label'],
                "label_id": row['label_id']
            })
        
        # å‡†å¤‡éªŒè¯æ•°æ®
        val_data = []
        for _, row in val_df.iterrows():
            val_data.append({
                "text": row[text_col],
                "label": row['mapped_label'], 
                "label_id": row['label_id']
            })
        
        # ä¿å­˜ä¸ºJSON
        with open(self.processed_train_path, 'w', encoding='utf-8') as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)
        
        with open(self.processed_val_path, 'w', encoding='utf-8') as f:
            json.dump(val_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… è®­ç»ƒæ•°æ®ä¿å­˜åˆ°: {self.processed_train_path}")
        logger.info(f"âœ… éªŒè¯æ•°æ®ä¿å­˜åˆ°: {self.processed_val_path}")
    
    def process(self):
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æ•°æ®é¢„å¤„ç†æµç¨‹...")
        
        try:
            # 1. åŠ è½½åŸå§‹æ•°æ®
            df = self.load_raw_data()
            
            # 2. æ¸…æ´—æ•°æ®
            df = self.clean_data(df)
            
            # 3. æ˜ å°„æ ‡ç­¾
            df = self.map_labels(df)
            
            # 4. åˆ†å‰²æ•°æ®
            train_df, val_df = self.split_data(df)
            
            # 5. ä¿å­˜æ•°æ®
            self.save_processed_data(train_df, val_df)
            
            logger.info("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 3:
        print("ç”¨æ³•: python prepare_data.py <data_config.yaml> <bert_config.yaml>")
        sys.exit(1)
    
    data_config_path = sys.argv[1]
    bert_config_path = sys.argv[2]
    
    preprocessor = DataPreprocessor(data_config_path, bert_config_path)
    preprocessor.process()

if __name__ == "__main__":
    main()
