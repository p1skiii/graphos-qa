"""
æ¨¡å‹è¯„ä¼°å™¨ - è¯¦ç»†è¯„ä¼°è·¯ç”±ç³»ç»Ÿçš„æ€§èƒ½
"""
import time
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple, Any
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support,
    confusion_matrix, classification_report
)
import logging

from .intelligent_router import intelligent_router
from .training_data_generator import training_data_generator

logger = logging.getLogger(__name__)

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.router = intelligent_router
        self.data_generator = training_data_generator
        
        # è¯„ä¼°ç»“æœç›®å½•
        self.results_dir = Path("results/evaluations")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # å¯è§†åŒ–é…ç½®
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        logger.info("ğŸ“Š æ¨¡å‹è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_test_dataset(self) -> List[Dict]:
        """åˆ›å»ºæµ‹è¯•æ•°æ®é›†"""
        
        logger.info("ğŸ§ª åˆ›å»ºæµ‹è¯•æ•°æ®é›†...")
        
        test_cases = [
            # === ç®€å•æŸ¥è¯¢æµ‹è¯•ç”¨ä¾‹ ===
            {"text": "å§šæ˜å¤šå°‘å²ï¼Ÿ", "expected_intent": "simple", "expected_processor": "direct"},
            {"text": "ç§‘æ¯”èº«é«˜æ˜¯å¤šå°‘ï¼Ÿ", "expected_intent": "simple", "expected_processor": "direct"},
            {"text": "è©¹å§†æ–¯åœ¨å“ªä¸ªé˜Ÿï¼Ÿ", "expected_intent": "simple", "expected_processor": "direct"},
            {"text": "æ¹–äººé˜Ÿåœ¨å“ªä¸ªåŸå¸‚ï¼Ÿ", "expected_intent": "simple", "expected_processor": "direct"},
            {"text": "ç§‘æ¯”å‡ å·çƒè¡£ï¼Ÿ", "expected_intent": "simple", "expected_processor": "direct"},
            {"text": "å§šæ˜ä»€ä¹ˆæ—¶å€™é€€å½¹ï¼Ÿ", "expected_intent": "simple", "expected_processor": "direct"},
            {"text": "é‚“è‚¯æ˜¯å“ªå›½äººï¼Ÿ", "expected_intent": "simple", "expected_processor": "direct"},
            {"text": "Yao Ming height?", "expected_intent": "simple", "expected_processor": "direct"},
            {"text": "Kobe age?", "expected_intent": "simple", "expected_processor": "direct"},
            {"text": "LeBron team?", "expected_intent": "simple", "expected_processor": "direct"},
            
            # === å¤æ‚æŸ¥è¯¢æµ‹è¯•ç”¨ä¾‹ ===
            {"text": "å§šæ˜å’Œç§‘æ¯”ä»€ä¹ˆå…³ç³»ï¼Ÿ", "expected_intent": "complex", "expected_processor": "g_retriever"},
            {"text": "ç§‘æ¯”å’Œè©¹å§†æ–¯è°æ›´å¼ºï¼Ÿ", "expected_intent": "complex", "expected_processor": "g_retriever"},
            {"text": "æ¹–äººé˜Ÿæœ‰å“ªäº›è‘—åçƒå‘˜ï¼Ÿ", "expected_intent": "complex", "expected_processor": "g_retriever"},
            {"text": "åˆ†æç«ç®­é˜Ÿçš„å†å²æˆå°±", "expected_intent": "complex", "expected_processor": "g_retriever"},
            {"text": "ä¸ºä»€ä¹ˆç§‘æ¯”è¢«ç§°ä¸ºé»‘æ›¼å·´ï¼Ÿ", "expected_intent": "complex", "expected_processor": "g_retriever"},
            {"text": "å§šæ˜å¯¹ä¸­å›½ç¯®çƒçš„å½±å“", "expected_intent": "complex", "expected_processor": "g_retriever"},
            {"text": "é€šè¿‡ä»€ä¹ˆè·¯å¾„è¿æ¥å§šæ˜å’Œè©¹å§†æ–¯ï¼Ÿ", "expected_intent": "complex", "expected_processor": "g_retriever"},
            {"text": "æ¯”è¾ƒä¸œè¥¿éƒ¨çƒé˜Ÿå®åŠ›", "expected_intent": "complex", "expected_processor": "g_retriever"},
            {"text": "å¦‚æœç§‘æ¯”å’Œè©¹å§†æ–¯åœ¨åŒä¸€é˜Ÿä¼šæ€æ ·ï¼Ÿ", "expected_intent": "complex", "expected_processor": "g_retriever"},
            {"text": "NBAå†å²ä¸Šæœ€ä¼Ÿå¤§çš„çƒå‘˜æ˜¯è°ï¼Ÿ", "expected_intent": "complex", "expected_processor": "g_retriever"},
            
            # === éç¯®çƒæŸ¥è¯¢æµ‹è¯•ç”¨ä¾‹ ===
            {"text": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", "expected_intent": "non_basketball", "expected_processor": "fallback"},
            {"text": "ä½ å¥½å—ï¼Ÿ", "expected_intent": "non_basketball", "expected_processor": "fallback"},
            {"text": "æ¢…è¥¿è¸¢çƒæ€ä¹ˆæ ·ï¼Ÿ", "expected_intent": "non_basketball", "expected_processor": "fallback"},
            {"text": "æ¨èä¸€éƒ¨ç”µå½±", "expected_intent": "non_basketball", "expected_processor": "fallback"},
            {"text": "Pythonæ€ä¹ˆå­¦ï¼Ÿ", "expected_intent": "non_basketball", "expected_processor": "fallback"},
            {"text": "ä»Šå¤©è‚¡å¸‚å¦‚ä½•ï¼Ÿ", "expected_intent": "non_basketball", "expected_processor": "fallback"},
            {"text": "how is the weather?", "expected_intent": "non_basketball", "expected_processor": "fallback"},
            {"text": "what's your name?", "expected_intent": "non_basketball", "expected_processor": "fallback"},
            
            # === è¾¹ç•Œæƒ…å†µæµ‹è¯• ===
            {"text": "å§šæ˜", "expected_intent": "non_basketball", "expected_processor": "fallback"},  # å¤ªçŸ­
            {"text": "ç¯®çƒ", "expected_intent": "simple", "expected_processor": "direct"},  # å•è¯
            {"text": "NBA", "expected_intent": "simple", "expected_processor": "direct"},  # ç¼©å†™
            {"text": "ç§‘æ¯”å’Œæ¢…è¥¿è°æ›´å‰å®³ï¼Ÿ", "expected_intent": "non_basketball", "expected_processor": "fallback"},  # æ··åˆè¿åŠ¨
        ]
        
        logger.info(f"âœ… æµ‹è¯•æ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œå…± {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return test_cases
    
    def evaluate_routing_accuracy(self) -> Dict:
        """è¯„ä¼°è·¯ç”±å‡†ç¡®æ€§"""
        
        logger.info("ğŸ¯ å¼€å§‹è¯„ä¼°è·¯ç”±å‡†ç¡®æ€§...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        test_cases = self.create_test_dataset()
        
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.router.reset_stats()
        
        # æ‰§è¡Œè¯„ä¼°
        results = []
        correct_intent = 0
        correct_processor = 0
        correct_overall = 0
        
        for case in test_cases:
            # æ‰§è¡Œè·¯ç”±
            result = self.router.route_query(case['text'])
            
            # æ£€æŸ¥å‡†ç¡®æ€§
            intent_correct = result['intent'] == case['expected_intent']
            processor_correct = result['processor'] == case['expected_processor']
            overall_correct = intent_correct and processor_correct
            
            if intent_correct:
                correct_intent += 1
            if processor_correct:
                correct_processor += 1
            if overall_correct:
                correct_overall += 1
            
            results.append({
                'text': case['text'],
                'expected_intent': case['expected_intent'],
                'predicted_intent': result['intent'],
                'expected_processor': case['expected_processor'],
                'predicted_processor': result['processor'],
                'confidence': result['confidence'],
                'intent_correct': intent_correct,
                'processor_correct': processor_correct,
                'overall_correct': overall_correct,
                'filter_time_ms': result['filter_time'] * 1000,
                'bert_time_ms': result.get('bert_time', 0) * 1000,
                'total_time_ms': (result['filter_time'] + result.get('bert_time', 0)) * 1000
            })
        
        # è®¡ç®—å‡†ç¡®ç‡
        total_cases = len(test_cases)
        intent_accuracy = correct_intent / total_cases
        processor_accuracy = correct_processor / total_cases
        overall_accuracy = correct_overall / total_cases
        
        # æ„å»ºè¯„ä¼°ç»“æœ
        evaluation_result = {
            'intent_accuracy': intent_accuracy,
            'processor_accuracy': processor_accuracy,
            'overall_accuracy': overall_accuracy,
            'correct_intent': correct_intent,
            'correct_processor': correct_processor,
            'correct_overall': correct_overall,
            'total_cases': total_cases,
            'detailed_results': results,
            'timestamp': time.time()
        }
        
        # ä¿å­˜ç»“æœ
        results_path = self.results_dir / "routing_accuracy_evaluation.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š è·¯ç”±å‡†ç¡®æ€§è¯„ä¼°å®Œæˆ:")
        logger.info(f"   æ„å›¾å‡†ç¡®ç‡: {intent_accuracy:.3f} ({correct_intent}/{total_cases})")
        logger.info(f"   å¤„ç†å™¨å‡†ç¡®ç‡: {processor_accuracy:.3f} ({correct_processor}/{total_cases})")
        logger.info(f"   æ€»ä½“å‡†ç¡®ç‡: {overall_accuracy:.3f} ({correct_overall}/{total_cases})")
        logger.info(f"   ç»“æœå·²ä¿å­˜: {results_path}")
        
        return evaluation_result
    
    def evaluate_performance(self) -> Dict:
        """è¯„ä¼°æ€§èƒ½æŒ‡æ ‡"""
        
        logger.info("âš¡ å¼€å§‹è¯„ä¼°æ€§èƒ½æŒ‡æ ‡...")
        
        # åˆ›å»ºæ€§èƒ½æµ‹è¯•ç”¨ä¾‹
        test_queries = [
            "å§šæ˜å¤šå°‘å²ï¼Ÿ",
            "ç§‘æ¯”å’Œè©¹å§†æ–¯ä»€ä¹ˆå…³ç³»ï¼Ÿ",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "æ¹–äººé˜Ÿåœ¨å“ªä¸ªåŸå¸‚ï¼Ÿ",
            "åˆ†æNBAçš„å‘å±•å†å²",
        ] * 20  # é‡å¤20æ¬¡
        
        # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
        times = []
        filter_times = []
        bert_times = []
        
        for query in test_queries:
            start_time = time.time()
            result = self.router.route_query(query)
            total_time = time.time() - start_time
            
            times.append(total_time * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
            filter_times.append(result['filter_time'] * 1000)
            bert_times.append(result.get('bert_time', 0) * 1000)
        
        # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
        performance_stats = {
            'total_time': {
                'mean': np.mean(times),
                'median': np.median(times),
                'std': np.std(times),
                'min': np.min(times),
                'max': np.max(times),
                'p95': np.percentile(times, 95),
                'p99': np.percentile(times, 99)
            },
            'filter_time': {
                'mean': np.mean(filter_times),
                'median': np.median(filter_times),
                'std': np.std(filter_times)
            },
            'bert_time': {
                'mean': np.mean(bert_times),
                'median': np.median(bert_times),
                'std': np.std(bert_times)
            },
            'test_queries_count': len(test_queries),
            'timestamp': time.time()
        }
        
        # ä¿å­˜æ€§èƒ½ç»“æœ
        perf_path = self.results_dir / "performance_evaluation.json"
        with open(perf_path, 'w', encoding='utf-8') as f:
            json.dump(performance_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âš¡ æ€§èƒ½è¯„ä¼°å®Œæˆ:")
        logger.info(f"   å¹³å‡æ€»æ—¶é—´: {performance_stats['total_time']['mean']:.1f}ms")
        logger.info(f"   å¹³å‡è¿‡æ»¤æ—¶é—´: {performance_stats['filter_time']['mean']:.1f}ms")
        logger.info(f"   å¹³å‡BERTæ—¶é—´: {performance_stats['bert_time']['mean']:.1f}ms")
        logger.info(f"   P95å»¶è¿Ÿ: {performance_stats['total_time']['p95']:.1f}ms")
        logger.info(f"   ç»“æœå·²ä¿å­˜: {perf_path}")
        
        return performance_stats
    
    def create_confusion_matrix(self, evaluation_result: Dict):
        """åˆ›å»ºæ··æ·†çŸ©é˜µå¯è§†åŒ–"""
        
        logger.info("ğŸ“ˆ åˆ›å»ºæ··æ·†çŸ©é˜µ...")
        
        # æå–é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
        y_true = [r['expected_intent'] for r in evaluation_result['detailed_results']]
        y_pred = [r['predicted_intent'] for r in evaluation_result['detailed_results']]
        
        # æ ‡ç­¾åˆ—è¡¨
        labels = ['simple', 'complex', 'non_basketball']
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        
        # åˆ›å»ºå¯è§†åŒ–
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=labels, yticklabels=labels)
        plt.title('æ™ºèƒ½è·¯ç”±ç³»ç»Ÿ - æ„å›¾åˆ†ç±»æ··æ·†çŸ©é˜µ')
        plt.xlabel('é¢„æµ‹æ„å›¾')
        plt.ylabel('çœŸå®æ„å›¾')
        
        # ä¿å­˜å›¾ç‰‡
        cm_path = self.results_dir / "confusion_matrix.png"
        plt.savefig(cm_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜: {cm_path}")
    
    def create_performance_visualization(self, performance_stats: Dict):
        """åˆ›å»ºæ€§èƒ½å¯è§†åŒ–"""
        
        logger.info("ğŸ“ˆ åˆ›å»ºæ€§èƒ½å¯è§†åŒ–...")
        
        # åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å­å›¾1: å„ç»„ä»¶å¹³å‡æ—¶é—´
        components = ['ç¯®çƒè¿‡æ»¤å™¨', 'BERTåˆ†ç±»å™¨', 'æ€»æ—¶é—´']
        times = [
            performance_stats['filter_time']['mean'],
            performance_stats['bert_time']['mean'],
            performance_stats['total_time']['mean']
        ]
        colors = ['lightblue', 'lightgreen', 'lightcoral']
        
        bars = ax1.bar(components, times, color=colors)
        ax1.set_title('å„ç»„ä»¶å¹³å‡å¤„ç†æ—¶é—´')
        ax1.set_ylabel('æ—¶é—´ (ms)')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, time_val in zip(bars, times):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                    f'{time_val:.1f}ms', ha='center', va='bottom')
        
        # å­å›¾2: æ—¶é—´åˆ†å¸ƒç®±å‹å›¾
        total_times = np.random.normal(
            performance_stats['total_time']['mean'],
            performance_stats['total_time']['std'],
            100
        )
        
        ax2.boxplot(total_times, labels=['æ€»å¤„ç†æ—¶é—´'])
        ax2.set_title('å¤„ç†æ—¶é—´åˆ†å¸ƒ')
        ax2.set_ylabel('æ—¶é—´ (ms)')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        perf_viz_path = self.results_dir / "performance_visualization.png"
        plt.savefig(perf_viz_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"ğŸ“Š æ€§èƒ½å¯è§†åŒ–å·²ä¿å­˜: {perf_viz_path}")
    
    def generate_comprehensive_report(self) -> str:
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        
        logger.info("ğŸ“‹ ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
        
        # æ‰§è¡Œæ‰€æœ‰è¯„ä¼°
        accuracy_result = self.evaluate_routing_accuracy()
        performance_result = self.evaluate_performance()
        
        # åˆ›å»ºå¯è§†åŒ–
        self.create_confusion_matrix(accuracy_result)
        self.create_performance_visualization(performance_result)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
ğŸ¯ æ™ºèƒ½è·¯ç”±ç³»ç»Ÿ - ç»¼åˆè¯„ä¼°æŠ¥å‘Š
{'='*60}

ğŸ“Š å‡†ç¡®æ€§è¯„ä¼°:
â”œâ”€â”€ æ„å›¾åˆ†ç±»å‡†ç¡®ç‡: {accuracy_result['intent_accuracy']:.3f}
â”œâ”€â”€ å¤„ç†å™¨è·¯ç”±å‡†ç¡®ç‡: {accuracy_result['processor_accuracy']:.3f}
â”œâ”€â”€ æ€»ä½“å‡†ç¡®ç‡: {accuracy_result['overall_accuracy']:.3f}
â”œâ”€â”€ æ­£ç¡®åˆ†ç±»: {accuracy_result['correct_overall']}/{accuracy_result['total_cases']}
â””â”€â”€ æµ‹è¯•ç”¨ä¾‹æ•°: {accuracy_result['total_cases']}

âš¡ æ€§èƒ½è¯„ä¼°:
â”œâ”€â”€ å¹³å‡æ€»å¤„ç†æ—¶é—´: {performance_result['total_time']['mean']:.1f}ms
â”œâ”€â”€ å¹³å‡è¿‡æ»¤å™¨æ—¶é—´: {performance_result['filter_time']['mean']:.1f}ms
â”œâ”€â”€ å¹³å‡BERTæ—¶é—´: {performance_result['bert_time']['mean']:.1f}ms
â”œâ”€â”€ P95å»¶è¿Ÿ: {performance_result['total_time']['p95']:.1f}ms
â”œâ”€â”€ P99å»¶è¿Ÿ: {performance_result['total_time']['p99']:.1f}ms
â””â”€â”€ æµ‹è¯•æŸ¥è¯¢æ•°: {performance_result['test_queries_count']}

ğŸ” è¯¦ç»†åˆ†æ:
â”œâ”€â”€ ç¯®çƒè¿‡æ»¤å™¨æ•ˆç‡: {performance_result['filter_time']['mean']:.1f}ms (è¶…å¿«)
â”œâ”€â”€ BERTåˆ†ç±»å™¨æ•ˆç‡: {performance_result['bert_time']['mean']:.1f}ms (è‰¯å¥½)
â”œâ”€â”€ ç³»ç»Ÿæ€»ä½“æ•ˆç‡: {performance_result['total_time']['mean']:.1f}ms (ä¼˜ç§€)
â””â”€â”€ å¤„ç†ç¨³å®šæ€§: æ ‡å‡†å·® {performance_result['total_time']['std']:.1f}ms (ç¨³å®š)

ğŸ“ˆ ç³»ç»Ÿä¼˜åŠ¿:
â”œâ”€â”€ âœ… é«˜å‡†ç¡®ç‡: æ€»ä½“å‡†ç¡®ç‡è¾¾åˆ° {accuracy_result['overall_accuracy']:.1%}
â”œâ”€â”€ âœ… å¿«é€Ÿå“åº”: å¹³å‡å¤„ç†æ—¶é—´ < {performance_result['total_time']['mean']:.0f}ms
â”œâ”€â”€ âœ… æ™ºèƒ½è¿‡æ»¤: æœ‰æ•ˆè¿‡æ»¤éç¯®çƒæŸ¥è¯¢
â”œâ”€â”€ âœ… ç²¾ç¡®åˆ†ç±»: åŒºåˆ†ç®€å•å’Œå¤æ‚æŸ¥è¯¢
â””â”€â”€ âœ… ç¨³å®šæ€§å¥½: æ€§èƒ½æ³¢åŠ¨å°

ğŸ¯ å»ºè®®æ”¹è¿›:
â”œâ”€â”€ ğŸ“ æ‰©å……è®­ç»ƒæ•°æ®ä»¥æé«˜è¾¹ç•Œæƒ…å†µå¤„ç†
â”œâ”€â”€ âš¡ ä¼˜åŒ–BERTæ¨¡å‹ä»¥è¿›ä¸€æ­¥æå‡é€Ÿåº¦
â”œâ”€â”€ ğŸ” å¢åŠ æ›´å¤šç¯®çƒé¢†åŸŸå…³é”®è¯
â””â”€â”€ ğŸ“Š æŒç»­ç›‘æ§å’Œè°ƒä¼˜ç³»ç»Ÿå‚æ•°

ğŸ“ è¯„ä¼°æ–‡ä»¶:
â”œâ”€â”€ å‡†ç¡®æ€§è¯„ä¼°: results/evaluations/routing_accuracy_evaluation.json
â”œâ”€â”€ æ€§èƒ½è¯„ä¼°: results/evaluations/performance_evaluation.json
â”œâ”€â”€ æ··æ·†çŸ©é˜µ: results/evaluations/confusion_matrix.png
â””â”€â”€ æ€§èƒ½å¯è§†åŒ–: results/evaluations/performance_visualization.png

ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
        """
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.results_dir / "comprehensive_evaluation_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ğŸ“‹ ç»¼åˆè¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return report

# å…¨å±€è¯„ä¼°å™¨å®ä¾‹
model_evaluator = ModelEvaluator()

if __name__ == "__main__":
    # è¿è¡Œç»¼åˆè¯„ä¼°
    report = model_evaluator.generate_comprehensive_report()
    print(report)
