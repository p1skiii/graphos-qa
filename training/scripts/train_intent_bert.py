#!/usr/bin/env python3
"""
BERTæ„å›¾åˆ†ç±»è®­ç»ƒè„šæœ¬
ç‹¬ç«‹çš„è®­ç»ƒæ¨¡å—ï¼Œä¸ä¸šåŠ¡ä»£ç åˆ†ç¦»
"""
import os
import sys
import yaml
import json
import torch
from pathlib import Path
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments, 
    Trainer,
    EarlyStoppingCallback,
    DataCollatorWithPadding
)
from datasets import Dataset
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class IntentBERTTrainer:
    def __init__(self, config_path: str):
        """åˆå§‹åŒ–BERTè®­ç»ƒå™¨"""
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.model_name = self.config['model']['name']
        self.output_dir = Path(self.config['output']['model_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾å¤‡æ£€æµ‹
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ ‡ç­¾æ˜ å°„
        self.id2label = {v: k for k, v in self.config['labels'].items()}
        self.label2id = self.config['labels']
        
    def load_data(self):
        """åŠ è½½å¤„ç†åçš„æ•°æ®"""
        logger.info("ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®...")
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        train_path = Path("training/data/processed/train.json")
        val_path = Path("training/data/processed/val.json")
        
        if not train_path.exists() or not val_path.exists():
            logger.error("âŒ å¤„ç†åçš„æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬")
            raise FileNotFoundError("è¯·å…ˆè¿è¡Œ: python prepare_data.py")
        
        # åŠ è½½JSONæ•°æ®
        with open(train_path, 'r', encoding='utf-8') as f:
            train_data = json.load(f)
        
        with open(val_path, 'r', encoding='utf-8') as f:
            val_data = json.load(f)
        
        # è½¬æ¢ä¸ºDatasetæ ¼å¼
        train_dataset = Dataset.from_list(train_data)
        val_dataset = Dataset.from_list(val_data)
        
        logger.info(f"ğŸ“Š è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        logger.info(f"ğŸ“Š éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        
        return train_dataset, val_dataset
    
    def tokenize_data(self, train_dataset: Dataset, val_dataset: Dataset):
        """åˆ†è¯å¤„ç†"""
        logger.info("ğŸ”¤ å¼€å§‹åˆ†è¯å¤„ç†...")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        def tokenize_function(examples):
            """åˆ†è¯å‡½æ•°"""
            return tokenizer(
                examples["text"], 
                truncation=True, 
                padding=False,  # ä½¿ç”¨DataCollatorè¿›è¡ŒåŠ¨æ€padding
                max_length=self.config['model']['max_length']
            )
        
        # åº”ç”¨åˆ†è¯
        train_dataset = train_dataset.map(tokenize_function, batched=True)
        val_dataset = val_dataset.map(tokenize_function, batched=True)
        
        # é‡å‘½åæ ‡ç­¾åˆ—
        train_dataset = train_dataset.rename_column("label_id", "labels")
        val_dataset = val_dataset.rename_column("label_id", "labels")
        
        # è®¾ç½®æ ¼å¼
        train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
        val_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
        
        logger.info("âœ… åˆ†è¯å¤„ç†å®Œæˆ")
        
        return train_dataset, val_dataset, tokenizer
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        logger.info(f"ğŸ¤– åˆ›å»ºæ¨¡å‹: {self.model_name}")
        
        model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=self.config['model']['num_labels'],
            id2label=self.id2label,
            label2id=self.label2id
        )
        
        return model
    
    def compute_metrics(self, eval_pred):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        # å‡†ç¡®ç‡
        accuracy = accuracy_score(labels, predictions)
        
        # ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
        
        return {
            'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall
        }
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹BERTè®­ç»ƒ...")
        
        try:
            # 1. åŠ è½½æ•°æ®
            train_dataset, val_dataset = self.load_data()
            
            # 2. åˆ†è¯å¤„ç†
            train_dataset, val_dataset, tokenizer = self.tokenize_data(train_dataset, val_dataset)
            
            # 3. åˆ›å»ºæ¨¡å‹
            model = self.create_model()
            
            # 4. æ•°æ®æ•´ç†å™¨
            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
            
            # 5. è®­ç»ƒå‚æ•°
            training_args = TrainingArguments(
                output_dir=str(self.output_dir),
                num_train_epochs=self.config['training']['num_epochs'],
                per_device_train_batch_size=self.config['training']['batch_size'],
                per_device_eval_batch_size=self.config['training']['batch_size'],
                learning_rate=self.config['training']['learning_rate'],
                warmup_steps=self.config['training']['warmup_steps'],
                weight_decay=self.config['training']['weight_decay'],
                logging_dir=self.config['output']['logs_dir'],
                logging_steps=self.config['output']['logging_steps'],
                evaluation_strategy="steps",
                eval_steps=self.config['output']['eval_steps'],
                save_steps=self.config['output']['save_steps'],
                load_best_model_at_end=True,
                metric_for_best_model=self.config['early_stopping']['metric'],
                greater_is_better=True,
                save_total_limit=3,  # åªä¿ç•™æœ€è¿‘3ä¸ªcheckpoint
                fp16=self.config['training']['fp16'],
                gradient_accumulation_steps=self.config['training']['gradient_accumulation_steps'],
                report_to="none",  # ç¦ç”¨wandbç­‰æŠ¥å‘Š
            )
            
            # 6. æ—©åœå›è°ƒ
            early_stopping = EarlyStoppingCallback(
                early_stopping_patience=self.config['early_stopping']['patience']
            )
            
            # 7. è®­ç»ƒå™¨
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                data_collator=data_collator,
                compute_metrics=self.compute_metrics,
                callbacks=[early_stopping]
            )
            
            # 8. å¼€å§‹è®­ç»ƒ
            logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
            trainer.train()
            
            # 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_path = self.output_dir / "final"
            trainer.save_model(str(final_model_path))
            tokenizer.save_pretrained(str(final_model_path))
            
            # 10. è¯„ä¼°æ¨¡å‹
            logger.info("ğŸ“Š è¯„ä¼°æ¨¡å‹...")
            eval_results = trainer.evaluate()
            
            logger.info("ğŸ¯ è®­ç»ƒå®Œæˆï¼è¯„ä¼°ç»“æœ:")
            for key, value in eval_results.items():
                logger.info(f"   {key}: {value:.4f}")
            
            # 11. ç”Ÿæˆè¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
            predictions = trainer.predict(val_dataset)
            y_pred = np.argmax(predictions.predictions, axis=1)
            y_true = predictions.label_ids
            
            target_names = [self.id2label[i] for i in range(len(self.id2label))]
            report = classification_report(y_true, y_pred, target_names=target_names)
            
            logger.info("ğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
            logger.info(f"\n{report}")
            
            # ä¿å­˜æŠ¥å‘Š
            with open(self.output_dir / "classification_report.txt", 'w') as f:
                f.write(report)
            
            logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼ä¿å­˜ä½ç½®: {final_model_path}")
            
            return str(final_model_path)
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python train_intent_bert.py <bert_config.yaml>")
        sys.exit(1)
    
    config_path = sys.argv[1]
    
    if not Path(config_path).exists():
        print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        sys.exit(1)
    
    trainer = IntentBERTTrainer(config_path)
    model_path = trainer.train()
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
    print(f"ğŸ”— ä½ å¯ä»¥åœ¨ä¸šåŠ¡ä»£ç ä¸­ä½¿ç”¨è¯¥æ¨¡å‹è¿›è¡Œæ„å›¾åˆ†ç±»")

if __name__ == "__main__":
    main()
