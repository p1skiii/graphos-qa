#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬
è¯„ä¼°è®­ç»ƒå¥½çš„BERTæ¨¡å‹æ€§èƒ½
"""
import os
import sys
import yaml
import json
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import (
    accuracy_score, 
    precision_recall_fscore_support,
    classification_report,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ModelEvaluator:
    def __init__(self, model_path: str, config_path: str = None):
        """åˆå§‹åŒ–æ¨¡å‹è¯„ä¼°å™¨"""
        self.model_path = Path(model_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åŠ è½½é…ç½®ï¼ˆå¦‚æœæä¾›ï¼‰
        if config_path:
            with open(config_path, 'r') as f:
                self.config = yaml.safe_load(f)
        else:
            self.config = None
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(str(self.model_path))
        self.model = AutoModelForSequenceClassification.from_pretrained(str(self.model_path))
        self.model.to(self.device)
        self.model.eval()
        
        # è·å–æ ‡ç­¾æ˜ å°„
        self.id2label = self.model.config.id2label
        self.label2id = self.model.config.label2id
        
        logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
        logger.info(f"ğŸ·ï¸ æ”¯æŒçš„æ ‡ç­¾: {list(self.label2id.keys())}")
    
    def predict_single(self, text: str):
        """å•æ¡æ–‡æœ¬é¢„æµ‹"""
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=128
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probabilities = torch.softmax(outputs.logits, dim=-1)
        
        predicted_id = outputs.logits.argmax().item()
        confidence = probabilities[0][predicted_id].item()
        predicted_label = self.id2label[predicted_id]
        
        return predicted_label, confidence, probabilities[0].cpu().numpy()
    
    def predict_batch(self, texts: list):
        """æ‰¹é‡é¢„æµ‹"""
        predictions = []
        confidences = []
        
        for text in texts:
            pred_label, confidence, _ = self.predict_single(text)
            predictions.append(pred_label)
            confidences.append(confidence)
        
        return predictions, confidences
    
    def evaluate_on_validation_data(self):
        """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°"""
        logger.info("ğŸ“Š åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
        
        # åŠ è½½éªŒè¯æ•°æ®
        val_path = Path("training/data/processed/val.json")
        if not val_path.exists():
            logger.error("éªŒè¯æ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†")
            return None
        
        with open(val_path, 'r', encoding='utf-8') as f:
            val_data = json.load(f)
        
        # æå–æ–‡æœ¬å’ŒçœŸå®æ ‡ç­¾
        texts = [item['text'] for item in val_data]
        true_labels = [item['label'] for item in val_data]
        
        # æ‰¹é‡é¢„æµ‹
        pred_labels, confidences = self.predict_batch(texts)
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(true_labels, pred_labels)
        precision, recall, f1, _ = precision_recall_fscore_support(
            true_labels, pred_labels, average='weighted'
        )
        
        # è¯¦ç»†æŠ¥å‘Š
        target_names = list(self.label2id.keys())
        report = classification_report(true_labels, pred_labels, target_names=target_names)
        
        # ç»“æœå±•ç¤º
        logger.info("ğŸ¯ éªŒè¯é›†è¯„ä¼°ç»“æœ:")
        logger.info(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
        logger.info(f"   ç²¾ç¡®ç‡: {precision:.4f}")
        logger.info(f"   å¬å›ç‡: {recall:.4f}")
        logger.info(f"   F1åˆ†æ•°: {f1:.4f}")
        logger.info(f"   å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidences):.4f}")
        
        logger.info("ğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        logger.info(f"\n{report}")
        
        # ç”Ÿæˆæ··æ·†çŸ©é˜µ
        self.plot_confusion_matrix(true_labels, pred_labels, target_names)
        
        # ç½®ä¿¡åº¦åˆ†æ
        self.analyze_confidence(pred_labels, true_labels, confidences)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'avg_confidence': np.mean(confidences),
            'predictions': pred_labels,
            'true_labels': true_labels,
            'confidences': confidences
        }
    
    def plot_confusion_matrix(self, true_labels, pred_labels, target_names):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        try:
            cm = confusion_matrix(true_labels, pred_labels, labels=target_names)
            
            plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=target_names, yticklabels=target_names)
            plt.title('Confusion Matrix')
            plt.xlabel('Predicted Label')
            plt.ylabel('True Label')
            plt.xticks(rotation=45)
            plt.yticks(rotation=0)
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            output_path = self.model_path.parent / "confusion_matrix.png"
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            logger.info(f"ğŸ“Š æ··æ·†çŸ©é˜µä¿å­˜åˆ°: {output_path}")
            plt.close()
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•ç”Ÿæˆæ··æ·†çŸ©é˜µå›¾ç‰‡: {e}")
    
    def analyze_confidence(self, pred_labels, true_labels, confidences):
        """åˆ†æç½®ä¿¡åº¦åˆ†å¸ƒ"""
        try:
            # æŒ‰æ­£ç¡®æ€§åˆ†æç½®ä¿¡åº¦
            correct_mask = np.array(pred_labels) == np.array(true_labels)
            correct_confidences = np.array(confidences)[correct_mask]
            incorrect_confidences = np.array(confidences)[~correct_mask]
            
            logger.info("ğŸ” ç½®ä¿¡åº¦åˆ†æ:")
            logger.info(f"   æ­£ç¡®é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {np.mean(correct_confidences):.4f}")
            logger.info(f"   é”™è¯¯é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {np.mean(incorrect_confidences):.4f}")
            
            # æŒ‰æ ‡ç­¾åˆ†æç½®ä¿¡åº¦
            logger.info("ğŸ“Š å„æ ‡ç­¾ç½®ä¿¡åº¦:")
            for label in set(pred_labels):
                label_mask = np.array(pred_labels) == label
                label_confidences = np.array(confidences)[label_mask]
                logger.info(f"   {label}: {np.mean(label_confidences):.4f}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ç½®ä¿¡åº¦åˆ†æå¤±è´¥: {e}")
    
    def test_custom_examples(self):
        """æµ‹è¯•è‡ªå®šä¹‰æ ·ä¾‹"""
        logger.info("ğŸ§ª æµ‹è¯•è‡ªå®šä¹‰æ ·ä¾‹...")
        
        test_examples = [
            "How old is Kobe Bryant?",
            "Compare LeBron James and Michael Jordan",
            "Did Shaq and Kobe play together?",
            "I love watching basketball games",
            "What's the weather like today?",
            "Who is the tallest player in NBA?",
            "Lakers vs Warriors who is better?",
            "Tell me about basketball history"
        ]
        
        for i, text in enumerate(test_examples, 1):
            pred_label, confidence, probs = self.predict_single(text)
            logger.info(f"æ ·ä¾‹ {i}: {text}")
            logger.info(f"   é¢„æµ‹: {pred_label} (ç½®ä¿¡åº¦: {confidence:.3f})")
            
            # æ˜¾ç¤ºtop-2é¢„æµ‹
            top_indices = np.argsort(probs)[-2:][::-1]
            for idx in top_indices:
                label = self.id2label[idx]
                prob = probs[idx]
                logger.info(f"   {label}: {prob:.3f}")
            logger.info("")

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python evaluate_model.py <model_path> [config_path]")
        sys.exit(1)
    
    model_path = sys.argv[1]
    config_path = sys.argv[2] if len(sys.argv) > 2 else None
    
    if not Path(model_path).exists():
        print(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        sys.exit(1)
    
    try:
        evaluator = ModelEvaluator(model_path, config_path)
        
        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        results = evaluator.evaluate_on_validation_data()
        
        # æµ‹è¯•è‡ªå®šä¹‰æ ·ä¾‹
        evaluator.test_custom_examples()
        
        if results:
            print(f"\nğŸ¯ è¯„ä¼°å®Œæˆï¼")
            print(f"å‡†ç¡®ç‡: {results['accuracy']:.4f}")
            print(f"F1åˆ†æ•°: {results['f1']:.4f}")
            print(f"å¹³å‡ç½®ä¿¡åº¦: {results['avg_confidence']:.4f}")
        
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
