"""
RAG æ£€ç´¢å™¨ç»„ä»¶é›†åˆ
å®ç°å¤šç§æ£€ç´¢ç­–ç•¥ï¼šè¯­ä¹‰æ£€ç´¢ã€å‘é‡æ£€ç´¢ã€å…³é”®è¯æ£€ç´¢ã€æ··åˆæ£€ç´¢
"""
import numpy as np
from typing import List, Dict, Any, Optional, Union
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import logging
from app.database import NebulaGraphConnection
from config import Config
from app.rag.component_factory import BaseRetriever, component_factory

logger = logging.getLogger(__name__)

# =============================================================================
# è¯­ä¹‰æ£€ç´¢å™¨
# =============================================================================

class SemanticRetriever(BaseRetriever):
    """è¯­ä¹‰æ£€ç´¢å™¨ - åŸºäºå¥å­åµŒå…¥çš„ç›¸ä¼¼åº¦æ£€ç´¢"""
    
    def __init__(self, embedding_model: str = None, top_k: int = 5, 
                 similarity_threshold: float = 0.3, nebula_conn=None):
        """åˆå§‹åŒ–è¯­ä¹‰æ£€ç´¢å™¨"""
        self.embedding_model_name = embedding_model or Config.EMBEDDING_MODEL
        self.top_k = top_k
        self.similarity_threshold = similarity_threshold
        
        self.embedding_model = None
        self.node_embeddings = {}
        # ä½¿ç”¨ä¼ é€’çš„è¿æ¥æˆ–åˆ›å»ºæ–°è¿æ¥
        self.nebula_conn = nebula_conn if nebula_conn is not None else NebulaGraphConnection()
        self.is_initialized = False
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        try:
            logger.info(f"ğŸ”„ åˆå§‹åŒ–è¯­ä¹‰æ£€ç´¢å™¨ ({self.embedding_model_name})...")
            
            # åŠ è½½åµŒå…¥æ¨¡å‹
            self.embedding_model = SentenceTransformer(self.embedding_model_name)
            
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            if not self.nebula_conn.session:
                if not self.nebula_conn.connect():
                    logger.error("âŒ NebulaGraphè¿æ¥å¤±è´¥")
                    return False
            
            # é¢„è®¡ç®—èŠ‚ç‚¹åµŒå…¥
            self._precompute_embeddings()
            
            self.is_initialized = True
            logger.info("âœ… è¯­ä¹‰æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def _precompute_embeddings(self):
        """é¢„è®¡ç®—èŠ‚ç‚¹åµŒå…¥å‘é‡"""
        logger.info("ğŸ”¢ é¢„è®¡ç®—èŠ‚ç‚¹åµŒå…¥å‘é‡...")
        
        # è®¡ç®—çƒå‘˜èŠ‚ç‚¹åµŒå…¥
        self._compute_player_embeddings()
        
        # è®¡ç®—çƒé˜ŸèŠ‚ç‚¹åµŒå…¥
        self._compute_team_embeddings()
        
        logger.info(f"âœ… åµŒå…¥å‘é‡è®¡ç®—å®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {len(self.node_embeddings)}")
    
    def _compute_player_embeddings(self):
        """è®¡ç®—çƒå‘˜èŠ‚ç‚¹åµŒå…¥"""
        query = """
        MATCH (p:player)
        RETURN p.player.name AS name, p.player.age AS age
        LIMIT 100
        """
        
        result = self.nebula_conn.execute_query(query)
        if result['success']:
            for row in result['rows']:
                name, age = row[0], row[1]
                # å¤„ç†å¯èƒ½çš„NULLå€¼
                if name is None:
                    continue
                if age is None:
                    age = 0
                    
                description = f"çƒå‘˜ {name}, å¹´é¾„ {age} å²"
                
                embedding = self.embedding_model.encode([description])[0]
                
                node_id = f"player:{name}"
                self.node_embeddings[node_id] = {
                    'embedding': embedding,
                    'type': 'player',
                    'name': name,
                    'description': description,
                    'properties': {'age': age}
                }
    
    def _compute_team_embeddings(self):
        """è®¡ç®—çƒé˜ŸèŠ‚ç‚¹åµŒå…¥"""
        query = """
        MATCH (t:team)
        RETURN t.team.name AS name
        LIMIT 50
        """
        
        result = self.nebula_conn.execute_query(query)
        if result['success']:
            for row in result['rows']:
                name = row[0]
                # å¤„ç†å¯èƒ½çš„NULLå€¼
                if name is None:
                    continue
                    
                description = f"çƒé˜Ÿ {name}"
                
                embedding = self.embedding_model.encode([description])[0]
                
                node_id = f"team:{name}"
                self.node_embeddings[node_id] = {
                    'embedding': embedding,
                    'type': 'team',
                    'name': name,
                    'description': description,
                    'properties': {}
                }
    
    def retrieve(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æ£€ç´¢ç›¸å…³èŠ‚ç‚¹"""
        if not self.is_initialized:
            raise RuntimeError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–")
        
        top_k = top_k or self.top_k
        
        # è®¡ç®—æŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_model.encode([query])[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for node_id, node_data in self.node_embeddings.items():
            similarity = cosine_similarity(
                [query_embedding], 
                [node_data['embedding']]
            )[0][0]
            
            if similarity >= self.similarity_threshold:
                similarities.append({
                    'node_id': node_id,
                    'similarity': float(similarity),
                    'type': node_data['type'],
                    'name': node_data['name'],
                    'description': node_data['description'],
                    'properties': node_data['properties']
                })
        
        # æ’åºå¹¶è¿”å›top_k
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        return similarities[:top_k]

# =============================================================================
# å‘é‡æ£€ç´¢å™¨
# =============================================================================

class VectorRetriever(BaseRetriever):
    """å‘é‡æ£€ç´¢å™¨ - ä¼˜åŒ–çš„å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢"""
    
    def __init__(self, embedding_model: str = None, top_k: int = 5, 
                 use_faiss: bool = False, nebula_conn=None):
        """åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨"""
        self.embedding_model_name = embedding_model or Config.EMBEDDING_MODEL
        self.top_k = top_k
        self.use_faiss = use_faiss
        
        self.embedding_model = None
        self.vectors = None
        self.node_index = {}
        # ä½¿ç”¨ä¼ é€’çš„è¿æ¥æˆ–åˆ›å»ºæ–°è¿æ¥
        self.nebula_conn = nebula_conn if nebula_conn is not None else NebulaGraphConnection()
        self.is_initialized = False
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        try:
            logger.info(f"ğŸ”„ åˆå§‹åŒ–å‘é‡æ£€ç´¢å™¨...")
            
            # åŠ è½½åµŒå…¥æ¨¡å‹
            self.embedding_model = SentenceTransformer(self.embedding_model_name)
            
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            if not self.nebula_conn.session:
                if not self.nebula_conn.connect():
                    logger.error("âŒ NebulaGraphè¿æ¥å¤±è´¥")
                    return False
            
            # æ„å»ºå‘é‡ç´¢å¼•
            self._build_vector_index()
            
            self.is_initialized = True
            logger.info("âœ… å‘é‡æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def _build_vector_index(self):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        logger.info("ğŸ”¢ æ„å»ºå‘é‡ç´¢å¼•...")
        
        nodes_data = []
        embeddings = []
        
        # è·å–æ‰€æœ‰èŠ‚ç‚¹
        queries = [
            "MATCH (p:player) RETURN 'player' AS type, p.player.name AS name, p.player.age AS age LIMIT 100",
            "MATCH (t:team) RETURN 'team' AS type, t.team.name AS name, NULL AS age LIMIT 50"
        ]
        
        for query in queries:
            result = self.nebula_conn.execute_query(query)
            if result['success']:
                for row in result['rows']:
                    node_type, name, age = row[0], row[1], row[2]
                    
                    # å¤„ç†NULLå€¼
                    if name is None:
                        continue
                    if age is None:
                        age = 0
                    
                    # æ„å»ºæè¿°
                    if node_type == 'player':
                        description = f"çƒå‘˜ {name}, å¹´é¾„ {age} å²"
                    else:
                        description = f"çƒé˜Ÿ {name}"
                    
                    # ç”ŸæˆåµŒå…¥
                    embedding = self.embedding_model.encode([description])[0]
                    
                    node_data = {
                        'node_id': f"{node_type}:{name}",
                        'type': node_type,
                        'name': name,
                        'description': description,
                        'properties': {'age': age} if age else {}
                    }
                    
                    nodes_data.append(node_data)
                    embeddings.append(embedding)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        self.vectors = np.array(embeddings)
        
        # æ„å»ºç´¢å¼•æ˜ å°„
        for i, node_data in enumerate(nodes_data):
            self.node_index[i] = node_data
        
        logger.info(f"âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {len(self.node_index)}")
    
    def retrieve(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:
        """å‘é‡æ£€ç´¢ç›¸å…³èŠ‚ç‚¹"""
        if not self.is_initialized:
            raise RuntimeError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–")
        
        top_k = top_k or self.top_k
        
        # è®¡ç®—æŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_model.encode([query])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_embedding, self.vectors)[0]
        
        # è·å–top_kç»“æœ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            node_data = self.node_index[idx].copy()
            node_data['similarity'] = float(similarities[idx])
            results.append(node_data)
        
        return results

# =============================================================================
# å…³é”®è¯æ£€ç´¢å™¨
# =============================================================================

class KeywordRetriever(BaseRetriever):
    """å…³é”®è¯æ£€ç´¢å™¨ - åŸºäºTF-IDFçš„å…³é”®è¯åŒ¹é…"""
    
    def __init__(self, top_k: int = 5, use_tfidf: bool = True, 
                 min_score: float = 0.1, nebula_conn=None):
        """åˆå§‹åŒ–å…³é”®è¯æ£€ç´¢å™¨"""
        self.top_k = top_k
        self.use_tfidf = use_tfidf
        self.min_score = min_score
        
        self.tfidf_vectorizer = None
        self.tfidf_matrix = None
        self.documents = []
        self.node_index = {}
        # ä½¿ç”¨ä¼ é€’çš„è¿æ¥æˆ–åˆ›å»ºæ–°è¿æ¥
        self.nebula_conn = nebula_conn if nebula_conn is not None else NebulaGraphConnection()
        self.is_initialized = False
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        try:
            logger.info("ğŸ”„ åˆå§‹åŒ–å…³é”®è¯æ£€ç´¢å™¨...")
            
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            if not self.nebula_conn.session:
                if not self.nebula_conn.connect():
                    logger.error("âŒ NebulaGraphè¿æ¥å¤±è´¥")
                    return False
            
            # æ„å»ºTF-IDFç´¢å¼•
            self._build_tfidf_index()
            
            self.is_initialized = True
            logger.info("âœ… å…³é”®è¯æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å…³é”®è¯æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def _build_tfidf_index(self):
        """æ„å»ºTF-IDFç´¢å¼•"""
        logger.info("ğŸ“ æ„å»ºTF-IDFç´¢å¼•...")
        
        documents = []
        
        # è·å–æ‰€æœ‰èŠ‚ç‚¹æ–‡æ¡£
        queries = [
            "MATCH (p:player) RETURN 'player' AS type, p.player.name AS name, p.player.age AS age LIMIT 100",
            "MATCH (t:team) RETURN 'team' AS type, t.team.name AS name, NULL AS age LIMIT 50"
        ]
        
        doc_idx = 0
        for query in queries:
            result = self.nebula_conn.execute_query(query)
            if result['success']:
                for row in result['rows']:
                    node_type, name, age = row[0], row[1], row[2]
                    
                    # å¤„ç†NULLå€¼
                    if name is None:
                        continue
                    if age is None:
                        age = 0
                    
                    # æ„å»ºæ–‡æ¡£æ–‡æœ¬
                    if node_type == 'player':
                        document = f"çƒå‘˜ {name} å¹´é¾„ {age} å²"
                        keywords = [name, str(age), 'çƒå‘˜', 'å¹´é¾„']
                    else:
                        document = f"çƒé˜Ÿ {name}"
                        keywords = [name, 'çƒé˜Ÿ']
                    
                    documents.append(document)
                    
                    self.node_index[doc_idx] = {
                        'node_id': f"{node_type}:{name}",
                        'type': node_type,
                        'name': name,
                        'document': document,
                        'keywords': keywords,
                        'properties': {'age': age} if age else {}
                    }
                    doc_idx += 1
        
        self.documents = documents
        
        # æ„å»ºTF-IDFçŸ©é˜µ
        if self.use_tfidf:
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words=None,
                lowercase=True
            )
            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)
        
        logger.info(f"âœ… TF-IDFç´¢å¼•æ„å»ºå®Œæˆï¼Œæ–‡æ¡£æ•°: {len(documents)}")
    
    def retrieve(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:
        """å…³é”®è¯æ£€ç´¢ç›¸å…³èŠ‚ç‚¹"""
        if not self.is_initialized:
            raise RuntimeError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–")
        
        top_k = top_k or self.top_k
        
        if self.use_tfidf:
            return self._tfidf_retrieve(query, top_k)
        else:
            return self._keyword_match_retrieve(query, top_k)
    
    def _tfidf_retrieve(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """TF-IDFæ£€ç´¢"""
        # è½¬æ¢æŸ¥è¯¢
        query_vector = self.tfidf_vectorizer.transform([query])
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]
        
        # è·å–top_kç»“æœ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            if similarities[idx] >= self.min_score:
                node_data = self.node_index[idx].copy()
                node_data['similarity'] = float(similarities[idx])
                results.append(node_data)
        
        return results
    
    def _keyword_match_retrieve(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """ç®€å•å…³é”®è¯åŒ¹é…"""
        query_keywords = re.findall(r'\w+', query.lower())
        
        scores = []
        for idx, node_data in self.node_index.items():
            score = 0
            for keyword in query_keywords:
                if any(keyword in str(k).lower() for k in node_data['keywords']):
                    score += 1
            
            if score > 0:
                scores.append({
                    'index': idx,
                    'score': score / len(query_keywords)
                })
        
        # æ’åºå¹¶è¿”å›top_k
        scores.sort(key=lambda x: x['score'], reverse=True)
        
        results = []
        for item in scores[:top_k]:
            if item['score'] >= self.min_score:
                node_data = self.node_index[item['index']].copy()
                node_data['similarity'] = float(item['score'])
                results.append(node_data)
        
        return results

# =============================================================================
# æ··åˆæ£€ç´¢å™¨
# =============================================================================

class HybridRetriever(BaseRetriever):
    """æ··åˆæ£€ç´¢å™¨ - ç»“åˆå¤šç§æ£€ç´¢ç­–ç•¥"""
    
    def __init__(self, semantic_weight: float = 0.5, keyword_weight: float = 0.3, 
                 vector_weight: float = 0.2, top_k: int = 5, nebula_conn=None):
        """åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨"""
        self.semantic_weight = semantic_weight
        self.keyword_weight = keyword_weight
        self.vector_weight = vector_weight
        self.top_k = top_k
        
        # ä¼ é€’è¿æ¥ç»™å­æ£€ç´¢å™¨
        self.semantic_retriever = SemanticRetriever(nebula_conn=nebula_conn)
        self.keyword_retriever = KeywordRetriever(nebula_conn=nebula_conn)
        self.vector_retriever = VectorRetriever(nebula_conn=nebula_conn)
        self.is_initialized = False
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        try:
            logger.info("ğŸ”„ åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨...")
            
            # åˆå§‹åŒ–æ‰€æœ‰å­æ£€ç´¢å™¨
            if not self.semantic_retriever.initialize():
                logger.error("âŒ è¯­ä¹‰æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥")
                return False
            
            if not self.keyword_retriever.initialize():
                logger.error("âŒ å…³é”®è¯æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥")
                return False
            
            if not self.vector_retriever.initialize():
                logger.error("âŒ å‘é‡æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥")
                return False
            
            self.is_initialized = True
            logger.info("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def retrieve(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢ç›¸å…³èŠ‚ç‚¹"""
        if not self.is_initialized:
            raise RuntimeError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–")
        
        top_k = top_k or self.top_k
        
        # ä»å„ä¸ªæ£€ç´¢å™¨è·å–ç»“æœ
        semantic_results = self.semantic_retriever.retrieve(query, top_k * 2)
        keyword_results = self.keyword_retriever.retrieve(query, top_k * 2)
        vector_results = self.vector_retriever.retrieve(query, top_k * 2)
        
        # åˆå¹¶å’Œé‡æ’åº
        combined_scores = {}
        
        # å¤„ç†è¯­ä¹‰æ£€ç´¢ç»“æœ
        for result in semantic_results:
            node_id = result['node_id']
            combined_scores[node_id] = combined_scores.get(node_id, 0) + \
                                     result['similarity'] * self.semantic_weight
        
        # å¤„ç†å…³é”®è¯æ£€ç´¢ç»“æœ
        for result in keyword_results:
            node_id = result['node_id']
            combined_scores[node_id] = combined_scores.get(node_id, 0) + \
                                     result['similarity'] * self.keyword_weight
        
        # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
        for result in vector_results:
            node_id = result['node_id']
            combined_scores[node_id] = combined_scores.get(node_id, 0) + \
                                     result['similarity'] * self.vector_weight
        
        # åˆ›å»ºèŠ‚ç‚¹æ•°æ®æ˜ å°„
        all_results = {}
        for results in [semantic_results, keyword_results, vector_results]:
            for result in results:
                all_results[result['node_id']] = result
        
        # æ’åºå¹¶è¿”å›top_k
        sorted_nodes = sorted(combined_scores.items(), 
                            key=lambda x: x[1], reverse=True)
        
        final_results = []
        for node_id, score in sorted_nodes[:top_k]:
            if node_id in all_results:
                node_data = all_results[node_id].copy()
                node_data['similarity'] = float(score)
                final_results.append(node_data)
        
        return final_results

# =============================================================================
# æ³¨å†Œæ‰€æœ‰æ£€ç´¢å™¨
# =============================================================================

def register_all_retrievers():
    """æ³¨å†Œæ‰€æœ‰æ£€ç´¢å™¨åˆ°å·¥å‚"""
    component_factory.register_retriever('semantic', SemanticRetriever)
    component_factory.register_retriever('vector', VectorRetriever)
    component_factory.register_retriever('keyword', KeywordRetriever)
    component_factory.register_retriever('hybrid', HybridRetriever)
    logger.info("âœ… æ‰€æœ‰æ£€ç´¢å™¨å·²æ³¨å†Œåˆ°ç»„ä»¶å·¥å‚")

# è‡ªåŠ¨æ³¨å†Œ
register_all_retrievers()
