"""
Phase 1 LLMé›†æˆæ¼”ç¤ºè„šæœ¬
å±•ç¤ºåŸºç¡€LLMæ¡†æ¶çš„æ ¸å¿ƒåŠŸèƒ½å’Œå¤„ç†æµç¨‹
"""
import sys
import os
import time
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/Users/wang/i/graphos-qa')

try:
    from app.llm import (
        create_llm_system, get_available_presets,
        UnifiedInput, LLMResponse
    )
    print("âœ… æˆåŠŸå¯¼å…¥LLMæ¨¡å—")
except ImportError as e:
    print(f"âŒ LLMæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

def demo_system_creation():
    """æ¼”ç¤ºç³»ç»Ÿåˆ›å»ºå’Œåˆå§‹åŒ–"""
    print("ğŸ¯ æ¼”ç¤º1: LLMç³»ç»Ÿåˆ›å»ºå’Œåˆå§‹åŒ–")
    print("="*50)
    
    # æ˜¾ç¤ºå¯ç”¨é¢„è®¾
    presets = get_available_presets()
    print(f"ğŸ“‹ å¯ç”¨é¢„è®¾: {presets}")
    
    # åˆ›å»ºMacOSä¼˜åŒ–ç³»ç»Ÿ
    print(f"\nğŸ”§ åˆ›å»ºMacOSä¼˜åŒ–LLMç³»ç»Ÿ...")
    system = create_llm_system('macos_optimized')
    
    print(f"âœ… ç³»ç»Ÿåˆ›å»ºå®Œæˆ")
    print(f"   é…ç½®ç±»å‹: macos_optimized")
    print(f"   æ¨¡å‹: {system.config.llm_config.model_config.model_name}")
    print(f"   è®¾å¤‡: {system.config.llm_config.model_config.device}")
    print(f"   å¤šæ¨¡æ€æ”¯æŒ: {system.config.llm_config.enable_multimodal}")
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    print(f"\nğŸ”„ åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶...")
    if system.initialize():
        print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        
        # æ˜¾ç¤ºç»„ä»¶çŠ¶æ€
        status = system.get_system_status()
        print(f"   åˆå§‹åŒ–çŠ¶æ€: {status['initialized']}")
        print(f"   å°±ç»ªçŠ¶æ€: {status['ready']}")
        print(f"   ç»„ä»¶çŠ¶æ€:")
        for component, loaded in status['components'].items():
            print(f"     {component}: {'âœ…' if loaded else 'âŒ'}")
    else:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        return None
    
    return system

def demo_input_processing():
    """æ¼”ç¤ºè¾“å…¥å¤„ç†æµç¨‹"""
    print("\nğŸ¯ æ¼”ç¤º2: è¾“å…¥å¤„ç†æµç¨‹")
    print("="*50)
    
    # åˆ›å»ºç³»ç»Ÿ
    system = create_llm_system('development')
    system.initialize()
    
    # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„RAGå¤„ç†å™¨è¾“å‡º
    test_cases = [
        {
            'name': 'Directå¤„ç†å™¨ - ç®€å•é—®ç­”',
            'query': 'ç§‘æ¯”çš„èº«é«˜æ˜¯å¤šå°‘ï¼Ÿ',
            'processor_output': {
                'textual_context': {
                    'formatted_text': 'ç§‘æ¯”Â·å¸ƒè±æ©ç‰¹èº«é«˜6è‹±å°º6è‹±å¯¸ï¼ˆ198å˜ç±³ï¼‰ï¼Œæ˜¯ä¸€åå¾—åˆ†åå«ã€‚'
                },
                'metadata': {'processor_name': 'direct_processor', 'confidence': 0.95},
                'success': True
            }
        },
        {
            'name': 'Simple Gå¤„ç†å™¨ - å›¾è°±å¢å¼º',
            'query': 'ç§‘æ¯”å’Œæ²™å…‹çš„å…³ç³»å¦‚ä½•ï¼Ÿ',
            'processor_output': {
                'textual_context': {
                    'formatted_text': 'ç§‘æ¯”Â·å¸ƒè±æ©ç‰¹å’Œæ²™å¥‡å°”Â·å¥¥å°¼å°”åœ¨æ´›æ‰çŸ¶æ¹–äººé˜Ÿæ˜¯é˜Ÿå‹ï¼Œä»–ä»¬ä¸€èµ·è·å¾—äº†ä¸‰è¿å† ï¼ˆ2000-2002ï¼‰ã€‚'
                },
                'graph': {
                    'nodes': [
                        {'id': 'kobe', 'name': 'ç§‘æ¯”', 'type': 'player'},
                        {'id': 'shaq', 'name': 'æ²™å…‹', 'type': 'player'},
                        {'id': 'lakers', 'name': 'æ¹–äºº', 'type': 'team'}
                    ],
                    'edges': [
                        {'source': 'kobe', 'target': 'lakers', 'relation': 'plays_for'},
                        {'source': 'shaq', 'target': 'lakers', 'relation': 'plays_for'},
                        {'source': 'kobe', 'target': 'shaq', 'relation': 'teammate'}
                    ]
                },
                'metadata': {'processor_name': 'simple_g_processor'},
                'success': True
            }
        },
        {
            'name': 'Complex Gå¤„ç†å™¨ - å¤šæ¨¡æ€å¢å¼º',
            'query': 'åˆ†æç§‘æ¯”å’Œè©¹å§†æ–¯çš„èŒä¸šæˆå°±å¯¹æ¯”',
            'processor_output': {
                'mode': 'enhanced',
                'traditional_result': {
                    'textual_context': {
                        'formatted_text': 'ç§‘æ¯”è·å¾—5æ¬¡æ€»å† å†›ã€18æ¬¡å…¨æ˜æ˜Ÿï¼Œè©¹å§†æ–¯è·å¾—4æ¬¡æ€»å† å†›ã€19æ¬¡å…¨æ˜æ˜Ÿã€‚ä¸¤äººéƒ½æ˜¯NBAå†å²ä¸Šçš„ä¼ å¥‡çƒå‘˜ã€‚'
                    }
                },
                'graph_embedding': {
                    'embedding': [0.23, -0.15, 0.67, 0.42, -0.31, 0.89, -0.56, 0.78, 0.12, -0.44],
                    'encoding_success': True
                },
                'multimodal_context': None,  # åœ¨å®é™…åº”ç”¨ä¸­ä¼šæœ‰MultimodalContextå¯¹è±¡
                'metadata': {'processor_name': 'complex_g_processor'},
                'success': True
            }
        },
        {
            'name': 'Comparisonå¤„ç†å™¨ - æ¯”è¾ƒåˆ†æ',
            'query': 'æ¯”è¾ƒç§‘æ¯”å’Œä¹”ä¸¹çš„å¾—åˆ†èƒ½åŠ›',
            'processor_output': {
                'comparison_result': {
                    'formatted_text': 'ç§‘æ¯”èŒä¸šç”Ÿæ¶¯æ€»å¾—åˆ†33,643åˆ†ï¼Œåœºå‡25.0åˆ†ï¼›ä¹”ä¸¹èŒä¸šç”Ÿæ¶¯æ€»å¾—åˆ†32,292åˆ†ï¼Œåœºå‡30.1åˆ†ã€‚ä¹”ä¸¹çš„åœºå‡å¾—åˆ†æ›´é«˜ï¼Œä½†ç§‘æ¯”çš„æ€»å¾—åˆ†æ›´å¤šã€‚'
                },
                'comparison_subjects': ['ç§‘æ¯”', 'ä¹”ä¸¹'],
                'comparison_aspects': ['å¾—åˆ†èƒ½åŠ›', 'æ•ˆç‡', 'èŒä¸šç”Ÿæ¶¯é•¿åº¦'],
                'metadata': {'processor_name': 'comparison_processor'},
                'success': True
            }
        },
        {
            'name': 'Chitchatå¤„ç†å™¨ - é—²èŠäº’åŠ¨',
            'query': 'ä½ è§‰å¾—ç¯®çƒæ¯”èµ›æœ€ç²¾å½©çš„æ˜¯ä»€ä¹ˆï¼Ÿ',
            'processor_output': {
                'response': 'ç¯®çƒæ¯”èµ›æœ€ç²¾å½©çš„æ˜¯é‚£äº›å…³é”®æ—¶åˆ»çš„ç»æ€ï¼Œè¿˜æœ‰çƒå‘˜ä¹‹é—´çš„ç²¾å¦™é…åˆã€‚',
                'metadata': {'processor_name': 'chitchat_processor'},
                'success': True
            }
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n   ğŸ“ æ¡ˆä¾‹ {i}: {test_case['name']}")
        print(f"   æŸ¥è¯¢: {test_case['query']}")
        
        # è¾“å…¥è·¯ç”±
        unified_input = system.input_router.route_processor_output(
            test_case['processor_output'], 
            test_case['query']
        )
        
        print(f"   âœ… è·¯ç”±å®Œæˆ: {unified_input.processor_type}")
        print(f"   å¤šæ¨¡æ€æ•°æ®: {'æœ‰' if unified_input.has_multimodal_data() else 'æ— '}")
        print(f"   æ–‡æœ¬å†…å®¹é•¿åº¦: {len(unified_input.get_text_content())}")
        
        # Promptç”Ÿæˆ
        prompt = system.prompt_manager.format_prompt_for_input(
            unified_input, 
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¯®çƒçŸ¥è¯†é—®ç­”åŠ©æ‰‹"
        )
        print(f"   âœ… Promptç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(prompt)}")
    
    # æ˜¾ç¤ºè·¯ç”±å™¨ç»Ÿè®¡
    stats = system.input_router.get_stats()
    print(f"\nğŸ“Š è¾“å…¥è·¯ç”±å™¨ç»Ÿè®¡:")
    print(f"   æ€»å¤„ç†æ•°: {stats['total_processed']}")
    print(f"   å¤„ç†å™¨åˆ†å¸ƒ: {stats['processor_distribution']}")
    print(f"   å¤šæ¨¡æ€æ¯”ä¾‹: {stats['multimodal_ratio']:.2%}")

def demo_prompt_templates():
    """æ¼”ç¤ºPromptæ¨¡æ¿ç³»ç»Ÿ"""
    print("\nğŸ¯ æ¼”ç¤º3: Promptæ¨¡æ¿ç³»ç»Ÿ")
    print("="*50)
    
    # åˆ›å»ºç³»ç»Ÿ
    system = create_llm_system('development')
    system.initialize()
    
    # è·å–æ¨¡æ¿ç®¡ç†å™¨
    manager = system.prompt_manager
    
    print(f"ğŸ“ æ¨¡æ¿ç®¡ç†å™¨ä¿¡æ¯:")
    print(f"   æ€»æ¨¡æ¿æ•°: {len(manager.templates)}")
    print(f"   å¯ç”¨æ¨¡æ¿: {manager.list_templates()}")
    
    # å±•ç¤ºæ¯ç§æ¨¡æ¿çš„è¯¦ç»†ä¿¡æ¯
    template_names = ['simple_qa', 'multimodal_qa', 'comparison_qa', 'chitchat']
    
    for template_name in template_names:
        print(f"\n   ğŸ“‹ æ¨¡æ¿: {template_name}")
        info = manager.get_template_info(template_name)
        if info:
            print(f"      ç±»å‹: {info['type']}")
            print(f"      å¿…éœ€å­—æ®µ: {info['required_fields']}")
            print(f"      å¯é€‰å­—æ®µ: {info['optional_fields']}")
            print(f"      æè¿°: {info['description']}")
            print(f"      æœ‰æ•ˆæ€§: {'âœ…' if info['is_valid'] else 'âŒ'}")
    
    # æ¼”ç¤ºè‡ªå®šä¹‰Promptç”Ÿæˆ
    print(f"\nğŸ”§ è‡ªå®šä¹‰Promptç”Ÿæˆæ¼”ç¤º:")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    unified_input = UnifiedInput(
        query="ç§‘æ¯”è·å¾—è¿‡å“ªäº›è£èª‰ï¼Ÿ",
        processor_type="complex_g",
        text_context="ç§‘æ¯”åœ¨NBAç”Ÿæ¶¯ä¸­è·å¾—äº†ä¼—å¤šè£èª‰ï¼ŒåŒ…æ‹¬æ€»å† å†›ã€å…¨æ˜æ˜Ÿã€MVPç­‰ã€‚",
        graph_embedding=[0.1, 0.2, 0.3, 0.4, 0.5]
    )
    
    # ç”Ÿæˆå¤šæ¨¡æ€Prompt
    prompt = manager.format_prompt_for_input(
        unified_input, 
        "ä½ æ˜¯NBAå†å²ä¸“å®¶ï¼Œæ“…é•¿åˆ†æçƒå‘˜æˆå°±"
    )
    
    print(f"   âœ… å¤šæ¨¡æ€Promptç”ŸæˆæˆåŠŸ")
    print(f"   é•¿åº¦: {len(prompt)}")
    print(f"   é¢„è§ˆ:")
    print("   " + "-" * 40)
    print("   " + prompt[:200].replace('\n', '\n   ') + "...")
    print("   " + "-" * 40)

def demo_response_formatting():
    """æ¼”ç¤ºå“åº”æ ¼å¼åŒ–"""
    print("\nğŸ¯ æ¼”ç¤º4: å“åº”æ ¼å¼åŒ–")
    print("="*50)
    
    # åˆ›å»ºç³»ç»Ÿ
    system = create_llm_system('development')
    system.initialize()
    
    # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„LLMå“åº”
    test_responses = [
        {
            'name': 'ç®€å•é—®ç­”å“åº”',
            'llm_response': LLMResponse(
                content="ç§‘æ¯”Â·å¸ƒè±æ©ç‰¹èº«é«˜6è‹±å°º6è‹±å¯¸ï¼Œçº¦198å˜ç±³ã€‚ä»–æ˜¯ä¸€åå¾—åˆ†åå«ï¼Œåœ¨NBAç”Ÿæ¶¯ä¸­ä»¥å…¶å‡ºè‰²çš„å¾—åˆ†èƒ½åŠ›è€Œé—»åã€‚",
                metadata={'model': 'phi-3-mini', 'temperature': 0.7},
                processing_time=1.2,
                token_usage={'input_tokens': 45, 'output_tokens': 35, 'total_tokens': 80}
            ),
            'unified_input': UnifiedInput(
                query="ç§‘æ¯”çš„èº«é«˜æ˜¯å¤šå°‘ï¼Ÿ",
                processor_type="direct",
                text_context="ç§‘æ¯”åŸºæœ¬ä¿¡æ¯"
            )
        },
        {
            'name': 'æ¯”è¾ƒåˆ†æå“åº”',
            'llm_response': LLMResponse(
                content="ç§‘æ¯”å’Œä¹”ä¸¹åœ¨å¾—åˆ†æ–¹é¢å„æœ‰ç‰¹è‰²ã€‚1. åœºå‡å¾—åˆ†ï¼šä¹”ä¸¹30.1åˆ†ï¼Œç§‘æ¯”25.0åˆ†ã€‚2. æ€»å¾—åˆ†ï¼šç§‘æ¯”33,643åˆ†ï¼Œä¹”ä¸¹32,292åˆ†ã€‚3. å¾—åˆ†æ•ˆç‡ï¼šä¹”ä¸¹æ›´é«˜ã€‚4. èŒä¸šç”Ÿæ¶¯é•¿åº¦ï¼šç§‘æ¯”æ›´é•¿ã€‚",
                metadata={'model': 'phi-3-mini', 'temperature': 0.7},
                processing_time=2.8,
                token_usage={'input_tokens': 120, 'output_tokens': 85, 'total_tokens': 205}
            ),
            'unified_input': UnifiedInput(
                query="æ¯”è¾ƒç§‘æ¯”å’Œä¹”ä¸¹çš„å¾—åˆ†èƒ½åŠ›",
                processor_type="comparison",
                text_context="ç§‘æ¯”å’Œä¹”ä¸¹çš„å¾—åˆ†æ•°æ®æ¯”è¾ƒ"
            )
        },
        {
            'name': 'å¤šæ¨¡æ€åˆ†æå“åº”',
            'llm_response': LLMResponse(
                content="åŸºäºå›¾è°±åˆ†æå’Œæ–‡æœ¬ä¿¡æ¯ï¼Œç§‘æ¯”å’Œè©¹å§†æ–¯éƒ½æ˜¯NBAå†å²ä¸Šçš„ä¼ å¥‡çƒå‘˜ã€‚ç§‘æ¯”åœ¨æ¹–äººé˜Ÿè·å¾—5æ¬¡æ€»å† å†›ï¼Œè©¹å§†æ–¯åˆ™åœ¨å¤šæ”¯çƒé˜Ÿéƒ½å–å¾—äº†æˆåŠŸã€‚ä»–ä»¬çš„å…³ç³»ä»æ—©æœŸçš„ç«äº‰å‘å±•ä¸ºåæ¥çš„ç›¸äº’å°Šé‡ã€‚",
                metadata={'model': 'phi-3-mini', 'temperature': 0.7, 'multimodal': True},
                processing_time=3.5,
                token_usage={'input_tokens': 180, 'output_tokens': 120, 'total_tokens': 300}
            ),
            'unified_input': UnifiedInput(
                query="åˆ†æç§‘æ¯”å’Œè©¹å§†æ–¯çš„å…³ç³»",
                processor_type="complex_g",
                text_context="ç§‘æ¯”å’Œè©¹å§†æ–¯çš„èŒä¸šç”Ÿæ¶¯åˆ†æ",
                graph_embedding=[0.1, 0.2, 0.3, 0.4, 0.5]
            )
        }
    ]
    
    for i, test_case in enumerate(test_responses, 1):
        print(f"\n   ğŸ“ æ¡ˆä¾‹ {i}: {test_case['name']}")
        
        # æ ¼å¼åŒ–å“åº”
        formatted_response = system.response_formatter.format_response(
            test_case['llm_response'],
            test_case['unified_input']
        )
        
        print(f"   âœ… æ ¼å¼åŒ–å®Œæˆ")
        print(f"   æ ¼å¼ç±»å‹: {formatted_response.format_type}")
        print(f"   åŸå§‹é•¿åº¦: {len(test_case['llm_response'].content)}")
        print(f"   æ ¼å¼åŒ–åé•¿åº¦: {len(formatted_response.content)}")
        print(f"   å¤„ç†æ—¶é—´: {formatted_response.processing_info['formatting_time']:.3f}ç§’")
        
        # æ˜¾ç¤ºæ ¼å¼åŒ–åçš„å†…å®¹
        print(f"   å†…å®¹é¢„è§ˆ:")
        preview = formatted_response.content[:150].replace('\n', '\n      ')
        print(f"      {preview}...")
    
    # æ˜¾ç¤ºæ ¼å¼åŒ–å™¨ç»Ÿè®¡
    stats = system.response_formatter.get_stats()
    print(f"\nğŸ“Š å“åº”æ ¼å¼åŒ–å™¨ç»Ÿè®¡:")
    print(f"   æ€»æ ¼å¼åŒ–æ•°: {stats['total_formatted']}")
    print(f"   å¹³å‡å¤„ç†æ—¶é—´: {stats['performance']['avg_formatting_time']:.4f}ç§’")
    print(f"   æˆªæ–­ç‡: {stats['quality_metrics']['truncation_rate']:.2%}")

def demo_configuration_options():
    """æ¼”ç¤ºé…ç½®é€‰é¡¹"""
    print("\nğŸ¯ æ¼”ç¤º5: é…ç½®é€‰é¡¹å’Œè‡ªå®šä¹‰")
    print("="*50)
    
    from app.llm.factory import llm_factory
    
    # æ˜¾ç¤ºæ‰€æœ‰é¢„è®¾é…ç½®
    presets = llm_factory.list_presets()
    print(f"ğŸ“‹ å¯ç”¨é¢„è®¾é…ç½®:")
    
    for preset in presets:
        info = llm_factory.get_preset_info(preset)
        if info:
            print(f"   ğŸ”§ {preset}:")
            print(f"      æ¨¡å‹: {info['model_name']}")
            print(f"      è®¾å¤‡: {info['device']}")
            print(f"      å¤šæ¨¡æ€: {info['multimodal_enabled']}")
            print(f"      è‡ªåŠ¨åŠ è½½: {info['auto_load_model']}")
            print(f"      æè¿°: {info['description']}")
    
    # æ¼”ç¤ºè‡ªå®šä¹‰é…ç½®
    print(f"\nğŸ›ï¸ è‡ªå®šä¹‰é…ç½®æ¼”ç¤º:")
    
    custom_config = {
        'llm_config': {
            'max_input_tokens': 2048,
            'temperature': 0.5,
            'system_prompt': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„NBAåˆ†æå¸ˆ'
        },
        'formatter_config': {
            'enable_markdown': True,
            'highlight_keywords': True,
            'max_response_length': 1024
        },
        'auto_load_model': False
    }
    
    print(f"   è‡ªå®šä¹‰é…ç½®:")
    print(f"   {json.dumps(custom_config, indent=6, ensure_ascii=False)}")
    
    # åˆ›å»ºè‡ªå®šä¹‰ç³»ç»Ÿ
    custom_system = llm_factory.create_system('macos_optimized', custom_config)
    custom_system.initialize()
    
    print(f"   âœ… è‡ªå®šä¹‰ç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
    print(f"   æœ€å¤§è¾“å…¥tokens: {custom_system.config.llm_config.max_input_tokens}")
    print(f"   ç³»ç»Ÿprompt: {custom_system.config.llm_config.system_prompt}")

def demo_system_status():
    """æ¼”ç¤ºç³»ç»ŸçŠ¶æ€ç›‘æ§"""
    print("\nğŸ¯ æ¼”ç¤º6: ç³»ç»ŸçŠ¶æ€ç›‘æ§")
    print("="*50)
    
    # åˆ›å»ºç³»ç»Ÿ
    system = create_llm_system('macos_optimized')
    system.initialize()
    
    # è·å–å®Œæ•´ç³»ç»ŸçŠ¶æ€
    status = system.get_system_status()
    
    print(f"ğŸ–¥ï¸ ç³»ç»Ÿæ•´ä½“çŠ¶æ€:")
    print(f"   åˆå§‹åŒ–: {'âœ…' if status['initialized'] else 'âŒ'}")
    print(f"   å°±ç»ª: {'âœ…' if status['ready'] else 'âŒ'}")
    
    print(f"\nğŸ”§ ç»„ä»¶çŠ¶æ€:")
    for component, loaded in status['components'].items():
        print(f"   {component}: {'âœ…' if loaded else 'âŒ'}")
    
    # æ˜¾ç¤ºå„ç»„ä»¶è¯¦ç»†ç»Ÿè®¡
    if 'engine_stats' in status:
        engine_stats = status['engine_stats']
        print(f"\nğŸ¤– LLMå¼•æ“çŠ¶æ€:")
        print(f"   æ¨¡å‹åŠ è½½: {'âœ…' if engine_stats['model_loaded'] else 'âŒ'}")
        print(f"   æ¨¡å‹åç§°: {engine_stats['model_name']}")
        print(f"   è®¾å¤‡: {engine_stats['device']}")
        print(f"   å¤šæ¨¡æ€å¯ç”¨: {'âœ…' if engine_stats['multimodal_enabled'] else 'âŒ'}")
        print(f"   å›¾æŠ•å½±å™¨å¯ç”¨: {'âœ…' if engine_stats['graph_projector_enabled'] else 'âŒ'}")
    
    if 'router_stats' in status:
        router_stats = status['router_stats']
        print(f"\nğŸš¦ è¾“å…¥è·¯ç”±å™¨çŠ¶æ€:")
        print(f"   æ€»å¤„ç†æ•°: {router_stats['total_processed']}")
        print(f"   æ”¯æŒå¤„ç†å™¨: {router_stats['supported_processors']}")
    
    if 'formatter_stats' in status:
        formatter_stats = status['formatter_stats']
        print(f"\nğŸ“ å“åº”æ ¼å¼åŒ–å™¨çŠ¶æ€:")
        print(f"   æ€»æ ¼å¼åŒ–æ•°: {formatter_stats['total_formatted']}")
        print(f"   é…ç½®: {formatter_stats['configuration']}")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ Phase 1 LLMé›†æˆæ¼”ç¤º")
    print("åŸºäºG-Retrieverè®ºæ–‡çš„ä¸‰é˜¶æ®µLLMé›†æˆæ–¹æ¡ˆ")
    print("=" * 60)
    print()
    
    try:
        # æ¼”ç¤º1: ç³»ç»Ÿåˆ›å»º
        system = demo_system_creation()
        if not system:
            print("âŒ ç³»ç»Ÿåˆ›å»ºå¤±è´¥ï¼Œç»ˆæ­¢æ¼”ç¤º")
            return
        
        # æ¼”ç¤º2: è¾“å…¥å¤„ç†
        demo_input_processing()
        
        # æ¼”ç¤º3: Promptæ¨¡æ¿
        demo_prompt_templates()
        
        # æ¼”ç¤º4: å“åº”æ ¼å¼åŒ–
        demo_response_formatting()
        
        # æ¼”ç¤º5: é…ç½®é€‰é¡¹
        demo_configuration_options()
        
        # æ¼”ç¤º6: ç³»ç»ŸçŠ¶æ€
        demo_system_status()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ Phase 1 LLMé›†æˆæ¼”ç¤ºå®Œæˆï¼")
        print()
        print("ğŸ“‹ Phase 1 æ ¸å¿ƒåŠŸèƒ½æ€»ç»“:")
        print("   âœ… ç»Ÿä¸€LLMæ¶æ„ - æ”¯æŒPhi-3-miniæ¨¡å‹")
        print("   âœ… ç»Ÿä¸€è¾“å…¥æ¥å£ - å¤„ç†5ç§RAGå¤„ç†å™¨è¾“å‡º")
        print("   âœ… Promptæ¨¡æ¿ç³»ç»Ÿ - é€‚é…ä¸åŒæŸ¥è¯¢ç±»å‹")
        print("   âœ… å“åº”æ ¼å¼åŒ– - ç»Ÿä¸€è¾“å‡ºæ ¼å¼")
        print("   âœ… é…ç½®ç®¡ç† - å¤šç§é¢„è®¾å’Œè‡ªå®šä¹‰é€‰é¡¹")
        print("   âœ… å·¥å‚æ¨¡å¼ - ä¾¿æ·çš„ç»„ä»¶åˆ›å»º")
        print("   âœ… çŠ¶æ€ç›‘æ§ - å®Œæ•´çš„ç³»ç»ŸçŠ¶æ€è·Ÿè¸ª")
        print()
        print("ğŸ¯ Phase 1 ç›®æ ‡è¾¾æˆ:")
        print("   âœ… æ­å»ºäº†Phi-3-miniåŸºç¡€æ¡†æ¶")
        print("   âœ… å®ç°äº†ç»Ÿä¸€è¾“å…¥æ¥å£")
        print("   âœ… å®Œæˆäº†ç®€å•å¤„ç†å™¨çš„LLMå¯¹æ¥")
        print("   âœ… ä¸ºPhase 2å¤šæ¨¡æ€èåˆå¥ å®šäº†åŸºç¡€")
        print()
        print("ğŸš€ Ready for Phase 2: å¤šæ¨¡æ€èåˆæœºåˆ¶å®ç°ï¼")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
