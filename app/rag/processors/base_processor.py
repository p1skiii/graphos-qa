"""
RAG å¤„ç†å™¨åŸºç±»
å®šä¹‰æ‰€æœ‰å¤„ç†å™¨çš„é€šç”¨æ¥å£å’ŒåŠŸèƒ½
"""
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import time
import logging
from app.rag.cache_manager import CacheManager
from app.rag.components import component_factory, ProcessorConfig

logger = logging.getLogger(__name__)

class BaseProcessor(ABC):
    """RAGå¤„ç†å™¨åŸºç±»"""
    
    def __init__(self, config: ProcessorConfig):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.config = config
        self.processor_name = config.processor_name
        
        # ç»„ä»¶å®ä¾‹
        self.retriever = None
        self.graph_builder = None
        self.textualizer = None
        
        # ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = None
        if config.cache_enabled:
            self.cache_manager = CacheManager({
                'memory_size': 1000,
                'disk_cache_dir': 'cache',
                'default_ttl': config.cache_ttl
            })
        
        self.is_initialized = False
        self.stats = {
            'queries_processed': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'avg_processing_time': 0.0,
            'errors': 0
        }
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        try:
            logger.info(f"ğŸ”„ åˆå§‹åŒ–å¤„ç†å™¨: {self.processor_name}")
            
            # åˆ›å»ºç»„ä»¶
            components = component_factory.create_processor_components(self.config)
            
            self.retriever = components['retriever']
            self.graph_builder = components['graph_builder']
            self.textualizer = components['textualizer']
            
            # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
            if self.cache_manager:
                # CacheManagerä¸éœ€è¦å•ç‹¬çš„initializeæ–¹æ³•ï¼Œåœ¨æ„é€ æ—¶å·²ç»åˆå§‹åŒ–
                logger.info(f"âœ… ç¼“å­˜ç®¡ç†å™¨å·²å°±ç»ªï¼Œå¤„ç†å™¨ {self.processor_name} å°†ä½¿ç”¨ç¼“å­˜")
            else:
                logger.info(f"ğŸ“ å¤„ç†å™¨ {self.processor_name} æœªå¯ç”¨ç¼“å­˜")
            
            self.is_initialized = True
            logger.info(f"âœ… å¤„ç†å™¨ {self.processor_name} åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å™¨ {self.processor_name} åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def process(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """å¤„ç†æŸ¥è¯¢"""
        if not self.is_initialized:
            raise RuntimeError(f"å¤„ç†å™¨ {self.processor_name} æœªåˆå§‹åŒ–")
        
        start_time = time.time()
        
        try:
            # æ„å»ºç¼“å­˜é”®
            cache_key = None
            if self.cache_manager:
                cache_key = self._build_cache_key(query, context)
                
                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = self.cache_manager.get(cache_key)
                if cached_result is not None:
                    self.stats['cache_hits'] += 1
                    self.stats['queries_processed'] += 1
                    
                    logger.info(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {self.processor_name}")
                    return self._add_metadata(cached_result, from_cache=True)
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå¤„ç†
            if self.cache_manager:
                self.stats['cache_misses'] += 1
            
            # è°ƒç”¨å…·ä½“å¤„ç†é€»è¾‘
            result = self._process_impl(query, context)
            
            # ç¼“å­˜ç»“æœ
            if self.cache_manager and cache_key:
                self.cache_manager.set(cache_key, result)
            
            # æ›´æ–°ç»Ÿè®¡
            processing_time = time.time() - start_time
            self._update_stats(processing_time)
            
            return self._add_metadata(result, processing_time=processing_time)
            
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"âŒ å¤„ç†å™¨ {self.processor_name} å¤„ç†å¤±è´¥: {str(e)}")
            
            return {
                'success': False,
                'error': str(e),
                'processor': self.processor_name,
                'query': query,
                'context': context or {},
                'timestamp': time.time()
            }
    
    @abstractmethod
    def _process_impl(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """å…·ä½“å¤„ç†é€»è¾‘ï¼ˆç”±å­ç±»å®ç°ï¼‰"""
        pass
    
    def _build_cache_key(self, query: str, context: Optional[Dict[str, Any]]) -> str:
        """æ„å»ºç¼“å­˜é”®"""
        context_str = str(sorted(context.items())) if context else ""
        return f"{query}|{context_str}"
    
    def _add_metadata(self, result: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """æ·»åŠ å…ƒæ•°æ®"""
        if isinstance(result, dict):
            result = result.copy()
            result.update({
                'processor': self.processor_name,
                'timestamp': time.time(),
                **kwargs
            })
        return result
    
    def _update_stats(self, processing_time: float):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['queries_processed'] += 1
        
        # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
        total_queries = self.stats['queries_processed']
        current_avg = self.stats['avg_processing_time']
        self.stats['avg_processing_time'] = (
            (current_avg * (total_queries - 1) + processing_time) / total_queries
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        cache_stats = {}
        if self.cache_manager:
            cache_stats = self.cache_manager.get_stats()
        
        return {
            'processor_name': self.processor_name,
            'is_initialized': self.is_initialized,
            'processing_stats': self.stats.copy(),
            'cache_stats': cache_stats,
            'config': {
                'cache_enabled': self.config.cache_enabled,
                'cache_ttl': self.config.cache_ttl,
                'max_tokens': self.config.max_tokens
            }
        }
    
    def clear_cache(self) -> bool:
        """æ¸…ç©ºç¼“å­˜"""
        if self.cache_manager:
            return self.cache_manager.clear()
        return True
    
    def get_component_info(self) -> Dict[str, str]:
        """è·å–ç»„ä»¶ä¿¡æ¯"""
        return {
            'retriever': self.config.retriever_config.component_name,
            'graph_builder': self.config.graph_builder_config.component_name,
            'textualizer': self.config.textualizer_config.component_name
        }

# =============================================================================
# é€šç”¨å¤„ç†æµç¨‹è¾…åŠ©æ–¹æ³•
# =============================================================================

class ProcessorUtils:
    """å¤„ç†å™¨å·¥å…·ç±»"""
    
    @staticmethod
    def extract_entities_from_query(query: str) -> Dict[str, Any]:
        """ä»æŸ¥è¯¢ä¸­æå–å®ä½“ä¿¡æ¯"""
        entities = {
            'players': [],
            'teams': [],
            'attributes': [],
            'numbers': []
        }
        
        query_lower = query.lower()
        
        # ç®€å•çš„å®ä½“æå–é€»è¾‘
        import re
        
        # æå–æ•°å­—
        numbers = re.findall(r'\d+', query)
        entities['numbers'] = numbers
        
        # æ‰©å±•çš„çƒå‘˜åç§°åº“ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        player_names = [
            # è‹±æ–‡åç§°
            'yao ming', 'kobe bryant', 'lebron james', 'michael jordan', 'stephen curry',
            'kevin durant', 'james harden', 'russell westbrook', 'chris paul', 'carmelo anthony',
            'tracy mcgrady', 'dwight howard', 'shaquille oneal', 'tim duncan', 'magic johnson',
            'larry bird', 'kareem abdul-jabbar', 'wilt chamberlain', 'bill russell',
            # ç®€çŸ­ç‰ˆæœ¬
            'yao', 'kobe', 'lebron', 'jordan', 'curry', 'durant', 'harden', 'westbrook',
            'paul', 'anthony', 'mcgrady', 'howard', 'shaq', 'duncan', 'magic',
            # ä¸­æ–‡åç§°
            'å§šæ˜', 'ç§‘æ¯”', 'è©¹å§†æ–¯', 'ä¹”ä¸¹', 'åº“é‡Œ', 'æœå…°ç‰¹', 'å“ˆç™»', 'å¨å°‘'
        ]
        
        # æ‰©å±•çš„çƒé˜Ÿåç§°åº“ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        team_names = [
            # è‹±æ–‡åç§°
            'lakers', 'warriors', 'bulls', 'celtics', 'heat', 'spurs', 'rockets', 'nets',
            'thunder', 'clippers', 'mavericks', 'knicks', 'hawks', 'pacers', 'cavaliers',
            'los angeles lakers', 'golden state warriors', 'chicago bulls', 'boston celtics',
            'miami heat', 'san antonio spurs', 'houston rockets', 'brooklyn nets',
            # ä¸­æ–‡åç§°
            'æ¹–äºº', 'å‹‡å£«', 'å…¬ç‰›', 'å‡¯å°”ç‰¹äºº', 'çƒ­ç«', 'é©¬åˆº', 'ç«ç®­', 'ç¯®ç½‘', 'é›·éœ†'
        ]
        
        # æ£€æŸ¥çƒå‘˜åç§°
        for player in player_names:
            if player in query_lower:
                entities['players'].append(player)
        
        # æ£€æŸ¥çƒé˜Ÿåç§°
        for team in team_names:
            if team in query_lower:
                entities['teams'].append(team)
        
        # æ‰©å±•çš„å±æ€§å…³é”®è¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        attribute_keywords = [
            # ä¸­æ–‡
            'å¹´é¾„', 'èº«é«˜', 'ä½“é‡', 'å¾—åˆ†', 'åŠ©æ”»', 'ç¯®æ¿', 'ä½ç½®', 'çƒè¡£å·ç ',
            # è‹±æ–‡
            'age', 'old', 'height', 'tall', 'weight', 'position', 'jersey', 'number',
            'stats', 'points', 'assists', 'rebounds', 'born', 'birthday'
        ]
        
        for attr in attribute_keywords:
            if attr in query_lower:
                entities['attributes'].append(attr)
        
        return entities
    
    @staticmethod
    def validate_subgraph(subgraph: Dict[str, Any]) -> bool:
        """éªŒè¯å­å›¾ç»“æ„"""
        if not isinstance(subgraph, dict):
            return False
        
        required_keys = ['nodes', 'edges', 'node_count', 'edge_count']
        for key in required_keys:
            if key not in subgraph:
                return False
        
        # éªŒè¯èŠ‚ç‚¹å’Œè¾¹æ˜¯å¦ä¸ºåˆ—è¡¨
        if not isinstance(subgraph['nodes'], list):
            return False
        if not isinstance(subgraph['edges'], list):
            return False
        
        return True
    
    @staticmethod
    def limit_tokens(text: str, max_tokens: int) -> str:
        """é™åˆ¶æ–‡æœ¬é•¿åº¦"""
        if len(text) <= max_tokens:
            return text
        
        # ç®€å•æˆªæ–­å¹¶æ·»åŠ çœç•¥å·
        truncated = text[:max_tokens - 3]
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
        last_period = truncated.rfind('ã€‚')
        last_newline = truncated.rfind('\n')
        
        cut_point = max(last_period, last_newline)
        if cut_point > max_tokens * 0.8:  # å¦‚æœæˆªæ–­ç‚¹ä¸ä¼šæŸå¤±å¤ªå¤šå†…å®¹
            truncated = truncated[:cut_point + 1]
        
        return truncated + "..."
    
    @staticmethod
    def merge_results(results: list, max_items: int = 10) -> list:
        """åˆå¹¶å¤šä¸ªç»“æœåˆ—è¡¨"""
        if not results:
            return []
        
        # å»é‡å¹¶åˆå¹¶
        seen = set()
        merged = []
        
        for result_list in results:
            if not isinstance(result_list, list):
                continue
                
            for item in result_list:
                if isinstance(item, dict):
                    # ä½¿ç”¨node_idæˆ–idä½œä¸ºå»é‡é”®
                    key = item.get('node_id') or item.get('id') or str(item)
                    if key not in seen:
                        seen.add(key)
                        merged.append(item)
                        
                        if len(merged) >= max_items:
                            return merged
        
        return merged
