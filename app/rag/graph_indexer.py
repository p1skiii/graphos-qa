"""
G-Retriever å›¾ç´¢å¼•å™¨
ç”¨äºé«˜æ•ˆçš„èŠ‚ç‚¹ç´¢å¼•å’Œå‘é‡å­˜å‚¨ç®¡ç†
"""
import pickle
import json
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import faiss
from sentence_transformers import SentenceTransformer
from app.database import NebulaGraphConnection
from config import Config
import logging

logger = logging.getLogger(__name__)

class GraphIndexer:
    """å›¾ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(self, index_dir: str = "data/indexes"):
        """åˆå§‹åŒ–ç´¢å¼•å™¨"""
        self.index_dir = Path(index_dir)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        # æ ¸å¿ƒç»„ä»¶
        self.embedding_model = None
        self.nebula_conn = NebulaGraphConnection()
        
        # ç´¢å¼•å­˜å‚¨
        self.node_index = None      # FAISSç´¢å¼•
        self.edge_index = None      # FAISSç´¢å¼•
        self.node_metadata = {}     # èŠ‚ç‚¹å…ƒæ•°æ®
        self.edge_metadata = {}     # è¾¹å…ƒæ•°æ®
        self.id_to_index = {}       # IDåˆ°ç´¢å¼•çš„æ˜ å°„
        self.index_to_id = {}       # ç´¢å¼•åˆ°IDçš„æ˜ å°„
        
        # é…ç½®
        self.embedding_dim = 384    # sentence-transformersé»˜è®¤ç»´åº¦
        self.is_initialized = False
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–ç´¢å¼•å™¨"""
        try:
            logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–å›¾ç´¢å¼•å™¨...")
            
            # åŠ è½½åµŒå…¥æ¨¡å‹
            self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL)
            self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
            logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»´åº¦: {self.embedding_dim}")
            
            # è¿æ¥å›¾æ•°æ®åº“
            if not self.nebula_conn.connect():
                logger.error("âŒ NebulaGraphè¿æ¥å¤±è´¥")
                return False
            
            # å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
            if self._load_existing_indexes():
                logger.info("âœ… æˆåŠŸåŠ è½½ç°æœ‰ç´¢å¼•")
            else:
                logger.info("ğŸ—ï¸  å¼€å§‹æ„å»ºæ–°ç´¢å¼•...")
                self._build_indexes()
                self._save_indexes()
                logger.info("âœ… æ–°ç´¢å¼•æ„å»ºå®Œæˆ")
            
            self.is_initialized = True
            logger.info("âœ… å›¾ç´¢å¼•å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å›¾ç´¢å¼•å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def _load_existing_indexes(self) -> bool:
        """åŠ è½½ç°æœ‰ç´¢å¼•"""
        try:
            # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            node_index_path = self.index_dir / "node_index.faiss"
            edge_index_path = self.index_dir / "edge_index.faiss"
            node_metadata_path = self.index_dir / "node_metadata.pkl"
            edge_metadata_path = self.index_dir / "edge_metadata.pkl"
            mappings_path = self.index_dir / "mappings.json"
            
            if not all(p.exists() for p in [node_index_path, edge_index_path, 
                                           node_metadata_path, edge_metadata_path, 
                                           mappings_path]):
                return False
            
            # åŠ è½½FAISSç´¢å¼•
            self.node_index = faiss.read_index(str(node_index_path))
            self.edge_index = faiss.read_index(str(edge_index_path))
            
            # åŠ è½½å…ƒæ•°æ®
            with open(node_metadata_path, 'rb') as f:
                self.node_metadata = pickle.load(f)
            
            with open(edge_metadata_path, 'rb') as f:
                self.edge_metadata = pickle.load(f)
            
            # åŠ è½½æ˜ å°„
            with open(mappings_path, 'r', encoding='utf-8') as f:
                mappings = json.load(f)
                self.id_to_index = mappings['id_to_index']
                self.index_to_id = mappings['index_to_id']
            
            logger.info(f"ğŸ“š ç´¢å¼•åŠ è½½å®Œæˆ - èŠ‚ç‚¹: {self.node_index.ntotal}, è¾¹: {self.edge_index.ntotal}")
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸  ç´¢å¼•åŠ è½½å¤±è´¥: {str(e)}")
            return False
    
    def _save_indexes(self):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        try:
            # ä¿å­˜FAISSç´¢å¼•
            faiss.write_index(self.node_index, str(self.index_dir / "node_index.faiss"))
            faiss.write_index(self.edge_index, str(self.index_dir / "edge_index.faiss"))
            
            # ä¿å­˜å…ƒæ•°æ®
            with open(self.index_dir / "node_metadata.pkl", 'wb') as f:
                pickle.dump(self.node_metadata, f)
            
            with open(self.index_dir / "edge_metadata.pkl", 'wb') as f:
                pickle.dump(self.edge_metadata, f)
            
            # ä¿å­˜æ˜ å°„
            mappings = {
                'id_to_index': self.id_to_index,
                'index_to_id': self.index_to_id
            }
            with open(self.index_dir / "mappings.json", 'w', encoding='utf-8') as f:
                json.dump(mappings, f, ensure_ascii=False, indent=2)
            
            logger.info("ğŸ’¾ ç´¢å¼•ä¿å­˜å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•ä¿å­˜å¤±è´¥: {str(e)}")
    
    def _build_indexes(self):
        """æ„å»ºæ–°ç´¢å¼•"""
        logger.info("ğŸ—ï¸  å¼€å§‹æ„å»ºå›¾ç´¢å¼•...")
        
        # æ„å»ºèŠ‚ç‚¹ç´¢å¼•
        self._build_node_index()
        
        # æ„å»ºè¾¹ç´¢å¼•
        self._build_edge_index()
        
        logger.info("âœ… å›¾ç´¢å¼•æ„å»ºå®Œæˆ")
    
    def _build_node_index(self):
        """æ„å»ºèŠ‚ç‚¹ç´¢å¼•"""
        logger.info("ğŸ“Š æ„å»ºèŠ‚ç‚¹ç´¢å¼•...")
        
        # è·å–æ‰€æœ‰èŠ‚ç‚¹
        nodes_data = []
        
        # è·å–çƒå‘˜èŠ‚ç‚¹
        players_query = """
        MATCH (p:player)
        RETURN p.player.name AS name, p.player.age AS age
        LIMIT 200
        """
        
        result = self.nebula_conn.execute_query(players_query)
        if result['success']:
            for row in result['rows']:
                name, age = row[0], row[1]
                node_id = f"player:{name}"
                description = f"çƒå‘˜ {name}, å¹´é¾„ {age} å²"
                
                nodes_data.append({
                    'node_id': node_id,  # æ”¹ä¸ºnode_idä»¥ä¿æŒä¸€è‡´æ€§
                    'id': node_id,      # ä¿ç•™idä½œä¸ºå¤‡ç”¨
                    'type': 'player',
                    'name': name,
                    'description': description,
                    'properties': {'age': age}
                })
        
        # è·å–çƒé˜ŸèŠ‚ç‚¹
        teams_query = """
        MATCH (t:team)
        RETURN t.team.name AS name
        LIMIT 100
        """
        
        result = self.nebula_conn.execute_query(teams_query)
        if result['success']:
            for row in result['rows']:
                team_name = row[0]
                node_id = f"team:{team_name}"
                description = f"ç¯®çƒé˜Ÿ {team_name}"
                
                nodes_data.append({
                    'node_id': node_id,  # æ”¹ä¸ºnode_idä»¥ä¿æŒä¸€è‡´æ€§
                    'id': node_id,      # ä¿ç•™idä½œä¸ºå¤‡ç”¨
                    'type': 'team',
                    'name': team_name,
                    'description': description,
                    'properties': {}
                })
        
        # ç”ŸæˆåµŒå…¥å¹¶æ„å»ºç´¢å¼•
        if nodes_data:
            descriptions = [node['description'] for node in nodes_data]
            embeddings = self.embedding_model.encode(descriptions)
            
            # åˆ›å»ºFAISSç´¢å¼•
            self.node_index = faiss.IndexFlatIP(self.embedding_dim)  # å†…ç§¯ç›¸ä¼¼åº¦
            
            # æ·»åŠ åˆ°ç´¢å¼•
            self.node_index.add(embeddings.astype('float32'))
            
            # å­˜å‚¨å…ƒæ•°æ®å’Œæ˜ å°„
            for i, node in enumerate(nodes_data):
                self.node_metadata[i] = node
                self.id_to_index[node['id']] = i
                self.index_to_id[str(i)] = node['id']
        
        logger.info(f"âœ… èŠ‚ç‚¹ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(nodes_data)} ä¸ªèŠ‚ç‚¹")
    
    def _build_edge_index(self):
        """æ„å»ºè¾¹ç´¢å¼•"""
        logger.info("ğŸ”— æ„å»ºè¾¹ç´¢å¼•...")
        
        # è·å–æ‰€æœ‰è¾¹
        edges_data = []
        
        # è·å–æ•ˆåŠ›å…³ç³»
        serve_query = """
        MATCH (p:player)-[s:serve]->(t:team)
        RETURN p.player.name AS player, t.team.name AS team
        LIMIT 300
        """
        
        result = self.nebula_conn.execute_query(serve_query)
        if result['success']:
            for row in result['rows']:
                player_name, team_name = row[0], row[1]
                edge_id = f"serve:{player_name}->{team_name}"
                description = f"{player_name} æ•ˆåŠ›äº {team_name}"
                
                edges_data.append({
                    'edge_id': edge_id,  # æ”¹ä¸ºedge_idä»¥ä¿æŒä¸€è‡´æ€§
                    'id': edge_id,      # ä¿ç•™idä½œä¸ºå¤‡ç”¨
                    'type': 'serve',
                    'source': f"player:{player_name}",
                    'target': f"team:{team_name}",
                    'description': description,
                    'properties': {}
                })
        
        # ç”ŸæˆåµŒå…¥å¹¶æ„å»ºç´¢å¼•
        if edges_data:
            descriptions = [edge['description'] for edge in edges_data]
            embeddings = self.embedding_model.encode(descriptions)
            
            # åˆ›å»ºFAISSç´¢å¼•
            self.edge_index = faiss.IndexFlatIP(self.embedding_dim)
            
            # æ·»åŠ åˆ°ç´¢å¼•
            self.edge_index.add(embeddings.astype('float32'))
            
            # å­˜å‚¨å…ƒæ•°æ®
            for i, edge in enumerate(edges_data):
                edge_idx = f"edge_{i}"
                self.edge_metadata[edge_idx] = edge
        
        logger.info(f"âœ… è¾¹ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {len(edges_data)} æ¡è¾¹")
    
    def search_nodes(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³èŠ‚ç‚¹"""
        if not self.is_initialized or self.node_index is None:
            return []
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_model.encode([query]).astype('float32')
        
        # æœç´¢
        scores, indices = self.node_index.search(query_embedding, top_k)
        
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx in self.node_metadata:
                node_data = self.node_metadata[idx].copy()
                node_data['similarity'] = float(score)
                node_data['rank'] = i + 1
                results.append(node_data)
        
        return results
    
    def search_edges(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³è¾¹"""
        if not self.is_initialized or self.edge_index is None:
            return []
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_model.encode([query]).astype('float32')
        
        # æœç´¢
        scores, indices = self.edge_index.search(query_embedding, top_k)
        
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            edge_key = f"edge_{idx}"
            if edge_key in self.edge_metadata:
                edge_data = self.edge_metadata[edge_key].copy()
                edge_data['similarity'] = float(score)
                edge_data['rank'] = i + 1
                results.append(edge_data)
        
        return results
    
    def get_node_by_id(self, node_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–èŠ‚ç‚¹ä¿¡æ¯"""
        if node_id in self.id_to_index:
            idx = self.id_to_index[node_id]
            if idx in self.node_metadata:
                return self.node_metadata[idx]
        return None
    
    def get_nodes_by_type(self, node_type: str) -> List[Dict[str, Any]]:
        """æ ¹æ®ç±»å‹è·å–èŠ‚ç‚¹"""
        results = []
        for node_data in self.node_metadata.values():
            if node_data.get('type') == node_type:
                results.append(node_data)
        return results
    
    def get_neighbors(self, node_id: str) -> List[str]:
        """è·å–èŠ‚ç‚¹çš„é‚»å±…"""
        neighbors = []
        
        # åœ¨è¾¹æ•°æ®ä¸­æŸ¥æ‰¾åŒ…å«è¯¥èŠ‚ç‚¹çš„è¾¹
        for edge_data in self.edge_metadata.values():
            source = edge_data.get('source')
            target = edge_data.get('target')
            
            if source == node_id and target not in neighbors:
                neighbors.append(target)
            elif target == node_id and source not in neighbors:
                neighbors.append(source)
        
        return neighbors
    
    def rebuild_index(self):
        """é‡å»ºç´¢å¼•"""
        logger.info("ğŸ”„ é‡å»ºç´¢å¼•ä¸­...")
        
        # æ¸…ç©ºç°æœ‰ç´¢å¼•
        self.node_index = None
        self.edge_index = None
        self.node_metadata.clear()
        self.edge_metadata.clear()
        self.id_to_index.clear()
        self.index_to_id.clear()
        
        # é‡æ–°æ„å»º
        self._build_indexes()
        self._save_indexes()
        
        logger.info("âœ… ç´¢å¼•é‡å»ºå®Œæˆ")
    
    def get_index_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'initialized': self.is_initialized,
            'embedding_dim': self.embedding_dim,
            'node_count': self.node_index.ntotal if self.node_index else 0,
            'edge_count': self.edge_index.ntotal if self.edge_index else 0,
            'node_types': {},
            'edge_types': {}
        }
        
        # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹
        for node_data in self.node_metadata.values():
            node_type = node_data.get('type', 'unknown')
            stats['node_types'][node_type] = stats['node_types'].get(node_type, 0) + 1
        
        # ç»Ÿè®¡è¾¹ç±»å‹
        for edge_data in self.edge_metadata.values():
            edge_type = edge_data.get('type', 'unknown')
            stats['edge_types'][edge_type] = stats['edge_types'].get(edge_type, 0) + 1
        
        return stats
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.nebula_conn:
            self.nebula_conn.close()
