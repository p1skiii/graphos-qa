"""
ç®€å•å›¾æŸ¥è¯¢å¤„ç†å™¨ (Simple Graph Processor)
ç”¨äºå¤„ç† SIMPLE_RELATION_QUERY ç±»å‹çš„æŸ¥è¯¢
é€‚ç”¨äºç®€å•çš„å…³ç³»æŸ¥è¯¢ï¼Œå¦‚"ç§‘æ¯”åœ¨å“ªä¸ªçƒé˜Ÿ"ã€"æ¹–äººé˜Ÿæœ‰å“ªäº›çƒå‘˜"
"""
from typing import Dict, Any, Optional, List
import logging
from .base_processor import BaseProcessor, ProcessorUtils
from app.rag.components import ProcessorDefaultConfigs

logger = logging.getLogger(__name__)

class SimpleGProcessor(BaseProcessor):
    """ç®€å•å›¾æŸ¥è¯¢å¤„ç†å™¨"""
    
    def __init__(self, config: Optional[ProcessorDefaultConfigs] = None):
        """åˆå§‹åŒ–ç®€å•å›¾æŸ¥è¯¢å¤„ç†å™¨"""
        if config is None:
            config = ProcessorDefaultConfigs.get_simple_g_processor_config()
        
        super().__init__(config)
        logger.info(f"ğŸŒ åˆ›å»ºç®€å•å›¾æŸ¥è¯¢å¤„ç†å™¨")
    
    def _process_impl(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """å¤„ç†ç®€å•å›¾æŸ¥è¯¢"""
        logger.info(f"ğŸ” ç®€å•å›¾æŸ¥è¯¢å¤„ç†: {query}")
        
        try:
            # 1. æŸ¥è¯¢åˆ†æ
            query_analysis = self._analyze_query(query, context)
            
            # 2. è¯­ä¹‰æ£€ç´¢ç›¸å…³èŠ‚ç‚¹
            retrieved_nodes = self.retriever.retrieve(
                query, 
                top_k=min(10, self.config.max_tokens // 150)
            )
            
            if not retrieved_nodes:
                return self._create_empty_result(query, "æœªæ‰¾åˆ°ç›¸å…³å®ä½“")
            
            # 3. æ„å»ºç®€å•å­å›¾ (BFSæ‰©å±•)
            seed_nodes = [node['node_id'] for node in retrieved_nodes[:6]]
            subgraph = self.graph_builder.build_subgraph(seed_nodes, query)
            
            if not ProcessorUtils.validate_subgraph(subgraph):
                return self._create_fallback_result(query, retrieved_nodes)
            
            # 4. å¢å¼ºå­å›¾ (æ·»åŠ å…³ç³»ä¿¡æ¯)
            enhanced_subgraph = self._enhance_subgraph(subgraph, query_analysis)
            
            # 5. æ–‡æœ¬åŒ– (ä½¿ç”¨æ¨¡æ¿æ ¼å¼ï¼Œé€‚åˆå…³ç³»å±•ç¤º)
            contextualized_text = self.textualizer.textualize(enhanced_subgraph, query)
            
            # 6. é™åˆ¶tokenæ•°é‡
            final_text = ProcessorUtils.limit_tokens(
                contextualized_text, 
                self.config.max_tokens
            )
            
            # 7. æ„å»ºç»“æœ
            result = {
                'success': True,
                'query': query,
                'context': context or {},
                'query_analysis': query_analysis,
                'retrieved_nodes_count': len(retrieved_nodes),
                'subgraph_summary': {
                    'nodes': enhanced_subgraph['node_count'],
                    'edges': enhanced_subgraph['edge_count'],
                    'algorithm': enhanced_subgraph.get('algorithm', 'bfs'),
                    'relation_types': self._get_relation_types(enhanced_subgraph)
                },
                'contextualized_text': final_text,
                'processing_strategy': 'simple_graph_relation',
                'confidence': self._calculate_confidence(retrieved_nodes, enhanced_subgraph)
            }
            
            logger.info(f"âœ… ç®€å•å›¾æŸ¥è¯¢å¤„ç†å®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {enhanced_subgraph['node_count']}, è¾¹æ•°: {enhanced_subgraph['edge_count']}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ç®€å•å›¾æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")
            raise
    
    def _analyze_query(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æç®€å•å›¾æŸ¥è¯¢"""
        analysis = {
            'query_type': 'simple_relation_query',
            'entities': ProcessorUtils.extract_entities_from_query(query),
            'relation_intent': self._detect_relation_intent(query),
            'graph_complexity': 'simple',
            'expected_hops': 1  # ç®€å•æŸ¥è¯¢é€šå¸¸åªéœ€è¦1è·³å…³ç³»
        }
        
        # åˆ†æä¸Šä¸‹æ–‡
        if context:
            analysis['context_entities'] = context.get('entities', [])
            analysis['context_relations'] = context.get('relations', [])
        
        return analysis
    
    def _detect_relation_intent(self, query: str) -> Dict[str, Any]:
        """æ£€æµ‹å…³ç³»æŸ¥è¯¢æ„å›¾"""
        query_lower = query.lower()
        
        intent = {
            'relation_type': 'unknown',
            'direction': 'bidirectional',  # forward, backward, bidirectional
            'target_entity_type': None,
            'relation_keywords': []
        }
        
        # æ£€æµ‹å…³ç³»ç±»å‹
        if any(word in query for word in ['æ•ˆåŠ›', 'åœ¨å“ªä¸ªé˜Ÿ', 'çƒé˜Ÿ']):
            intent['relation_type'] = 'team_affiliation'
            intent['relation_keywords'].append('serve')
            
            if 'çƒå‘˜' in query or any(name in query for name in ['ç§‘æ¯”', 'è©¹å§†æ–¯', 'ä¹”ä¸¹']):
                intent['direction'] = 'forward'  # çƒå‘˜ -> çƒé˜Ÿ
                intent['target_entity_type'] = 'team'
            elif 'çƒé˜Ÿ' in query or any(team in query for team in ['æ¹–äºº', 'å…¬ç‰›', 'å‹‡å£«']):
                intent['direction'] = 'backward'  # çƒé˜Ÿ -> çƒå‘˜
                intent['target_entity_type'] = 'player'
        
        elif any(word in query for word in ['é˜Ÿå‹', 'ä¸€èµ·æ‰“çƒ', 'åŒé˜Ÿ']):
            intent['relation_type'] = 'teammate'
            intent['direction'] = 'bidirectional'
            intent['target_entity_type'] = 'player'
        
        elif any(word in query for word in ['å¯¹æ‰‹', 'äº¤æ‰‹', 'æ¯”èµ›']):
            intent['relation_type'] = 'opponent'
            intent['direction'] = 'bidirectional'
        
        return intent
    
    def _enhance_subgraph(self, subgraph: Dict[str, Any], query_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºå­å›¾ä¿¡æ¯"""
        enhanced_subgraph = subgraph.copy()
        
        # æ·»åŠ å…³ç³»ç»Ÿè®¡
        relation_stats = self._analyze_subgraph_relations(subgraph)
        enhanced_subgraph['relation_stats'] = relation_stats
        
        # æ ¹æ®æŸ¥è¯¢æ„å›¾çªå‡ºé‡è¦å…³ç³»
        relation_intent = query_analysis.get('relation_intent', {})
        if relation_intent.get('relation_type') != 'unknown':
            enhanced_subgraph = self._highlight_relevant_relations(
                enhanced_subgraph, 
                relation_intent
            )
        
        return enhanced_subgraph
    
    def _analyze_subgraph_relations(self, subgraph: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå­å›¾ä¸­çš„å…³ç³»"""
        edges = subgraph.get('edges', [])
        
        relation_stats = {
            'total_relations': len(edges),
            'relation_types': {},
            'entity_connections': {}
        }
        
        # ç»Ÿè®¡å…³ç³»ç±»å‹
        for edge in edges:
            relation = edge.get('relation', 'unknown')
            relation_stats['relation_types'][relation] = \
                relation_stats['relation_types'].get(relation, 0) + 1
        
        # ç»Ÿè®¡å®ä½“è¿æ¥åº¦
        for edge in edges:
            source = edge.get('source', '')
            target = edge.get('target', '')
            
            for entity in [source, target]:
                if entity:
                    relation_stats['entity_connections'][entity] = \
                        relation_stats['entity_connections'].get(entity, 0) + 1
        
        return relation_stats
    
    def _highlight_relevant_relations(self, subgraph: Dict[str, Any], 
                                    relation_intent: Dict[str, Any]) -> Dict[str, Any]:
        """çªå‡ºæ˜¾ç¤ºç›¸å…³å…³ç³»"""
        highlighted_subgraph = subgraph.copy()
        
        target_relation = relation_intent.get('relation_type')
        if target_relation == 'team_affiliation':
            # ä¼˜å…ˆæ˜¾ç¤ºserveå…³ç³»
            edges = highlighted_subgraph.get('edges', [])
            serve_edges = [e for e in edges if e.get('relation') == 'serve']
            other_edges = [e for e in edges if e.get('relation') != 'serve']
            
            # é‡æ–°æ’åºï¼Œserveå…³ç³»åœ¨å‰
            highlighted_subgraph['edges'] = serve_edges + other_edges
            highlighted_subgraph['primary_relation'] = 'serve'
        
        return highlighted_subgraph
    
    def _get_relation_types(self, subgraph: Dict[str, Any]) -> List[str]:
        """è·å–å­å›¾ä¸­çš„å…³ç³»ç±»å‹"""
        edges = subgraph.get('edges', [])
        relation_types = set()
        
        for edge in edges:
            relation = edge.get('relation', 'unknown')
            relation_types.add(relation)
        
        return list(relation_types)
    
    def _create_empty_result(self, query: str, reason: str) -> Dict[str, Any]:
        """åˆ›å»ºç©ºç»“æœ"""
        return {
            'success': True,
            'query': query,
            'context': {},
            'query_analysis': {'query_type': 'simple_relation_query'},
            'retrieved_nodes_count': 0,
            'subgraph_summary': {
                'nodes': 0, 
                'edges': 0, 
                'algorithm': 'none',
                'relation_types': []
            },
            'contextualized_text': f"æŠ±æ­‰ï¼Œ{reason}ã€‚è¯·å°è¯•æåŠå…·ä½“çš„çƒå‘˜æˆ–çƒé˜Ÿåç§°ã€‚",
            'processing_strategy': 'simple_graph_relation',
            'confidence': 0.0,
            'empty_reason': reason
        }
    
    def _create_fallback_result(self, query: str, retrieved_nodes: List[Dict]) -> Dict[str, Any]:
        """åˆ›å»ºå›é€€ç»“æœ"""
        fallback_text = self._format_simple_relations(retrieved_nodes, query)
        
        return {
            'success': True,
            'query': query,
            'context': {},
            'query_analysis': {'query_type': 'simple_relation_query'},
            'retrieved_nodes_count': len(retrieved_nodes),
            'subgraph_summary': {
                'nodes': len(retrieved_nodes), 
                'edges': 0, 
                'algorithm': 'fallback',
                'relation_types': []
            },
            'contextualized_text': fallback_text,
            'processing_strategy': 'simple_graph_relation_fallback',
            'confidence': 0.7
        }
    
    def _format_simple_relations(self, nodes: List[Dict], query: str) -> str:
        """æ ¼å¼åŒ–ç®€å•å…³ç³»ä¿¡æ¯"""
        if not nodes:
            return "æœªæ‰¾åˆ°ç›¸å…³å…³ç³»ä¿¡æ¯ã€‚"
        
        text_parts = ["æ ¹æ®æŸ¥è¯¢æ‰¾åˆ°ä»¥ä¸‹å…³ç³»ä¿¡æ¯ï¼š"]
        
        # æŒ‰ç±»å‹åˆ†ç»„èŠ‚ç‚¹
        players = [n for n in nodes if n.get('type') == 'player']
        teams = [n for n in nodes if n.get('type') == 'team']
        
        # å¦‚æœåŒæ—¶æœ‰çƒå‘˜å’Œçƒé˜Ÿï¼Œæ¨æ–­å¯èƒ½çš„å…³ç³»
        if players and teams:
            text_parts.append("\nå¯èƒ½çš„æ•ˆåŠ›å…³ç³»ï¼š")
            for i, player in enumerate(players[:3]):
                for j, team in enumerate(teams[:2]):
                    if i < len(teams):  # ç®€å•é…å¯¹
                        player_name = player.get('name', 'æœªçŸ¥çƒå‘˜')
                        team_name = team.get('name', 'æœªçŸ¥çƒé˜Ÿ')
                        text_parts.append(f"- {player_name} å¯èƒ½ä¸ {team_name} ç›¸å…³")
        
        # å•ç‹¬åˆ—å‡ºå®ä½“
        elif players:
            text_parts.append(f"\nç›¸å…³çƒå‘˜ï¼š{', '.join([p.get('name', 'æœªçŸ¥') for p in players[:5]])}")
        elif teams:
            text_parts.append(f"\nç›¸å…³çƒé˜Ÿï¼š{', '.join([t.get('name', 'æœªçŸ¥') for t in teams[:5]])}")
        
        return "\n".join(text_parts)
    
    def _calculate_confidence(self, retrieved_nodes: List[Dict], subgraph: Dict[str, Any]) -> float:
        """è®¡ç®—ç»“æœç½®ä¿¡åº¦"""
        if not retrieved_nodes:
            return 0.0
        
        # åŸºç¡€ç½®ä¿¡åº¦
        base_confidence = 0.65
        
        # æ ¹æ®æ£€ç´¢è´¨é‡è°ƒæ•´
        similarities = [node.get('similarity', 0) for node in retrieved_nodes]
        if similarities:
            avg_similarity = sum(similarities) / len(similarities)
            similarity_bonus = avg_similarity * 0.15
        else:
            similarity_bonus = 0
        
        # æ ¹æ®å­å›¾è´¨é‡è°ƒæ•´
        subgraph_bonus = 0
        if subgraph:
            node_count = subgraph.get('node_count', 0)
            edge_count = subgraph.get('edge_count', 0)
            
            if edge_count > 0:  # æœ‰å…³ç³»è¾¹æ˜¯å…³é”®
                subgraph_bonus = min(0.2, edge_count * 0.05)
            elif node_count > 1:  # è‡³å°‘æœ‰å¤šä¸ªèŠ‚ç‚¹
                subgraph_bonus = 0.1
        
        total_confidence = base_confidence + similarity_bonus + subgraph_bonus
        return min(1.0, total_confidence)

# =============================================================================
# å·¥å‚å‡½æ•°
# =============================================================================

def create_simple_g_processor(custom_config: Optional[Dict[str, Any]] = None) -> SimpleGProcessor:
    """åˆ›å»ºç®€å•å›¾æŸ¥è¯¢å¤„ç†å™¨å®ä¾‹"""
    if custom_config:
        config = ProcessorDefaultConfigs.get_simple_g_processor_config()
        
        # æ›´æ–°é…ç½®é€»è¾‘ï¼ˆä¸DirectProcessorç±»ä¼¼ï¼‰
        if 'retriever' in custom_config:
            config.retriever_config.config.update(custom_config['retriever'])
        if 'graph_builder' in custom_config:
            config.graph_builder_config.config.update(custom_config['graph_builder'])
        if 'textualizer' in custom_config:
            config.textualizer_config.config.update(custom_config['textualizer'])
        
        for key in ['cache_enabled', 'cache_ttl', 'max_tokens']:
            if key in custom_config:
                setattr(config, key, custom_config[key])
        
        return SimpleGProcessor(config)
    else:
        return SimpleGProcessor()
