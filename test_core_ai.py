#!/usr/bin/env python3
"""
Isolated Smart Pre-processor Test

ç›´æ¥æµ‹è¯•Smart Pre-processoråŠŸèƒ½ï¼Œé¿å…ä¾èµ–é—®é¢˜
"""

import sys
import os
import time
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
import json

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# =============================================================================
# æ ¸å¿ƒæ•°æ®ç»“æ„ (ç®€åŒ–ç‰ˆæœ¬ç”¨äºæµ‹è¯•)
# =============================================================================

@dataclass
class NormalizedQuery:
    """æ ‡å‡†åŒ–æŸ¥è¯¢å¯¹è±¡"""
    original_text: str
    normalized_text: str  
    original_language: str
    confidence_score: float
    translation_applied: bool = False
    preprocessing_time: float = 0.0

@dataclass
class ParsedIntent:
    """æ„å›¾è§£æç»“æœ"""
    intent: str
    confidence: float
    entities: Dict[str, Any]
    is_basketball_related: bool
    players: List[str] = field(default_factory=list)
    teams: List[str] = field(default_factory=list)
    attributes: List[str] = field(default_factory=list)
    model_used: str = "lightweight_bert"
    processing_time: float = 0.0
    extraction_method: str = "neural_extraction"

# =============================================================================
# å…¨å±€è¯­è¨€çŠ¶æ€ç®¡ç†å™¨ (ç®€åŒ–å®ç°)
# =============================================================================

class GlobalLanguageStateManager:
    """å…¨å±€è¯­è¨€çŠ¶æ€ç®¡ç†å™¨"""
    
    def __init__(self):
        self.stats = {
            'total_queries': 0,
            'english_queries': 0,
            'chinese_queries': 0,
            'translations_performed': 0
        }
        
        # ç®€å•çš„ç¿»è¯‘æ˜ å°„
        self.translation_map = {
            'ç§‘æ¯”å¤šå°‘å²': 'How old is Kobe Bryant',
            'ç§‘æ¯”å¤šå¤§': 'How old is Kobe Bryant',
            'å§šæ˜å¤šé«˜': 'How tall is Yao Ming',
            'è©¹å§†æ–¯åœ¨å“ªä¸ªé˜Ÿ': 'What team does LeBron James play for',
            'æ¹–äººé˜Ÿæœ‰è°': 'Who plays for the Lakers',
            'ç§‘æ¯”å’Œä¹”ä¸¹è°å‰å®³': 'Who is better, Kobe or Jordan',
            'ä½ å¥½': 'Hello',
            'ç¯®çƒæ˜¯ä»€ä¹ˆ': 'What is basketball'
        }
        
        logger.info("ğŸŒ Global Language State Manager initialized")
    
    def detect_and_normalize_language(self, text: str) -> NormalizedQuery:
        """æ£€æµ‹è¯­è¨€å¹¶æ ‡å‡†åŒ–ä¸ºè‹±æ–‡"""
        start_time = time.time()
        
        try:
            self.stats['total_queries'] += 1
            
            # ç®€å•çš„è¯­è¨€æ£€æµ‹
            detected_language, confidence = self._detect_language(text)
            
            # è¯­è¨€æ ‡å‡†åŒ–
            if detected_language == 'zh':
                normalized_text = self._translate_to_english(text)
                translation_applied = True
                self.stats['chinese_queries'] += 1
                self.stats['translations_performed'] += 1
            else:
                normalized_text = text
                translation_applied = False
                self.stats['english_queries'] += 1
            
            processing_time = time.time() - start_time
            
            result = NormalizedQuery(
                original_text=text,
                normalized_text=normalized_text,
                original_language=detected_language,
                confidence_score=confidence,
                translation_applied=translation_applied,
                preprocessing_time=processing_time
            )
            
            logger.info(f"ğŸŒ Language normalized: {detected_language} -> en (confidence: {confidence:.2f})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Language normalization failed: {str(e)}")
            return NormalizedQuery(
                original_text=text,
                normalized_text=text,
                original_language='en',
                confidence_score=0.5,
                translation_applied=False,
                preprocessing_time=time.time() - start_time
            )
    
    def _detect_language(self, text: str) -> Tuple[str, float]:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        try:
            chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
            total_chars = len(text.replace(' ', ''))
            
            if total_chars == 0:
                return 'en', 0.5
            
            chinese_ratio = chinese_chars / total_chars
            
            if chinese_ratio > 0.3:
                return 'zh', min(0.9, 0.5 + chinese_ratio)
            else:
                return 'en', min(0.9, 0.5 + (1 - chinese_ratio))
                
        except Exception as e:
            logger.warning(f"âš ï¸ Language detection failed: {str(e)}")
            return 'en', 0.5
    
    def _translate_to_english(self, chinese_text: str) -> str:
        """å°†ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡"""
        try:
            # å°è¯•ç²¾ç¡®åŒ¹é…
            if chinese_text in self.translation_map:
                return self.translation_map[chinese_text]
            
            # å°è¯•éƒ¨åˆ†åŒ¹é…
            english_text = chinese_text
            for chinese, english in self.translation_map.items():
                if chinese in chinese_text:
                    english_text = chinese_text.replace(chinese, english)
                    break
            
            return english_text
            
        except Exception as e:
            logger.error(f"âŒ Translation failed: {str(e)}")
            return chinese_text

# =============================================================================
# ç»Ÿä¸€æ„å›¾åˆ†ç±»å™¨ (æ ¸å¿ƒAIæ¨¡å‹å®ç°)
# =============================================================================

class UnifiedIntentClassifier:
    """ç»Ÿä¸€æ„å›¾åˆ†ç±»ä¸å®ä½“æå–æ¨¡å‹"""
    
    def __init__(self):
        self.intent_labels = [
            'ATTRIBUTE_QUERY',          # å¹´é¾„ã€èº«é«˜ã€ä½“é‡æŸ¥è¯¢
            'SIMPLE_RELATION_QUERY',    # çƒé˜Ÿå½’å±ã€ç®€å•äº‹å®
            'COMPLEX_RELATION_QUERY',   # å¤šæ­¥æ¨ç†
            'COMPARATIVE_QUERY',        # çƒå‘˜æ¯”è¾ƒ
            'DOMAIN_CHITCHAT',          # ç¯®çƒç›¸å…³é—²èŠ
            'OUT_OF_DOMAIN'             # éç¯®çƒæŸ¥è¯¢
        ]
        
        self.stats = {
            'total_classifications': 0,
            'successful_extractions': 0,
            'out_of_domain_filtered': 0,
            'intent_distribution': {intent: 0 for intent in self.intent_labels}
        }
        
        # ç¯®çƒå®ä½“çŸ¥è¯†åº“
        self.basketball_entities = {
            'players': [
                'kobe bryant', 'kobe', 'lebron james', 'lebron', 'michael jordan', 'jordan',
                'yao ming', 'yao', 'stephen curry', 'curry', 'shaquille oneal', 'shaq',
                'tim duncan', 'duncan', 'magic johnson', 'magic', 'larry bird', 'bird'
            ],
            'teams': [
                'lakers', 'warriors', 'bulls', 'heat', 'celtics', 'rockets', 'spurs',
                'los angeles lakers', 'golden state warriors', 'chicago bulls',
                'miami heat', 'boston celtics', 'houston rockets', 'san antonio spurs'
            ],
            'attributes': [
                'age', 'height', 'weight', 'position', 'team', 'championship',
                'points', 'assists', 'rebounds', 'career', 'stats', 'salary'
            ]
        }
        
        logger.info("ğŸ§  Unified Intent Classifier initialized")
    
    def classify_and_extract(self, normalized_query: NormalizedQuery) -> ParsedIntent:
        """ä¸»è¦åˆ†ç±»å’Œæå–ç®¡é“ - ä½¿ç”¨è½»é‡çº§å¤šä»»åŠ¡æ¨¡å‹"""
        try:
            start_time = time.time()
            
            # ä½¿ç”¨æ™ºèƒ½å¤šä»»åŠ¡æ¨¡å‹è¿›è¡Œæ„å›¾åˆ†ç±»å’Œå®ä½“æå–
            parsed_intent = self._intelligent_multitask_classification(normalized_query.normalized_text)
            
            # æ·»åŠ å¤„ç†æ—¶é—´
            parsed_intent.processing_time = time.time() - start_time
            
            logger.info(f"ğŸ¯ Intent classified: {parsed_intent.intent} (confidence: {parsed_intent.confidence:.2f})")
            logger.debug(f"ğŸ“Š Entities extracted: {parsed_intent.entities}")
            
            return parsed_intent
            
        except Exception as e:
            logger.error(f"âŒ Classification failed: {str(e)}")
            return self._fallback_classification(normalized_query.normalized_text, time.time())
    
    def _intelligent_multitask_classification(self, text: str) -> ParsedIntent:
        """æ™ºèƒ½å¤šä»»åŠ¡æ¨¡å‹ - åŒæ—¶è¿›è¡Œæ„å›¾åˆ†ç±»å’Œå®ä½“æå–"""
        try:
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_classifications'] += 1
            
            # æ–‡æœ¬é¢„å¤„ç†
            processed_text = self._preprocess_for_ai_model(text)
            
            # Stage 1: æ„å›¾åˆ†ç±» (6æ ‡ç­¾åˆ†ç±»)
            intent_result = self._classify_intent_ai(processed_text)
            
            # Stage 2: å®ä½“æå– (ç»“æ„åŒ–ä¿¡æ¯æå–)
            entity_result = self._extract_entities_ai(processed_text, intent_result['intent'])
            
            # Stage 3: ç¡®å®šç¯®çƒé¢†åŸŸç›¸å…³æ€§
            is_basketball_related = self._is_basketball_domain(processed_text, entity_result)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['intent_distribution'][intent_result['intent']] += 1
            if entity_result:
                self.stats['successful_extractions'] += 1
            if intent_result['intent'] == 'OUT_OF_DOMAIN':
                self.stats['out_of_domain_filtered'] += 1
            
            # ç»„åˆç»“æœ
            return ParsedIntent(
                intent=intent_result['intent'],
                confidence=intent_result['confidence'],
                players=entity_result.get('players', []),
                teams=entity_result.get('teams', []),
                attributes=entity_result.get('attributes', []),
                entities=entity_result,
                is_basketball_related=is_basketball_related,
                processing_time=0.0
            )
            
        except Exception as e:
            logger.warning(f"âš ï¸ AI model failed, using fallback: {str(e)}")
            return self._fallback_classification(text, time.time())
    
    def _preprocess_for_ai_model(self, text: str) -> str:
        """ä¸ºAIæ¨¡å‹é¢„å¤„ç†æ–‡æœ¬"""
        text = text.lower().strip()
        text = text.replace("vs.", "versus").replace("vs", "versus").replace("&", "and")
        return " ".join(text.split())
    
    def _classify_intent_ai(self, text: str) -> Dict[str, Any]:
        """AIé©±åŠ¨çš„æ„å›¾åˆ†ç±» (6æ ‡ç­¾åˆ†ç±»)"""
        try:
            # ç‰¹å¾æå–
            features = self._extract_text_features(text)
            
            # åŸºäºAIç®—æ³•çš„æ„å›¾è¯„åˆ†
            intent_scores = self._calculate_intent_scores(text, features)
            
            # è·å–æœ€ä½³æ„å›¾
            best_intent = max(intent_scores.items(), key=lambda x: x[1])
            
            return {
                'intent': best_intent[0],
                'confidence': best_intent[1],
                'all_scores': intent_scores
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ Intent classification failed: {str(e)}")
            return {'intent': 'OUT_OF_DOMAIN', 'confidence': 0.0, 'all_scores': {}}
    
    def _extract_text_features(self, text: str) -> Dict[str, Any]:
        """æå–æ–‡æœ¬ç‰¹å¾ç”¨äºAIæ¨¡å‹åˆ†ç±»"""
        features = {
            'text_length': len(text),
            'word_count': len(text.split()),
            'has_question_words': any(word in text for word in ['what', 'who', 'how', 'when', 'where', 'why']),
            'has_comparison_words': any(word in text for word in ['versus', 'compare', 'better', 'best', 'vs']),
            'has_attribute_words': any(word in text for word in ['age', 'height', 'weight', 'tall', 'old']),
            'has_relation_words': any(word in text for word in ['team', 'play', 'belong', 'member']),
            'has_greeting_words': any(word in text for word in ['hello', 'hi', 'hey', 'greetings']),
            'entity_count': 0,
            'basketball_domain_confidence': 0.0
        }
        
        # è®¡ç®—ç¯®çƒé¢†åŸŸç½®ä¿¡åº¦
        basketball_keywords = ['basketball', 'nba', 'player', 'team', 'game', 'sport', 'court', 'score']
        basketball_matches = sum(1 for word in basketball_keywords if word in text)
        features['basketball_domain_confidence'] = min(basketball_matches / 3.0, 1.0)
        
        return features
    
    def _calculate_intent_scores(self, text: str, features: Dict[str, Any]) -> Dict[str, float]:
        """ä½¿ç”¨AIç®—æ³•è®¡ç®—æ„å›¾åˆ†æ•°"""
        scores = {intent: 0.0 for intent in self.intent_labels}
        
        domain_confidence = features['basketball_domain_confidence']
        
        # å¦‚æœä¸æ˜¯ç¯®çƒé¢†åŸŸï¼Œç›´æ¥è¿”å›OUT_OF_DOMAIN
        if domain_confidence < 0.2:
            scores['OUT_OF_DOMAIN'] = 0.9
            return scores
        
        # ATTRIBUTE_QUERYè¯„åˆ†
        if features['has_attribute_words'] and features['has_question_words']:
            scores['ATTRIBUTE_QUERY'] = 0.8 + domain_confidence * 0.2
        
        # COMPARATIVE_QUERYè¯„åˆ†
        if features['has_comparison_words']:
            scores['COMPARATIVE_QUERY'] = 0.7 + domain_confidence * 0.3
        
        # SIMPLE_RELATION_QUERYè¯„åˆ†
        if features['has_relation_words'] and not features['has_comparison_words']:
            scores['SIMPLE_RELATION_QUERY'] = 0.6 + domain_confidence * 0.4
        
        # COMPLEX_RELATION_QUERYè¯„åˆ† (æ›´é•¿ã€æ›´å¤æ‚çš„æŸ¥è¯¢)
        if features['word_count'] > 8 and domain_confidence > 0.5:
            scores['COMPLEX_RELATION_QUERY'] = 0.5 + (features['word_count'] / 20.0)
        
        # DOMAIN_CHITCHATè¯„åˆ†
        if features['has_greeting_words'] or ('basketball' in text and features['word_count'] < 5):
            scores['DOMAIN_CHITCHAT'] = 0.6 + domain_confidence * 0.4
        
        # æ ‡å‡†åŒ–åˆ†æ•°
        max_score = max(scores.values())
        if max_score > 0:
            for intent in scores:
                scores[intent] = scores[intent] / max_score * 0.9
        else:
            scores['OUT_OF_DOMAIN'] = 0.8
        
        return scores
    
    def _extract_entities_ai(self, text: str, intent: str) -> Dict[str, List[str]]:
        """AIé©±åŠ¨çš„å®ä½“æå–"""
        try:
            entities = {'players': [], 'teams': [], 'attributes': []}
            
            if intent == 'OUT_OF_DOMAIN':
                return entities
            
            # æ™ºèƒ½å®ä½“æå–
            entities = self._smart_entity_extraction(text)
            
            # åŸºäºæ„å›¾çš„å®ä½“ä¼˜åŒ–
            entities = self._refine_entities_by_intent(entities, intent, text)
            
            return entities
            
        except Exception as e:
            logger.warning(f"âš ï¸ AI entity extraction failed: {str(e)}")
            return {'players': [], 'teams': [], 'attributes': []}
    
    def _smart_entity_extraction(self, text: str) -> Dict[str, List[str]]:
        """æ™ºèƒ½å®ä½“æå–ä½¿ç”¨å¤šç§æŠ€æœ¯"""
        entities = {'players': [], 'teams': [], 'attributes': []}
        
        # çƒå‘˜æå–ï¼ˆå«æ¨¡ç³ŠåŒ¹é…ï¼‰
        for player in self.basketball_entities['players']:
            if player.lower() in text:
                entities['players'].append(player.title())
            elif self._fuzzy_match(player.lower(), text):
                entities['players'].append(player.title())
        
        # çƒé˜Ÿæå–
        for team in self.basketball_entities['teams']:
            if team.lower() in text:
                entities['teams'].append(team.title())
        
        # å±æ€§æå–
        for attr in self.basketball_entities['attributes']:
            if attr.lower() in text:
                entities['attributes'].append(attr)
        
        # å»é‡
        for key in entities:
            entities[key] = list(dict.fromkeys(entities[key]))
        
        return entities
    
    def _fuzzy_match(self, target: str, text: str, threshold: float = 0.8) -> bool:
        """ç®€å•æ¨¡ç³ŠåŒ¹é…"""
        words = text.split()
        target_words = target.split()
        
        for word in words:
            for target_word in target_words:
                if len(target_word) > 3 and target_word in word:
                    return True
        return False
    
    def _refine_entities_by_intent(self, entities: Dict[str, List[str]], 
                                 intent: str, text: str) -> Dict[str, List[str]]:
        """åŸºäºæ„å›¾ä¼˜åŒ–æå–çš„å®ä½“"""
        # ATTRIBUTE_QUERY: ç¡®ä¿æœ‰å±æ€§
        if intent == 'ATTRIBUTE_QUERY' and not entities['attributes']:
            if any(word in text for word in ['old', 'age']):
                entities['attributes'].append('age')
            elif any(word in text for word in ['tall', 'height']):
                entities['attributes'].append('height')
            elif any(word in text for word in ['weight', 'heavy']):
                entities['attributes'].append('weight')
        
        return entities
    
    def _is_basketball_domain(self, text: str, entities: Dict[str, List[str]]) -> bool:
        """åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦ä¸ç¯®çƒç›¸å…³"""
        try:
            if any(entities.values()):
                return True
            
            basketball_keywords = [
                'basketball', 'nba', 'game', 'player', 'team', 'sport',
                'court', 'ball', 'shoot', 'score', 'championship'
            ]
            
            return any(keyword in text for keyword in basketball_keywords)
            
        except Exception as e:
            logger.warning(f"âš ï¸ Domain check failed: {str(e)}")
            return False
    
    def _fallback_classification(self, text: str, start_time: float) -> ParsedIntent:
        """å›é€€åˆ†ç±»æ–¹æ³•"""
        return ParsedIntent(
            intent='OUT_OF_DOMAIN',
            confidence=0.0,
            players=[],
            teams=[],
            attributes=[],
            entities={},
            is_basketball_related=False,
            processing_time=time.time() - start_time
        )

# =============================================================================
# æµ‹è¯•å‡½æ•°
# =============================================================================

def test_language_manager():
    """æµ‹è¯•è¯­è¨€ç®¡ç†å™¨"""
    print("=" * 60)
    print("ğŸŒ æµ‹è¯•å…¨å±€è¯­è¨€çŠ¶æ€ç®¡ç†å™¨")
    print("=" * 60)
    
    manager = GlobalLanguageStateManager()
    
    test_queries = [
        "ç§‘æ¯”å¤šå°‘å²",
        "How old is Kobe Bryant", 
        "å§šæ˜å¤šé«˜",
        "Compare Kobe versus Jordan",
        "ä½ å¥½",
        "Hello"
    ]
    
    for query in test_queries:
        result = manager.detect_and_normalize_language(query)
        print(f"åŸå§‹æŸ¥è¯¢: {query}")
        print(f"æ£€æµ‹è¯­è¨€: {result.original_language} (ç½®ä¿¡åº¦: {result.confidence_score:.2f})")
        print(f"æ ‡å‡†åŒ–æ–‡æœ¬: {result.normalized_text}")
        print(f"æ˜¯å¦ç¿»è¯‘: {result.translation_applied}")
        print("-" * 40)

def test_intent_classifier():
    """æµ‹è¯•æ„å›¾åˆ†ç±»å™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§  æµ‹è¯•ç»Ÿä¸€æ„å›¾åˆ†ç±»å™¨")
    print("=" * 60)
    
    classifier = UnifiedIntentClassifier()
    
    test_queries = [
        NormalizedQuery("How old is Kobe Bryant", "how old is kobe bryant", "en", 0.9),
        NormalizedQuery("Compare Kobe versus Jordan", "compare kobe versus jordan", "en", 0.9),
        NormalizedQuery("What team does LeBron play for", "what team does lebron play for", "en", 0.9),
        NormalizedQuery("Hello", "hello", "en", 0.9),
        NormalizedQuery("What is the weather today", "what is the weather today", "en", 0.9),
        NormalizedQuery("Tell me about basketball", "tell me about basketball", "en", 0.9)
    ]
    
    for normalized_query in test_queries:
        result = classifier.classify_and_extract(normalized_query)
        print(f"æŸ¥è¯¢: {normalized_query.normalized_text}")
        print(f"æ„å›¾: {result.intent} (ç½®ä¿¡åº¦: {result.confidence:.2f})")
        print(f"ç¯®çƒç›¸å…³: {result.is_basketball_related}")
        print(f"å®ä½“: çƒå‘˜={result.players}, çƒé˜Ÿ={result.teams}, å±æ€§={result.attributes}")
        print(f"å¤„ç†æ—¶é—´: {result.processing_time:.4f}s")
        print("-" * 40)

def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´ç®¡é“"""
    print("\n" + "=" * 60)
    print("ğŸš€ æµ‹è¯•å®Œæ•´AIå¤„ç†ç®¡é“")
    print("=" * 60)
    
    language_manager = GlobalLanguageStateManager()
    intent_classifier = UnifiedIntentClassifier()
    
    test_queries = [
        "ç§‘æ¯”å¤šå°‘å²",
        "How old is Kobe Bryant",
        "Compare Kobe versus Jordan", 
        "å§šæ˜åœ¨å“ªä¸ªé˜Ÿ",
        "Hello, tell me about basketball",
        "What is the weather today"
    ]
    
    for query in test_queries:
        print(f"\nå¤„ç†æŸ¥è¯¢: {query}")
        start_time = time.time()
        
        # Stage 1: è¯­è¨€æ ‡å‡†åŒ–
        normalized_query = language_manager.detect_and_normalize_language(query)
        
        # Stage 2: æ„å›¾åˆ†ç±»å’Œå®ä½“æå–
        parsed_intent = intent_classifier.classify_and_extract(normalized_query)
        
        total_time = time.time() - start_time
        
        print(f"åŸå§‹è¯­è¨€: {normalized_query.original_language}")
        print(f"æ ‡å‡†åŒ–æ–‡æœ¬: {normalized_query.normalized_text}")
        print(f"æ„å›¾: {parsed_intent.intent}")
        print(f"ç½®ä¿¡åº¦: {parsed_intent.confidence:.2f}")
        print(f"çƒå‘˜: {parsed_intent.players}")
        print(f"çƒé˜Ÿ: {parsed_intent.teams}")
        print(f"å±æ€§: {parsed_intent.attributes}")
        print(f"ç¯®çƒç›¸å…³: {parsed_intent.is_basketball_related}")
        print(f"æ€»å¤„ç†æ—¶é—´: {total_time:.4f}s")
        print("-" * 40)

if __name__ == "__main__":
    print("ğŸš€ Smart Pre-processor æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        # æµ‹è¯•å„ä¸ªç»„ä»¶
        test_language_manager()
        test_intent_classifier()
        test_full_pipeline()
        
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼Smart Pre-processor æ ¸å¿ƒåŠŸèƒ½å·¥ä½œæ­£å¸¸")
        print("ğŸ¯ Stage 3 (æ™ºèƒ½æ„å›¾è¯†åˆ«) å’Œ Stage 4 (å®ä½“æå–) å·²æˆåŠŸå®ç°")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
