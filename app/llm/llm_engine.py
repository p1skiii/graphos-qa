"""
LLMå¼•æ“æ ¸å¿ƒç»„ä»¶
å®ç°Phi-3-miniæ¨¡å‹çš„åŠ è½½ã€æ¨ç†å’Œå¤šæ¨¡æ€èåˆ
åŸºäºG-Retrieverçš„æŠ•å½±å™¨è®¾è®¡
"""
import os
import time
import logging
from typing import Dict, Any, Optional, List, Union, Tuple
from dataclasses import dataclass
import json

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
    import numpy as np
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False

from .config import LLMConfig, Phi3Config
from .input_router import UnifiedInput
from .prompt_templates import PromptTemplateManager

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    """LLMå“åº”æ•°æ®ç»“æ„"""
    content: str
    metadata: Dict[str, Any]
    processing_time: float
    token_usage: Dict[str, int]
    extra_data: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        result = {
            'content': self.content,
            'metadata': self.metadata,
            'processing_time': self.processing_time,
            'token_usage': self.token_usage
        }
        if self.extra_data:
            result['extra_data'] = self.extra_data
        return result

class GraphProjector(nn.Module):
    """å›¾åµŒå…¥æŠ•å½±å™¨ - åŸºäºG-Retrieverè®¾è®¡"""
    
    def __init__(self, graph_dim: int = 128, llm_dim: int = 4096, hidden_dim: int = 512):
        """åˆå§‹åŒ–æŠ•å½±å™¨
        
        Args:
            graph_dim: å›¾åµŒå…¥ç»´åº¦ï¼ˆæ¥è‡ªGraphEncoderï¼‰
            llm_dim: LLMè¯æ±‡è¡¨ç©ºé—´ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super().__init__()
        self.graph_dim = graph_dim
        self.llm_dim = llm_dim
        self.hidden_dim = hidden_dim
        
        # å¤šå±‚æŠ•å½±ç½‘ç»œ
        self.projection = nn.Sequential(
            nn.Linear(graph_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, llm_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.projection:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, graph_embedding: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­
        
        Args:
            graph_embedding: å›¾åµŒå…¥å¼ é‡ [batch_size, graph_dim]
            
        Returns:
            æŠ•å½±åçš„å¼ é‡ [batch_size, llm_dim]
        """
        return self.projection(graph_embedding)

class LLMEngine:
    """LLMæ¨ç†å¼•æ“"""
    
    def __init__(self, config: LLMConfig):
        """åˆå§‹åŒ–LLMå¼•æ“"""
        self.config = config
        self.model = None
        self.tokenizer = None
        self.graph_projector = None
        self.prompt_manager = PromptTemplateManager()
        
        # çŠ¶æ€è·Ÿè¸ª
        self.is_loaded = False
        self.device = config.model_config.device
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens_generated': 0,
            'total_processing_time': 0.0,
            'multimodal_requests': 0,
            'text_only_requests': 0
        }
        
        logger.info(f"ğŸ¤– LLMEngineåˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {config.model_config.model_name}")
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        if not HAS_TRANSFORMERS:
            logger.error("âŒ transformersåº“æœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
            return False
        
        try:
            start_time = time.time()
            model_config = self.config.model_config
            
            logger.info(f"ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹: {model_config.model_name}")
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_config.model_name,
                trust_remote_code=getattr(model_config, 'trust_remote_code', True),
                padding_side='left'
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            model_kwargs = {
                'trust_remote_code': getattr(model_config, 'trust_remote_code', True),
                'torch_dtype': getattr(torch, model_config.torch_dtype, torch.float32),
                'device_map': 'auto' if model_config.device == 'cuda' else None,
                'attn_implementation': getattr(model_config, 'attn_implementation', 'eager')
            }
            
            # MacOS MPSæ”¯æŒ
            if model_config.device == 'mps':
                model_kwargs['device_map'] = None
                model_kwargs['torch_dtype'] = torch.float32
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_config.model_name,
                **model_kwargs
            )
            
            # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            if model_config.device in ['cpu', 'mps']:
                self.model = self.model.to(model_config.device)
            
            self.model.eval()
            
            # åˆå§‹åŒ–å›¾æŠ•å½±å™¨
            if self.config.enable_multimodal:
                self._init_graph_projector()
            
            loading_time = time.time() - start_time
            self.is_loaded = True
            
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {loading_time:.2f}ç§’")
            logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.is_loaded = False
            return False
    
    def _init_graph_projector(self):
        """åˆå§‹åŒ–å›¾æŠ•å½±å™¨"""
        try:
            # è·å–LLMçš„éšè—ç»´åº¦
            llm_hidden_dim = self.model.config.hidden_size
            
            self.graph_projector = GraphProjector(
                graph_dim=self.config.graph_embedding_dim,
                llm_dim=llm_hidden_dim,
                hidden_dim=512
            )
            
            self.graph_projector.to(self.device)
            self.graph_projector.eval()
            
            logger.info(f"ğŸ¯ å›¾æŠ•å½±å™¨åˆå§‹åŒ–å®Œæˆ: {self.config.graph_embedding_dim}d â†’ {llm_hidden_dim}d")
            
        except Exception as e:
            logger.error(f"âŒ å›¾æŠ•å½±å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.graph_projector = None
    
    def generate_response(self, unified_input: UnifiedInput) -> LLMResponse:
        """ç”Ÿæˆå“åº”"""
        if not self.is_loaded:
            return self._create_error_response("æ¨¡å‹æœªåŠ è½½", unified_input)
        
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        try:
            # æ›´æ–°ç»Ÿè®¡
            if unified_input.has_multimodal_data():
                self.stats['multimodal_requests'] += 1
            else:
                self.stats['text_only_requests'] += 1
            
            # æ„å»ºprompt
            prompt = self._build_prompt(unified_input)
            
            # å¤„ç†å¤šæ¨¡æ€è¾“å…¥
            processed_input = self._process_multimodal_input(unified_input, prompt)
            
            # ç”Ÿæˆå“åº”
            response_text = self._generate_text(processed_input['text'], processed_input['extra_inputs'])
            
            # åå¤„ç†
            formatted_response = self._post_process_response(response_text, unified_input)
            
            processing_time = time.time() - start_time
            
            # åˆ›å»ºå“åº”å¯¹è±¡
            response = LLMResponse(
                content=formatted_response,
                metadata={
                    'processor_type': unified_input.processor_type,
                    'has_multimodal': unified_input.has_multimodal_data(),
                    'prompt_length': len(prompt),
                    'model_name': self.config.model_config.model_name,
                    'fusion_metadata': processed_input.get('fusion_metadata', {}),
                    'extra_inputs_provided': processed_input.get('extra_inputs') is not None
                },
                processing_time=processing_time,
                token_usage=self._calculate_token_usage(prompt, response_text),
                extra_data={
                    'fusion_strategy': self.config.fusion_strategy,
                    'processed_input': processed_input
                }
            )
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['successful_requests'] += 1
            self.stats['total_processing_time'] += processing_time
            self.stats['total_tokens_generated'] += response.token_usage.get('output_tokens', 0)
            
            logger.info(f"âœ… å“åº”ç”ŸæˆæˆåŠŸï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            return response
            
        except Exception as e:
            logger.error(f"âŒ å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
            self.stats['failed_requests'] += 1
            return self._create_error_response(str(e), unified_input)
    
    def _build_prompt(self, unified_input: UnifiedInput) -> str:
        """æ„å»ºprompt"""
        try:
            system_prompt = self.config.system_prompt
            prompt = self.prompt_manager.format_prompt_for_input(unified_input, system_prompt)
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if len(prompt) > self.config.max_input_tokens * 4:  # å¤§æ¦‚ä¼°ç®—
                logger.warning(f"âš ï¸ Promptè¿‡é•¿ï¼Œå¯èƒ½è¢«æˆªæ–­")
            
            return prompt
            
        except Exception as e:
            logger.error(f"âŒ Promptæ„å»ºå¤±è´¥: {str(e)}")
            # ä½¿ç”¨ç®€å•çš„å›é€€prompt
            return f"<|user|>\n{unified_input.query}<|end|>\n<|assistant|>\n"
    
    def _process_multimodal_input(self, unified_input: UnifiedInput, prompt: str) -> Dict[str, Any]:
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥ - åŸºäºG-Retrieverçš„æ·±åº¦èåˆ"""
        result = {
            'text': prompt,
            'extra_inputs': None,
            'fusion_metadata': {}
        }
        
        # å¦‚æœæ²¡æœ‰å¤šæ¨¡æ€æ•°æ®æˆ–æŠ•å½±å™¨ï¼Œç›´æ¥è¿”å›æ–‡æœ¬
        if not unified_input.has_multimodal_data() or not self.graph_projector:
            result['fusion_metadata']['fusion_applied'] = False
            return result
        
        try:
            # è·å–å›¾åµŒå…¥
            graph_embedding = unified_input.get_graph_embedding()
            if not graph_embedding:
                result['fusion_metadata']['fusion_applied'] = False
                return result
            
            logger.info(f"ğŸ­ å¼€å§‹å¤šæ¨¡æ€èåˆå¤„ç†ï¼Œå›¾åµŒå…¥ç»´åº¦: {len(graph_embedding)}")
            
            # è½¬æ¢ä¸ºtensor
            graph_tensor = torch.tensor(graph_embedding, dtype=torch.float32).unsqueeze(0)  # [1, graph_dim]
            graph_tensor = graph_tensor.to(self.device)
            
            # æŠ•å½±åˆ°LLMç©ºé—´
            with torch.no_grad():
                projected_embedding = self.graph_projector(graph_tensor)  # [1, llm_dim]
            
            # åŸºäºG-Retrieverçš„èåˆç­–ç•¥å®ç°
            fusion_result = self._apply_fusion_strategy(
                prompt, 
                projected_embedding, 
                unified_input, 
                graph_embedding
            )
            
            result.update(fusion_result)
            result['fusion_metadata'].update({
                'fusion_applied': True,
                'graph_embedding_dim': len(graph_embedding),
                'projected_dim': projected_embedding.shape[-1],
                'fusion_strategy': self.config.fusion_strategy,
                'has_multimodal_context': unified_input.multimodal_context is not None
            })
            
            logger.info(f"âœ… å¤šæ¨¡æ€èåˆå®Œæˆï¼Œç­–ç•¥: {self.config.fusion_strategy}")
            
        except Exception as e:
            logger.error(f"âŒ å¤šæ¨¡æ€è¾“å…¥å¤„ç†å¤±è´¥: {str(e)}")
            result['fusion_metadata']['fusion_applied'] = False
            result['fusion_metadata']['error'] = str(e)
        
        return result
    
    def _apply_fusion_strategy(self, prompt: str, projected_embedding: torch.Tensor, 
                             unified_input: UnifiedInput, graph_embedding: List[float]) -> Dict[str, Any]:
        """åº”ç”¨èåˆç­–ç•¥ - åŸºäºG-Retrieverè®ºæ–‡å®ç°"""
        try:
            if self.config.fusion_strategy == "concatenate":
                return self._concatenate_fusion(prompt, projected_embedding, unified_input, graph_embedding)
            elif self.config.fusion_strategy == "weighted":
                return self._weighted_fusion(prompt, projected_embedding, unified_input, graph_embedding)
            elif self.config.fusion_strategy == "attention":
                return self._attention_fusion(prompt, projected_embedding, unified_input, graph_embedding)
            else:
                # é»˜è®¤concatenate
                return self._concatenate_fusion(prompt, projected_embedding, unified_input, graph_embedding)
                
        except Exception as e:
            logger.error(f"âŒ èåˆç­–ç•¥åº”ç”¨å¤±è´¥: {str(e)}")
            return {'text': prompt, 'extra_inputs': None}
    
    def _concatenate_fusion(self, prompt: str, projected_embedding: torch.Tensor, 
                          unified_input: UnifiedInput, graph_embedding: List[float]) -> Dict[str, Any]:
        """æ‹¼æ¥èåˆç­–ç•¥ - G-Retrieveræ ¸å¿ƒæ–¹æ³•"""
        # 1. æå–å›¾è°±è¯­ä¹‰ä¿¡æ¯æè¿°
        graph_description = self._extract_graph_semantics(unified_input)
        
        # 2. æ„å»ºèåˆæ–‡æœ¬ - å°†å›¾è°±ä¿¡æ¯åµŒå…¥åˆ°promptä¸­
        fusion_text = self._build_fusion_text(prompt, graph_description, unified_input)
        
        # 3. å‡†å¤‡é¢å¤–è¾“å…¥ï¼ˆæŠ•å½±åçš„å›¾åµŒå…¥ç”¨äºæ½œåœ¨çš„æ·±åº¦èåˆï¼‰
        extra_inputs = {
            'graph_projection': projected_embedding,
            'raw_graph_embedding': graph_embedding,
            'graph_semantics': graph_description
        }
        
        return {
            'text': fusion_text,
            'extra_inputs': extra_inputs
        }
    
    def _weighted_fusion(self, prompt: str, projected_embedding: torch.Tensor, 
                        unified_input: UnifiedInput, graph_embedding: List[float]) -> Dict[str, Any]:
        """åŠ æƒèåˆç­–ç•¥"""
        # è®¡ç®—æ–‡æœ¬å’Œå›¾åµŒå…¥çš„æƒé‡
        text_weight = 0.7  # æ–‡æœ¬æƒé‡
        graph_weight = 0.3  # å›¾æƒé‡
        
        # æ„å»ºåŠ æƒæè¿°
        graph_description = self._extract_graph_semantics(unified_input)
        weighted_description = f"(æƒé‡{graph_weight:.1f}) {graph_description}"
        
        fusion_text = self._build_fusion_text(prompt, weighted_description, unified_input)
        
        extra_inputs = {
            'graph_projection': projected_embedding * graph_weight,
            'weights': {'text': text_weight, 'graph': graph_weight},
            'fusion_mode': 'weighted'
        }
        
        return {
            'text': fusion_text,
            'extra_inputs': extra_inputs
        }
    
    def _attention_fusion(self, prompt: str, projected_embedding: torch.Tensor, 
                         unified_input: UnifiedInput, graph_embedding: List[float]) -> Dict[str, Any]:
        """æ³¨æ„åŠ›èåˆç­–ç•¥ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼ˆç®€åŒ–å®ç°ï¼‰
        attention_score = min(1.0, len(graph_embedding) / 128.0)  # åŸºäºåµŒå…¥ç»´åº¦çš„ç®€å•æ³¨æ„åŠ›
        
        graph_description = self._extract_graph_semantics(unified_input)
        attention_description = f"(æ³¨æ„åŠ›{attention_score:.2f}) {graph_description}"
        
        fusion_text = self._build_fusion_text(prompt, attention_description, unified_input)
        
        extra_inputs = {
            'graph_projection': projected_embedding * attention_score,
            'attention_score': attention_score,
            'fusion_mode': 'attention'
        }
        
        return {
            'text': fusion_text,
            'extra_inputs': extra_inputs
        }
    
    def _extract_graph_semantics(self, unified_input: UnifiedInput) -> str:
        """æå–å›¾è°±è¯­ä¹‰ä¿¡æ¯"""
        try:
            # ä»multimodal_contextä¸­æå–è¯­ä¹‰ä¿¡æ¯
            if unified_input.multimodal_context:
                metadata = unified_input.multimodal_context.metadata
                if 'graph_summary' in metadata:
                    return metadata['graph_summary']
            
            # ä»processor_dataä¸­æå–å›¾è°±ä¿¡æ¯
            if unified_input.processor_data:
                if 'graph_embedding' in unified_input.processor_data:
                    graph_info = unified_input.processor_data['graph_embedding']
                    if isinstance(graph_info, dict) and 'encoding_metadata' in graph_info:
                        return graph_info['encoding_metadata'].get('summary', 'å›¾è°±ç»“æ„ç‰¹å¾')
                
                # ä»traditional_resultä¸­æå–å›¾è°±ç»“æ„ä¿¡æ¯
                if 'traditional_result' in unified_input.processor_data:
                    trad_result = unified_input.processor_data['traditional_result']
                    if 'graph' in trad_result:
                        graph_data = trad_result['graph']
                        return self._summarize_graph_structure(graph_data)
            
            # é»˜è®¤æè¿°
            graph_dim = len(unified_input.get_graph_embedding()) if unified_input.get_graph_embedding() else 0
            return f"å›¾è°±ç»“æ„ç‰¹å¾ï¼ˆ{graph_dim}ç»´å‘é‡è¡¨ç¤ºï¼‰"
            
        except Exception as e:
            logger.warning(f"âš ï¸ å›¾è°±è¯­ä¹‰æå–å¤±è´¥: {str(e)}")
            return "å›¾è°±ç»“æ„ä¿¡æ¯"
    
    def _summarize_graph_structure(self, graph_data: Dict[str, Any]) -> str:
        """æ€»ç»“å›¾è°±ç»“æ„ä¿¡æ¯"""
        try:
            summary_parts = []
            
            if 'graph_structure' in graph_data:
                structure = graph_data['graph_structure']
                if 'num_nodes' in structure:
                    summary_parts.append(f"{structure['num_nodes']}ä¸ªèŠ‚ç‚¹")
                if 'num_edges' in structure:
                    summary_parts.append(f"{structure['num_edges']}æ¡è¾¹")
            
            if 'metadata' in graph_data:
                metadata = graph_data['metadata']
                if 'entity_types' in metadata:
                    types = metadata['entity_types']
                    if types:
                        summary_parts.append(f"å®ä½“ç±»å‹: {', '.join(types[:3])}")
            
            return "å›¾è°±åŒ…å«" + "ã€".join(summary_parts) if summary_parts else "å›¾è°±ç»“æ„ç‰¹å¾"
            
        except Exception:
            return "å›¾è°±ç»“æ„ç‰¹å¾"
    
    def _build_fusion_text(self, original_prompt: str, graph_description: str, 
                          unified_input: UnifiedInput) -> str:
        """æ„å»ºèåˆæ–‡æœ¬ - å°†å›¾è°±ä¿¡æ¯è‡ªç„¶åœ°èå…¥prompt"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æœ¬ä¸Šä¸‹æ–‡
            text_context = unified_input.get_text_content()
            
            # æ„å»ºå›¾è°±ä¿¡æ¯æè¿°
            graph_info_block = f"""
[å›¾è°±åˆ†æ]
{graph_description}
ç›¸å…³å›¾è°±ä¸Šä¸‹æ–‡: {text_context[:200] + '...' if len(text_context) > 200 else text_context}
"""
            
            # æ™ºèƒ½æ’å…¥å›¾è°±ä¿¡æ¯
            if '<|assistant|>' in original_prompt:
                # åœ¨assistantå›å¤å‰æ’å…¥å›¾è°±ä¿¡æ¯
                fusion_prompt = original_prompt.replace(
                    '<|assistant|>', 
                    f'{graph_info_block}\n<|assistant|>\næ ¹æ®ä¸Šè¿°æ–‡æœ¬å’Œå›¾è°±ä¿¡æ¯ï¼Œæˆ‘æ¥å›ç­”ï¼š'
                )
            else:
                # åœ¨promptæœ«å°¾æ·»åŠ å›¾è°±ä¿¡æ¯
                fusion_prompt = f"{original_prompt}\n\n{graph_info_block}\n\nè¯·ç»“åˆæ–‡æœ¬å’Œå›¾è°±ä¿¡æ¯å›ç­”é—®é¢˜ã€‚"
            
            return fusion_prompt
            
        except Exception as e:
            logger.warning(f"âš ï¸ èåˆæ–‡æœ¬æ„å»ºå¤±è´¥: {str(e)}")
            return original_prompt
    
    def _generate_text(self, prompt: str, extra_inputs: Optional[Dict[str, Any]] = None) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        try:
            # Tokenizeè¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=self.config.max_input_tokens,
                truncation=True,
                padding=False
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = inputs['input_ids'].to(self.device)
            attention_mask = inputs['attention_mask'].to(self.device)
            
            # ç”Ÿæˆé…ç½®
            generation_config = GenerationConfig(
                max_new_tokens=self.config.max_output_tokens,
                temperature=self.config.model_config.temperature,
                top_p=self.config.model_config.top_p,
                top_k=self.config.model_config.top_k,
                do_sample=self.config.model_config.do_sample,
                num_beams=self.config.model_config.num_beams,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1,
                length_penalty=1.0
            )
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    generation_config=generation_config,
                    use_cache=True
                )
            
            # è§£ç å“åº”
            response_ids = outputs[0][input_ids.shape[1]:]  # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            response_text = self.tokenizer.decode(response_ids, skip_special_tokens=True)
            
            return response_text.strip()
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def _post_process_response(self, response_text: str, unified_input: UnifiedInput) -> str:
        """åå¤„ç†å“åº”"""
        try:
            # åŸºæœ¬æ¸…ç†
            response = response_text.strip()
            
            # ç§»é™¤å¯èƒ½çš„é‡å¤æˆ–å¼‚å¸¸å†…å®¹
            if response.count('\n') > 10:  # è¿‡å¤šæ¢è¡Œ
                lines = response.split('\n')
                response = '\n'.join(lines[:10])
            
            # ç¡®ä¿å“åº”ä¸ä¸ºç©º
            if not response:
                response = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åŸºäºæä¾›çš„ä¿¡æ¯å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
            
            # Markdownæ ¼å¼åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.format_markdown:
                response = self._format_as_markdown(response)
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ å“åº”åå¤„ç†å¤±è´¥: {str(e)}")
            return response_text
    
    def _format_as_markdown(self, text: str) -> str:
        """æ ¼å¼åŒ–ä¸ºMarkdown"""
        # ç®€å•çš„Markdownæ ¼å¼åŒ–
        formatted = text
        
        # æ·»åŠ é€‚å½“çš„é—´è·
        if not formatted.endswith('\n'):
            formatted += '\n'
        
        return formatted
    
    def _calculate_token_usage(self, prompt: str, response: str) -> Dict[str, int]:
        """è®¡ç®—tokenä½¿ç”¨é‡"""
        try:
            prompt_tokens = len(self.tokenizer.encode(prompt))
            response_tokens = len(self.tokenizer.encode(response))
            
            return {
                'input_tokens': prompt_tokens,
                'output_tokens': response_tokens,
                'total_tokens': prompt_tokens + response_tokens
            }
        except:
            return {
                'input_tokens': len(prompt) // 4,  # ç²—ç•¥ä¼°ç®—
                'output_tokens': len(response) // 4,
                'total_tokens': (len(prompt) + len(response)) // 4
            }
    
    def _create_error_response(self, error_message: str, unified_input: UnifiedInput) -> LLMResponse:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return LLMResponse(
            content=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {error_message}",
            metadata={
                'error': True,
                'error_message': error_message,
                'processor_type': unified_input.processor_type
            },
            processing_time=0.0,
            token_usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = max(1, self.stats['total_requests'])
        
        return {
            'model_loaded': self.is_loaded,
            'model_name': self.config.model_config.model_name,
            'device': self.device,
            'multimodal_enabled': self.config.enable_multimodal,
            'graph_projector_enabled': self.graph_projector is not None,
            'requests': {
                'total': self.stats['total_requests'],
                'successful': self.stats['successful_requests'],
                'failed': self.stats['failed_requests'],
                'success_rate': self.stats['successful_requests'] / total_requests
            },
            'performance': {
                'avg_processing_time': self.stats['total_processing_time'] / max(1, self.stats['successful_requests']),
                'total_tokens_generated': self.stats['total_tokens_generated'],
                'avg_tokens_per_request': self.stats['total_tokens_generated'] / max(1, self.stats['successful_requests'])
            },
            'input_distribution': {
                'multimodal_requests': self.stats['multimodal_requests'],
                'text_only_requests': self.stats['text_only_requests'],
                'multimodal_ratio': self.stats['multimodal_requests'] / total_requests
            }
        }
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens_generated': 0,
            'total_processing_time': 0.0,
            'multimodal_requests': 0,
            'text_only_requests': 0
        }

# =============================================================================
# å·¥å‚å‡½æ•°
# =============================================================================

def create_llm_engine(config: Optional[LLMConfig] = None) -> LLMEngine:
    """åˆ›å»ºLLMå¼•æ“"""
    if config is None:
        from .config import get_macos_optimized_config
        config = get_macos_optimized_config()
    
    return LLMEngine(config)

# =============================================================================
# æµ‹è¯•å’Œæ¼”ç¤º
# =============================================================================

if __name__ == "__main__":
    # åˆ›å»ºé…ç½®
    from .config import get_macos_optimized_config
    from .input_router import UnifiedInput
    
    print("ğŸ¤– LLMå¼•æ“æµ‹è¯•")
    
    # åˆ›å»ºå¼•æ“
    config = get_macos_optimized_config()
    engine = create_llm_engine(config)
    
    print(f"å¼•æ“é…ç½®: {config.model_config.model_name}")
    print(f"è®¾å¤‡: {config.model_config.device}")
    print(f"å¤šæ¨¡æ€æ”¯æŒ: {config.enable_multimodal}")
    
    # æ³¨æ„ï¼šå®é™…åŠ è½½æ¨¡å‹éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¿™é‡Œåªåšé…ç½®æµ‹è¯•
    print("âœ… å¼•æ“åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = UnifiedInput(
        query="ç§‘æ¯”æ˜¯è°ï¼Ÿ",
        processor_type="direct",
        text_context="ç§‘æ¯”æ˜¯æ´›æ‰çŸ¶æ¹–äººé˜Ÿçš„ä¼ å¥‡çƒå‘˜ã€‚"
    )
    
    print(f"æµ‹è¯•è¾“å…¥: {test_input}")
    print(f"ç»Ÿè®¡ä¿¡æ¯: {engine.get_stats()}")
