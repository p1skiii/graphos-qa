"""
Phase 1 LLMé›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯•åŸºç¡€LLMæ¡†æ¶å’Œç»Ÿä¸€è¾“å…¥æ¥å£çš„åŠŸèƒ½
"""
import sys
import os
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/Users/wang/i/graphos-qa')

try:
    from app.llm import (
        LLMConfig, Phi3Config, 
        LLMEngine, create_llm_engine,
        InputRouter, UnifiedInput, LLMInputConfig, create_input_router,
        PromptTemplateManager, create_prompt_template_manager,
        ResponseFormatter, create_response_formatter,
        LLMFactory, llm_factory, create_llm_system
    )
    print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰LLMæ¨¡å—")
except ImportError as e:
    print(f"âŒ LLMæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

def test_config_creation():
    """æµ‹è¯•é…ç½®åˆ›å»º"""
    print("\nğŸ§ª æµ‹è¯•1: é…ç½®åˆ›å»º")
    print("-" * 40)
    
    try:
        # æµ‹è¯•Phi-3é…ç½®
        phi3_config = Phi3Config()
        print(f"âœ… Phi-3é…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å‹: {phi3_config.model_name}")
        print(f"   è®¾å¤‡: {phi3_config.device}")
        print(f"   æœ€å¤§é•¿åº¦: {phi3_config.max_length}")
        
        # æµ‹è¯•LLMé…ç½®
        from app.llm.config import get_macos_optimized_config, validate_config
        llm_config = get_macos_optimized_config()
        print(f"âœ… LLMé…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"   å¤šæ¨¡æ€æ”¯æŒ: {llm_config.enable_multimodal}")
        print(f"   æœ€å¤§è¾“å…¥tokens: {llm_config.max_input_tokens}")
        
        # éªŒè¯é…ç½®
        is_valid = validate_config(llm_config)
        print(f"âœ… é…ç½®éªŒè¯: {'é€šè¿‡' if is_valid else 'å¤±è´¥'}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_input_router():
    """æµ‹è¯•è¾“å…¥è·¯ç”±å™¨"""
    print("\nğŸ§ª æµ‹è¯•2: è¾“å…¥è·¯ç”±å™¨")
    print("-" * 40)
    
    try:
        # åˆ›å»ºè¾“å…¥è·¯ç”±å™¨
        router = create_input_router()
        print(f"âœ… è¾“å…¥è·¯ç”±å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„å¤„ç†å™¨è¾“å‡º
        test_cases = [
            {
                'name': 'Directå¤„ç†å™¨è¾“å‡º',
                'processor_output': {
                    'textual_context': {'formatted_text': 'ç§‘æ¯”æ˜¯æ´›æ‰çŸ¶æ¹–äººé˜Ÿçš„ä¼ å¥‡çƒå‘˜ã€‚'},
                    'metadata': {'processor_name': 'direct_processor'}
                },
                'query': 'ç§‘æ¯”æ˜¯è°ï¼Ÿ'
            },
            {
                'name': 'Simple Gå¤„ç†å™¨è¾“å‡º',
                'processor_output': {
                    'textual_context': {'formatted_text': 'ç§‘æ¯”å’Œæ²™å…‹æ˜¯æ¹–äººé˜Ÿçš„é»„é‡‘æ­æ¡£ã€‚'},
                    'graph': {'nodes': [{'id': '1', 'name': 'ç§‘æ¯”'}], 'edges': []},
                    'metadata': {'processor_name': 'simple_g_processor'}
                },
                'query': 'ç§‘æ¯”å’Œæ²™å…‹çš„å…³ç³»ï¼Ÿ'
            },
            {
                'name': 'Complex Gå¤„ç†å™¨å¢å¼ºæ¨¡å¼è¾“å‡º',
                'processor_output': {
                    'mode': 'enhanced',
                    'multimodal_context': None,  # å®é™…ä½¿ç”¨ä¸­ä¼šæ˜¯MultimodalContextå¯¹è±¡
                    'graph_embedding': {'embedding': [0.1, 0.2, 0.3, 0.4, 0.5]},
                    'traditional_result': {
                        'textual_context': {'formatted_text': 'ç§‘æ¯”å’Œè©¹å§†æ–¯éƒ½æ˜¯NBAå·¨æ˜Ÿã€‚'}
                    },
                    'metadata': {'processor_name': 'complex_g_processor'}
                },
                'query': 'ç§‘æ¯”å’Œè©¹å§†æ–¯çš„å…³ç³»ï¼Ÿ'
            }
        ]
        
        for i, test_case in enumerate(test_cases):
            print(f"\n   æµ‹è¯•æ¡ˆä¾‹ {i+1}: {test_case['name']}")
            unified_input = router.route_processor_output(
                test_case['processor_output'], 
                test_case['query']
            )
            print(f"   âœ… è·¯ç”±æˆåŠŸ: {unified_input.processor_type}")
            print(f"   å¤šæ¨¡æ€æ•°æ®: {'æœ‰' if unified_input.has_multimodal_data() else 'æ— '}")
            print(f"   æ–‡æœ¬é•¿åº¦: {len(unified_input.get_text_content())}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = router.get_stats()
        print(f"\nğŸ“Š è·¯ç”±å™¨ç»Ÿè®¡:")
        print(f"   æ€»å¤„ç†æ•°: {stats['total_processed']}")
        print(f"   å¤šæ¨¡æ€æ¯”ä¾‹: {stats['multimodal_ratio']:.2%}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¾“å…¥è·¯ç”±å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_prompt_templates():
    """æµ‹è¯•Promptæ¨¡æ¿ç³»ç»Ÿ"""
    print("\nğŸ§ª æµ‹è¯•3: Promptæ¨¡æ¿ç³»ç»Ÿ")
    print("-" * 40)
    
    try:
        # åˆ›å»ºæ¨¡æ¿ç®¡ç†å™¨
        manager = create_prompt_template_manager()
        print(f"âœ… Promptæ¨¡æ¿ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   åŠ è½½æ¨¡æ¿æ•°: {len(manager.templates)}")
        
        # æµ‹è¯•æ¨¡æ¿åˆ—è¡¨
        templates = manager.list_templates()
        print(f"   å¯ç”¨æ¨¡æ¿: {templates}")
        
        # æµ‹è¯•æ¨¡æ¿æ ¼å¼åŒ–
        from app.llm.input_router import UnifiedInput
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = UnifiedInput(
            query="ç§‘æ¯”æ˜¯è°ï¼Ÿ",
            processor_type="direct",
            text_context="ç§‘æ¯”æ˜¯æ´›æ‰çŸ¶æ¹–äººé˜Ÿçš„ä¼ å¥‡çƒå‘˜ï¼Œè·å¾—è¿‡5æ¬¡æ€»å† å†›ã€‚"
        )
        
        # æ ¼å¼åŒ–prompt
        prompt = manager.format_prompt_for_input(test_input, "ä½ æ˜¯ç¯®çƒä¸“å®¶")
        print(f"âœ… Promptæ ¼å¼åŒ–æˆåŠŸï¼Œé•¿åº¦: {len(prompt)}")
        print(f"   é¢„è§ˆ: {prompt[:150]}...")
        
        # æµ‹è¯•ä¸åŒå¤„ç†å™¨ç±»å‹
        processor_types = ['direct', 'simple_g', 'complex_g', 'comparison', 'chitchat']
        for proc_type in processor_types:
            template = manager.get_template_for_processor(proc_type)
            if template:
                print(f"   âœ… {proc_type} -> {template.name}")
            else:
                print(f"   âš ï¸ {proc_type} -> æ— å¯¹åº”æ¨¡æ¿")
        
        return True
        
    except Exception as e:
        print(f"âŒ Promptæ¨¡æ¿æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_response_formatter():
    """æµ‹è¯•å“åº”æ ¼å¼åŒ–å™¨"""
    print("\nğŸ§ª æµ‹è¯•4: å“åº”æ ¼å¼åŒ–å™¨")
    print("-" * 40)
    
    try:
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = create_response_formatter()
        print(f"âœ… å“åº”æ ¼å¼åŒ–å™¨åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        from app.llm.llm_engine import LLMResponse
        from app.llm.input_router import UnifiedInput
        
        test_llm_response = LLMResponse(
            content="ç§‘æ¯”Â·å¸ƒè±æ©ç‰¹æ˜¯æ´›æ‰çŸ¶æ¹–äººé˜Ÿçš„ä¼ å¥‡çƒå‘˜ã€‚ä»–åœ¨NBAç”Ÿæ¶¯ä¸­è·å¾—äº†5æ¬¡æ€»å† å†›ï¼Œè¢«èª‰ä¸ºç¯®çƒç•Œçš„ä¼ å¥‡äººç‰©ã€‚",
            metadata={'model': 'phi-3-mini'},
            processing_time=1.5,
            token_usage={'input_tokens': 50, 'output_tokens': 100, 'total_tokens': 150}
        )
        
        test_unified_input = UnifiedInput(
            query="ç§‘æ¯”æ˜¯è°ï¼Ÿ",
            processor_type="direct",
            text_context="ç§‘æ¯”ç›¸å…³ä¿¡æ¯"
        )
        
        # æµ‹è¯•æ ¼å¼åŒ–
        formatted_response = formatter.format_response(test_llm_response, test_unified_input)
        
        print(f"âœ… å“åº”æ ¼å¼åŒ–æˆåŠŸ")
        print(f"   åŸå§‹é•¿åº¦: {len(test_llm_response.content)}")
        print(f"   æ ¼å¼åŒ–åé•¿åº¦: {len(formatted_response.content)}")
        print(f"   æ ¼å¼ç±»å‹: {formatted_response.format_type}")
        print(f"   å¤„ç†æ—¶é—´: {formatted_response.processing_info['formatting_time']:.3f}ç§’")
        
        # æ˜¾ç¤ºæ ¼å¼åŒ–åçš„å†…å®¹é¢„è§ˆ
        print(f"   å†…å®¹é¢„è§ˆ: {formatted_response.content[:100]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ å“åº”æ ¼å¼åŒ–å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_llm_engine_creation():
    """æµ‹è¯•LLMå¼•æ“åˆ›å»ºï¼ˆä¸åŠ è½½æ¨¡å‹ï¼‰"""
    print("\nğŸ§ª æµ‹è¯•5: LLMå¼•æ“åˆ›å»º")
    print("-" * 40)
    
    try:
        # åˆ›å»ºé…ç½®
        from app.llm.config import get_macos_optimized_config
        config = get_macos_optimized_config()
        
        # åˆ›å»ºå¼•æ“
        engine = create_llm_engine(config)
        print(f"âœ… LLMå¼•æ“åˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å‹åç§°: {config.model_config.model_name}")
        print(f"   è®¾å¤‡: {config.model_config.device}")
        print(f"   å¤šæ¨¡æ€æ”¯æŒ: {config.enable_multimodal}")
        print(f"   æ¨¡å‹åŠ è½½çŠ¶æ€: {engine.is_loaded}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = engine.get_stats()
        print(f"âœ… å¼•æ“ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ")
        print(f"   å¤šæ¨¡æ€å¯ç”¨: {stats['multimodal_enabled']}")
        print(f"   å›¾æŠ•å½±å™¨å¯ç”¨: {stats['graph_projector_enabled']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ LLMå¼•æ“æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_llm_factory():
    """æµ‹è¯•LLMå·¥å‚"""
    print("\nğŸ§ª æµ‹è¯•6: LLMå·¥å‚")
    print("-" * 40)
    
    try:
        # è·å–å¯ç”¨é¢„è®¾
        presets = llm_factory.list_presets()
        print(f"âœ… å¯ç”¨é¢„è®¾: {presets}")
        
        # è·å–é¢„è®¾ä¿¡æ¯
        for preset in presets:
            info = llm_factory.get_preset_info(preset)
            if info:
                print(f"   ğŸ“‹ {preset}: {info['description']}")
                print(f"      æ¨¡å‹: {info['model_name']}")
                print(f"      è®¾å¤‡: {info['device']}")
        
        # åˆ›å»ºç³»ç»Ÿ
        system = create_llm_system('macos_optimized')
        print(f"âœ… LLMç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        if system.initialize():
            print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            
            # è·å–ç³»ç»ŸçŠ¶æ€
            status = system.get_system_status()
            print(f"   åˆå§‹åŒ–çŠ¶æ€: {status['initialized']}")
            print(f"   å°±ç»ªçŠ¶æ€: {status['ready']}")
            print(f"   ç»„ä»¶çŠ¶æ€: {status['components']}")
            
        else:
            print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ LLMå·¥å‚æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_end_to_end_flow():
    """æµ‹è¯•ç«¯åˆ°ç«¯æµç¨‹ï¼ˆä¸æ¶‰åŠå®é™…æ¨¡å‹æ¨ç†ï¼‰"""
    print("\nğŸ§ª æµ‹è¯•7: ç«¯åˆ°ç«¯æµç¨‹")
    print("-" * 40)
    
    try:
        # åˆ›å»ºç³»ç»Ÿ
        system = create_llm_system('development')
        
        if not system.initialize():
            print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
        
        print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
        # æ¨¡æ‹Ÿå¤„ç†å™¨è¾“å‡º
        mock_processor_output = {
            'textual_context': {
                'formatted_text': 'ç§‘æ¯”Â·å¸ƒè±æ©ç‰¹æ˜¯NBAå†å²ä¸Šæœ€ä¼Ÿå¤§çš„çƒå‘˜ä¹‹ä¸€ï¼Œä»–åœ¨æ´›æ‰çŸ¶æ¹–äººé˜Ÿæ•ˆåŠ›äº†20å¹´ï¼Œè·å¾—äº†5æ¬¡NBAæ€»å† å†›ã€‚'
            },
            'metadata': {
                'processor_name': 'direct_processor',
                'confidence': 0.95
            },
            'success': True
        }
        
        query = "è¯·ä»‹ç»ä¸€ä¸‹ç§‘æ¯”"
        
        # æµ‹è¯•è¾“å…¥è·¯ç”±
        unified_input = system.input_router.route_processor_output(mock_processor_output, query)
        print(f"âœ… è¾“å…¥è·¯ç”±å®Œæˆ: {unified_input.processor_type}")
        
        # æµ‹è¯•promptç”Ÿæˆ
        prompt = system.prompt_manager.format_prompt_for_input(unified_input, "ä½ æ˜¯ç¯®çƒä¸“å®¶")
        print(f"âœ… Promptç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(prompt)}")
        
        # æ¨¡æ‹ŸLLMå“åº”ï¼ˆè·³è¿‡å®é™…æ¨ç†ï¼‰
        from app.llm.llm_engine import LLMResponse
        mock_llm_response = LLMResponse(
            content="ç§‘æ¯”Â·å¸ƒè±æ©ç‰¹ï¼ˆKobe Bryantï¼‰æ˜¯NBAå†å²ä¸Šæœ€å…·å½±å“åŠ›çš„çƒå‘˜ä¹‹ä¸€ã€‚ä»–åœ¨æ´›æ‰çŸ¶æ¹–äººé˜Ÿåº¦è¿‡äº†æ•´ä¸ª20å¹´çš„èŒä¸šç”Ÿæ¶¯ï¼Œè·å¾—äº†5æ¬¡NBAæ€»å† å†›ã€18æ¬¡å…¨æ˜æ˜Ÿé€‰æ‹”ï¼Œå¹¶åœ¨2008å¹´è·å¾—å¸¸è§„èµ›MVPã€‚",
            metadata={'model': 'phi-3-mini', 'temperature': 0.7},
            processing_time=2.1,
            token_usage={'input_tokens': 120, 'output_tokens': 80, 'total_tokens': 200}
        )
        
        # æµ‹è¯•å“åº”æ ¼å¼åŒ–
        formatted_response = system.response_formatter.format_response(mock_llm_response, unified_input)
        print(f"âœ… å“åº”æ ¼å¼åŒ–å®Œæˆ")
        print(f"   æ ¼å¼ç±»å‹: {formatted_response.format_type}")
        print(f"   å†…å®¹é•¿åº¦: {len(formatted_response.content)}")
        print(f"   å†…å®¹é¢„è§ˆ: {formatted_response.content[:100]}...")
        
        # æ„å»ºæœ€ç»ˆå“åº”
        final_response = {
            'success': True,
            'content': formatted_response.content,
            'metadata': formatted_response.metadata,
            'processing_info': formatted_response.processing_info
        }
        
        print(f"âœ… ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•å®Œæˆ")
        print(f"   æœ€ç»ˆå“åº”é•¿åº¦: {len(final_response['content'])}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Phase 1 LLMé›†æˆæµ‹è¯•")
    print("=" * 60)
    print()
    
    test_functions = [
        test_config_creation,
        test_input_router,
        test_prompt_templates,
        test_response_formatter,
        test_llm_engine_creation,
        test_llm_factory,
        test_end_to_end_flow
    ]
    
    passed = 0
    total = len(test_functions)
    
    for test_func in test_functions:
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_func.__name__} é€šè¿‡")
            else:
                print(f"âŒ {test_func.__name__} å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_func.__name__} å¼‚å¸¸: {str(e)}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼Phase 1 åŸºç¡€LLMé›†æˆæ¡†æ¶å®ç°æˆåŠŸï¼")
        print("\nğŸ“‹ Phase 1 å®ŒæˆçŠ¶æ€:")
        print("   âœ… LLMé…ç½®ç³»ç»Ÿå®Œæˆ")
        print("   âœ… ç»Ÿä¸€è¾“å…¥æ¥å£å®Œæˆ")
        print("   âœ… Promptæ¨¡æ¿ç³»ç»Ÿå®Œæˆ")
        print("   âœ… å“åº”æ ¼å¼åŒ–å™¨å®Œæˆ")
        print("   âœ… LLMå¼•æ“æ¡†æ¶å®Œæˆ")
        print("   âœ… å·¥å‚æ¨¡å¼å®ç°å®Œæˆ")
        print("   âœ… ç«¯åˆ°ç«¯æµç¨‹éªŒè¯å®Œæˆ")
        print("\nğŸ¯ Ready for Phase 2: å¤šæ¨¡æ€èåˆå®ç°ï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
