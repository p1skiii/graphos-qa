#!/usr/bin/env python3
"""
Phase 2å¤šæ¨¡æ€èåˆæµ‹è¯•è„šæœ¬
æµ‹è¯•GraphEncoderä¸LLMå¼•æ“çš„æ·±åº¦é›†æˆ
éªŒè¯åŸºäºG-Retrieverçš„èåˆæœºåˆ¶
"""
import sys
import os
import time
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_multimodal_fusion_pipeline():
    """æµ‹è¯•å®Œæ•´çš„å¤šæ¨¡æ€èåˆæµæ°´çº¿"""
    print("\nğŸ”§ æµ‹è¯•Phase 2å¤šæ¨¡æ€èåˆæµæ°´çº¿...")
    
    try:
        from app.rag.processors.complex_g_processor import create_complex_g_processor
        from app.llm.factory import LLMFactory
        from app.llm.input_router import create_input_router
        
        # 1. åˆ›å»ºå¢å¼ºæ¨¡å¼ComplexGProcessor
        processor_config = {
            'processor_name': 'phase2_fusion_processor',
            'cache_enabled': False,
            'use_enhanced_mode': True,
            'graph_encoder_enabled': True,
            'graph_encoder_config': {
                'model_config': {
                    'input_dim': 768,
                    'hidden_dim': 256,
                    'output_dim': 128
                }
            },
            'enable_multimodal_fusion': True,
            'fusion_strategy': 'concatenate',
            'min_graph_nodes': 2,
            'fallback_to_traditional': True
        }
        
        processor = create_complex_g_processor(processor_config)
        
        # æ‰‹åŠ¨åˆå§‹åŒ–ä»¥ç¡®ä¿GraphEncoderæ­£ç¡®åˆ›å»º
        if not processor.initialize():
            print("âš ï¸ å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•ä¿®å¤...")
            
        # æ£€æŸ¥å¹¶æ‰‹åŠ¨åˆ›å»ºGraphEncoder
        if not processor.graph_encoder:
            try:
                from app.rag.components.graph_encoder import create_graph_encoder
                encoder_config = processor_config.get('graph_encoder_config', {})
                processor.graph_encoder = create_graph_encoder(encoder_config)
                if processor.graph_encoder and processor.graph_encoder.initialize():
                    print("âœ… æ‰‹åŠ¨åˆ›å»ºGraphEncoderæˆåŠŸ")
                else:
                    print("âš ï¸ GraphEncoderæ‰‹åŠ¨åˆ›å»ºä¹Ÿå¤±è´¥ï¼Œç»§ç»­æµ‹è¯•å…¶ä»–åŠŸèƒ½")
            except Exception as e:
                print(f"âš ï¸ GraphEncoderåˆ›å»ºå¼‚å¸¸: {str(e)}")
        
        print(f"âœ… ComplexGProcessoråˆ›å»ºæˆåŠŸ: {processor.processor_name}")
        print(f"   æ¨¡å¼: {processor.current_mode}")
        print(f"   GraphEncoder: {'å¯ç”¨' if processor.graph_encoder else 'æœªå¯ç”¨'}")
        
        # 2. åˆ›å»ºLLMç³»ç»Ÿ
        llm_system = LLMFactory().create_system('macos_optimized')
        print(f"âœ… LLMç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å‹: {llm_system.config.llm_config.model_config.model_name}")
        print(f"   èåˆç­–ç•¥: {llm_system.config.llm_config.fusion_strategy}")
        
        # 3. åˆ›å»ºè¾“å…¥è·¯ç”±å™¨
        input_router = create_input_router()
        print(f"âœ… è¾“å…¥è·¯ç”±å™¨åˆ›å»ºæˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤šæ¨¡æ€èåˆæµæ°´çº¿æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_enhanced_mode_processing():
    """æµ‹è¯•å¢å¼ºæ¨¡å¼å¤„ç†"""
    print("\nğŸ”§ æµ‹è¯•å¢å¼ºæ¨¡å¼å¤„ç†...")
    
    try:
        from app.rag.processors.complex_g_processor import create_complex_g_processor
        
        # åˆ›å»ºæµ‹è¯•ç”¨å¤„ç†å™¨
        config = {
            'processor_name': 'enhanced_test_processor',
            'use_enhanced_mode': True,
            'graph_encoder_enabled': True,
            'graph_encoder_config': {
                'model_config': {
                    'input_dim': 768,
                    'hidden_dim': 256,
                    'output_dim': 128
                }
            },
            'enable_multimodal_fusion': True,
            'fusion_strategy': 'concatenate'
        }
        
        processor = create_complex_g_processor(config)
        
        # æ¨¡æ‹ŸæŸ¥è¯¢å’Œä¸Šä¸‹æ–‡
        test_query = "ç§‘æ¯”å’Œè©¹å§†æ–¯çš„èŒä¸šæˆå°±å¯¹æ¯”åˆ†æ"
        test_context = {
            'graph_data': {
                'nodes': [
                    {'id': 'kobe', 'label': 'Player', 'name': 'ç§‘æ¯”'},
                    {'id': 'lebron', 'label': 'Player', 'name': 'è©¹å§†æ–¯'},
                    {'id': 'lakers', 'label': 'Team', 'name': 'æ¹–äººé˜Ÿ'},
                    {'id': 'championships', 'label': 'Achievement', 'name': 'æ€»å† å†›'}
                ],
                'edges': [
                    {'source': 'kobe', 'target': 'lakers', 'relation': 'plays_for'},
                    {'source': 'lebron', 'target': 'lakers', 'relation': 'plays_for'},
                    {'source': 'kobe', 'target': 'championships', 'relation': 'won'},
                    {'source': 'lebron', 'target': 'championships', 'relation': 'won'}
                ]
            }
        }
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨å¢å¼ºæ¨¡å¼
        should_enhance = processor._should_use_enhanced_mode(test_query, test_context)
        print(f"âœ… å¢å¼ºæ¨¡å¼å†³ç­–: {should_enhance}")
        
        # æµ‹è¯•å›¾è¯­ä¹‰å¢å¼º
        if processor.graph_encoder:
            # æ¨¡æ‹Ÿå›¾ç¼–ç ç»“æœ
            mock_graph_embedding = {
                'embedding': [0.1, 0.2, -0.3, 0.4, 0.5] * 25 + [0.1, 0.2, 0.3],  # 128ç»´
                'encoding_success': True,
                'encoding_metadata': {'summary': 'ç¯®çƒå›¾è°±ç¼–ç '}
            }
            
            enhanced_info = processor._enhance_graph_semantics(
                mock_graph_embedding, 
                test_context['graph_data'], 
                test_query
            )
            
            print(f"âœ… å›¾è¯­ä¹‰å¢å¼ºå®Œæˆ")
            print(f"   è¯­ä¹‰æ‘˜è¦: {enhanced_info.get('semantic_summary', 'N/A')}")
            
            if 'entity_analysis' in enhanced_info:
                entity_analysis = enhanced_info['entity_analysis']
                print(f"   å®ä½“åˆ†æ: {entity_analysis.get('entity_count', 0)}ä¸ªå®ä½“ï¼Œ{entity_analysis.get('relation_count', 0)}ä¸ªå…³ç³»")
            
            if 'query_relevance' in enhanced_info:
                relevance = enhanced_info['query_relevance']
                print(f"   æŸ¥è¯¢ç›¸å…³æ€§: {relevance.get('relevance_score', 0.0):.2f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¢å¼ºæ¨¡å¼å¤„ç†æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_fusion_strategies():
    """æµ‹è¯•ä¸åŒçš„èåˆç­–ç•¥"""
    print("\nğŸ”§ æµ‹è¯•èåˆç­–ç•¥...")
    
    try:
        from app.llm.factory import LLMFactory
        from app.llm.input_router import UnifiedInput
        from app.rag.components.graph_encoder import MultimodalContext
        
        strategies = ['concatenate', 'weighted', 'attention']
        
        for strategy in strategies:
            print(f"\n   æµ‹è¯•{strategy}èåˆç­–ç•¥:")
            
            # åˆ›å»ºLLMç³»ç»Ÿ
            llm_config = {
                'model_name': 'phi-3-mini',
                'fusion_strategy': strategy,
                'graph_embedding_dim': 128
            }
            
            llm_system = LLMFactory().create_system('macos_optimized', llm_config)
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            multimodal_context = MultimodalContext(
                text_context="ç§‘æ¯”å’Œè©¹å§†æ–¯éƒ½æ˜¯NBAå†å²ä¸Šçš„ä¼Ÿå¤§çƒå‘˜ã€‚ç§‘æ¯”è·å¾—äº†5æ¬¡æ€»å† å†›ï¼Œè©¹å§†æ–¯è·å¾—äº†4æ¬¡æ€»å† å†›ã€‚",
                graph_embedding=[0.1, 0.2, -0.3] * 42 + [0.1, 0.2],  # 128ç»´
                metadata={
                    'graph_summary': 'åŒ…å«2ä¸ªçƒå‘˜å®ä½“å’Œå¤šä¸ªæˆå°±å…³ç³»',
                    'entity_analysis': {'entity_count': 4, 'relation_count': 6},
                    'query_relevance': {'relevance_score': 0.9}
                }
            )
            
            unified_input = UnifiedInput(
                query="æ¯”è¾ƒç§‘æ¯”å’Œè©¹å§†æ–¯çš„èŒä¸šæˆå°±",
                processor_type='complex_g',
                text_context="ç§‘æ¯”å’Œè©¹å§†æ–¯éƒ½æ˜¯NBAå†å²ä¸Šçš„ä¼Ÿå¤§çƒå‘˜ã€‚",
                multimodal_context=multimodal_context,
                graph_embedding=[0.1, 0.2, -0.3] * 42 + [0.1, 0.2]
            )
            
            # æµ‹è¯•å¤šæ¨¡æ€å¤„ç†
            prompt = f"<|user|>\n{unified_input.query}<|end|>\n<|assistant|>\n"
            
            # åˆå§‹åŒ–ç³»ç»Ÿä»¥è·å–å¼•æ“
            if llm_system.initialize():
                processed_input = llm_system.engine._process_multimodal_input(unified_input, prompt)
                
                fusion_metadata = processed_input.get('fusion_metadata', {})
                print(f"      èåˆåº”ç”¨: {fusion_metadata.get('fusion_applied', False)}")
                print(f"      èåˆç­–ç•¥: {fusion_metadata.get('fusion_strategy', 'N/A')}")
                
                if 'extra_inputs' in processed_input and processed_input['extra_inputs']:
                    extra_inputs = processed_input['extra_inputs']
                    print(f"      é¢å¤–è¾“å…¥: {list(extra_inputs.keys())}")
            else:
                print(f"      ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡èåˆæµ‹è¯•")
        
        print("âœ… èåˆç­–ç•¥æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ èåˆç­–ç•¥æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_end_to_end_fusion():
    """æµ‹è¯•ç«¯åˆ°ç«¯èåˆæµç¨‹"""
    print("\nğŸ”§ æµ‹è¯•ç«¯åˆ°ç«¯èåˆæµç¨‹...")
    
    try:
        from app.rag.processors.complex_g_processor import create_complex_g_processor
        from app.llm.factory import LLMFactory
        from app.llm.input_router import create_input_router
        
        # 1. åˆ›å»ºå®Œæ•´ç³»ç»Ÿ
        processor = create_complex_g_processor({
            'processor_name': 'e2e_test_processor',
            'use_enhanced_mode': True,
            'graph_encoder_enabled': True,
            'graph_encoder_config': {
                'model_config': {
                    'input_dim': 768,
                    'hidden_dim': 256,
                    'output_dim': 128
                }
            },
            'enable_multimodal_fusion': True,
            'fusion_strategy': 'concatenate'
        })
        
        llm_system = LLMFactory().create_system('macos_optimized')
        input_router = create_input_router()
        
        # 2. æ¨¡æ‹ŸRAGå¤„ç†å™¨è¾“å‡º
        mock_processor_output = {
            'success': True,
            'mode': 'enhanced',
            'query': 'åˆ†æç§‘æ¯”å’Œè©¹å§†æ–¯çš„ç¯®çƒæˆå°±',
            'traditional_result': {
                'textual_context': {
                    'formatted_text': 'ç§‘æ¯”å¸ƒè±æ©ç‰¹å’Œå‹’å¸ƒæœ—è©¹å§†æ–¯éƒ½æ˜¯NBAå†å²ä¸Šæœ€ä¼Ÿå¤§çš„çƒå‘˜ä¹‹ä¸€ã€‚ç§‘æ¯”åœ¨æ¹–äººé˜Ÿæ•ˆåŠ›20å¹´ï¼Œè·å¾—5æ¬¡æ€»å† å†›ã€‚è©¹å§†æ–¯åœ¨å¤šæ”¯çƒé˜Ÿæ•ˆåŠ›ï¼Œè·å¾—4æ¬¡æ€»å† å†›ï¼Œ4æ¬¡FMVPã€‚'
                }
            },
            'graph_embedding': {
                'embedding': [0.15, -0.22, 0.33] * 42 + [0.11, 0.07],  # 128ç»´
                'encoding_success': True,
                'semantic_summary': 'ç¯®çƒå›¾è°±åŒ…å«2ä¸ªçƒå‘˜å®ä½“ã€2ä¸ªçƒé˜Ÿå®ä½“å’Œ8ä¸ªæˆå°±å…³ç³»',
                'entity_analysis': {'entity_count': 4, 'relation_count': 8},
                'query_relevance': {'relevance_score': 0.95, 'matched_nodes': ['kobe', 'lebron']}
            },
            'multimodal_context': None,  # å°†ç”±è·¯ç”±å™¨åˆ›å»º
            'enhanced_metadata': {
                'fusion_strategy': 'concatenate',
                'llm_ready': True
            }
        }
        
        # 3. è·¯ç”±å™¨å¤„ç†
        unified_input = input_router.route_processor_output(
            mock_processor_output, 
            'åˆ†æç§‘æ¯”å’Œè©¹å§†æ–¯çš„ç¯®çƒæˆå°±'
        )
        
        print(f"âœ… è¾“å…¥è·¯ç”±å®Œæˆ")
        print(f"   å¤„ç†å™¨ç±»å‹: {unified_input.processor_type}")
        print(f"   å¤šæ¨¡æ€æ•°æ®: {'æœ‰' if unified_input.has_multimodal_data() else 'æ— '}")
        print(f"   å›¾åµŒå…¥ç»´åº¦: {len(unified_input.get_graph_embedding()) if unified_input.get_graph_embedding() else 0}")
        
        # 4. LLMå“åº”ç”Ÿæˆï¼ˆæ¨¡æ‹Ÿï¼‰
        if not llm_system.initialize():
            print("âš ï¸ LLMç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
            
        if not llm_system.is_ready:
            print("âš ï¸ LLMæ¨¡å‹æœªåŠ è½½ï¼Œè·³è¿‡å®é™…æ¨ç†ï¼Œä½†éªŒè¯è¾“å…¥å¤„ç†")
            
            # ä»ç„¶æµ‹è¯•è¾“å…¥å¤„ç†
            prompt = llm_system.engine._build_prompt(unified_input)
            processed_input = llm_system.engine._process_multimodal_input(unified_input, prompt)
            
            print(f"âœ… è¾“å…¥å¤„ç†å®Œæˆ")
            fusion_metadata = processed_input.get('fusion_metadata', {})
            print(f"   èåˆåº”ç”¨: {fusion_metadata.get('fusion_applied', False)}")
            
            if fusion_metadata.get('fusion_applied'):
                print(f"   å›¾åµŒå…¥ç»´åº¦: {fusion_metadata.get('graph_embedding_dim', 0)}")
                print(f"   æŠ•å½±ç»´åº¦: {fusion_metadata.get('projected_dim', 0)}")
                print(f"   èåˆç­–ç•¥: {fusion_metadata.get('fusion_strategy', 'N/A')}")
        
        print("âœ… ç«¯åˆ°ç«¯èåˆæµç¨‹æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ ç«¯åˆ°ç«¯èåˆæµç¨‹æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_graph_projector():
    """æµ‹è¯•å›¾æŠ•å½±å™¨"""
    print("\nğŸ”§ æµ‹è¯•å›¾æŠ•å½±å™¨...")
    
    try:
        from app.llm.llm_engine import GraphProjector
        
        # æ£€æŸ¥æ˜¯å¦æœ‰torch
        try:
            import torch
            HAS_TORCH = True
        except ImportError:
            HAS_TORCH = False
            print("âš ï¸ PyTorchæœªå®‰è£…ï¼Œè·³è¿‡å›¾æŠ•å½±å™¨å®é™…æµ‹è¯•")
            return True
        
        if not HAS_TORCH:
            return True
            
        # åˆ›å»ºæŠ•å½±å™¨
        projector = GraphProjector(graph_dim=128, llm_dim=4096, hidden_dim=512)
        projector.eval()
        
        # æµ‹è¯•æŠ•å½±
        test_graph_embedding = torch.randn(1, 128)  # [batch_size=1, graph_dim=128]
        
        with torch.no_grad():
            projected = projector(test_graph_embedding)
        
        print(f"âœ… å›¾æŠ•å½±å™¨æµ‹è¯•æˆåŠŸ")
        print(f"   è¾“å…¥ç»´åº¦: {test_graph_embedding.shape}")
        print(f"   è¾“å‡ºç»´åº¦: {projected.shape}")
        print(f"   æŠ•å½±å™¨å‚æ•°: {sum(p.numel() for p in projector.parameters())}ä¸ª")
        
        return True
        
    except Exception as e:
        print(f"âŒ å›¾æŠ•å½±å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ Phase 2: å¤šæ¨¡æ€èåˆæ·±åº¦é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    test_results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("å›¾æŠ•å½±å™¨æµ‹è¯•", test_graph_projector),
        ("å¤šæ¨¡æ€èåˆæµæ°´çº¿æµ‹è¯•", test_multimodal_fusion_pipeline),
        ("å¢å¼ºæ¨¡å¼å¤„ç†æµ‹è¯•", test_enhanced_mode_processing),
        ("èåˆç­–ç•¥æµ‹è¯•", test_fusion_strategies),
        ("ç«¯åˆ°ç«¯èåˆæµç¨‹æµ‹è¯•", test_end_to_end_fusion)
    ]
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        
        start_time = time.time()
        try:
            result = test_func()
            test_results.append((test_name, result))
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            elapsed = time.time() - start_time
            print(f"\n{status} - {test_name} (è€—æ—¶: {elapsed:.2f}ç§’)")
        except Exception as e:
            test_results.append((test_name, False))
            print(f"\nâŒ å¤±è´¥ - {test_name}: {str(e)}")
    
    # æ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ¯ Phase 2æµ‹è¯•æ€»ç»“:")
    
    passed = sum(1 for _, result in test_results if result)
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ…" if result else "âŒ"
        print(f"   {status} {test_name}")
    
    print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ Phase 2å¤šæ¨¡æ€èåˆå®ç°å®Œæˆï¼")
        print("\nğŸš€ Phase 2å…³é”®æˆæœ:")
        print("   âœ… GraphProjectoræŠ•å½±å™¨å®ç°")
        print("   âœ… å¤šç§èåˆç­–ç•¥æ”¯æŒ(concatenate/weighted/attention)")
        print("   âœ… å¢å¼ºå›¾è¯­ä¹‰ç†è§£")
        print("   âœ… ComplexGProcessoræ·±åº¦é›†æˆ")
        print("   âœ… ç«¯åˆ°ç«¯å¤šæ¨¡æ€æµæ°´çº¿")
        print("\nâ¡ï¸  å¯ä»¥å¼€å§‹Phase 3: å¾®è°ƒä¼˜åŒ–")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤åæ‰èƒ½è¿›å…¥Phase 3")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
