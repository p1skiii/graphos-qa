"""
LLMå¼•æ“æ ¸å¿ƒç»„ä»¶
å®ç°Phi-3-miniæ¨¡å‹çš„åŠ è½½ã€æ¨ç†å’Œå¤šæ¨¡æ€èåˆ
åŸºäºG-Retrieverçš„æŠ•å½±å™¨è®¾è®¡
"""
import os
import time
import logging
from typing import Dict, Any, Optional, List, Union, Tuple
from dataclasses import dataclass
import json

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
    import numpy as np
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False

from .config import LLMConfig, Phi3Config
from .input_router import UnifiedInput
from .prompt_templates import PromptTemplateManager

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    """LLMå“åº”æ•°æ®ç»“æ„"""
    content: str
    metadata: Dict[str, Any]
    processing_time: float
    token_usage: Dict[str, int]
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'content': self.content,
            'metadata': self.metadata,
            'processing_time': self.processing_time,
            'token_usage': self.token_usage
        }

class GraphProjector(nn.Module):
    """å›¾åµŒå…¥æŠ•å½±å™¨ - åŸºäºG-Retrieverè®¾è®¡"""
    
    def __init__(self, graph_dim: int = 128, llm_dim: int = 4096, hidden_dim: int = 512):
        """åˆå§‹åŒ–æŠ•å½±å™¨
        
        Args:
            graph_dim: å›¾åµŒå…¥ç»´åº¦ï¼ˆæ¥è‡ªGraphEncoderï¼‰
            llm_dim: LLMè¯æ±‡è¡¨ç©ºé—´ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super().__init__()
        self.graph_dim = graph_dim
        self.llm_dim = llm_dim
        self.hidden_dim = hidden_dim
        
        # å¤šå±‚æŠ•å½±ç½‘ç»œ
        self.projection = nn.Sequential(
            nn.Linear(graph_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, llm_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.projection:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.zeros_(module.bias)
    
    def forward(self, graph_embedding: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­
        
        Args:
            graph_embedding: å›¾åµŒå…¥å¼ é‡ [batch_size, graph_dim]
            
        Returns:
            æŠ•å½±åçš„å¼ é‡ [batch_size, llm_dim]
        """
        return self.projection(graph_embedding)

class LLMEngine:
    """LLMæ¨ç†å¼•æ“"""
    
    def __init__(self, config: LLMConfig):
        """åˆå§‹åŒ–LLMå¼•æ“"""
        self.config = config
        self.model = None
        self.tokenizer = None
        self.graph_projector = None
        self.prompt_manager = PromptTemplateManager()
        
        # çŠ¶æ€è·Ÿè¸ª
        self.is_loaded = False
        self.device = config.model_config.device
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens_generated': 0,
            'total_processing_time': 0.0,
            'multimodal_requests': 0,
            'text_only_requests': 0
        }
        
        logger.info(f"ğŸ¤– LLMEngineåˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {config.model_config.model_name}")
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        if not HAS_TRANSFORMERS:
            logger.error("âŒ transformersåº“æœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
            return False
        
        try:
            start_time = time.time()
            model_config = self.config.model_config
            
            logger.info(f"ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹: {model_config.model_name}")
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_config.model_name,
                trust_remote_code=getattr(model_config, 'trust_remote_code', True),
                padding_side='left'
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            model_kwargs = {
                'trust_remote_code': getattr(model_config, 'trust_remote_code', True),
                'torch_dtype': getattr(torch, model_config.torch_dtype, torch.float32),
                'device_map': 'auto' if model_config.device == 'cuda' else None,
                'attn_implementation': getattr(model_config, 'attn_implementation', 'eager')
            }
            
            # MacOS MPSæ”¯æŒ
            if model_config.device == 'mps':
                model_kwargs['device_map'] = None
                model_kwargs['torch_dtype'] = torch.float32
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_config.model_name,
                **model_kwargs
            )
            
            # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            if model_config.device in ['cpu', 'mps']:
                self.model = self.model.to(model_config.device)
            
            self.model.eval()
            
            # åˆå§‹åŒ–å›¾æŠ•å½±å™¨
            if self.config.enable_multimodal:
                self._init_graph_projector()
            
            loading_time = time.time() - start_time
            self.is_loaded = True
            
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {loading_time:.2f}ç§’")
            logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.is_loaded = False
            return False
    
    def _init_graph_projector(self):
        """åˆå§‹åŒ–å›¾æŠ•å½±å™¨"""
        try:
            # è·å–LLMçš„éšè—ç»´åº¦
            llm_hidden_dim = self.model.config.hidden_size
            
            self.graph_projector = GraphProjector(
                graph_dim=self.config.graph_embedding_dim,
                llm_dim=llm_hidden_dim,
                hidden_dim=512
            )
            
            self.graph_projector.to(self.device)
            self.graph_projector.eval()
            
            logger.info(f"ğŸ¯ å›¾æŠ•å½±å™¨åˆå§‹åŒ–å®Œæˆ: {self.config.graph_embedding_dim}d â†’ {llm_hidden_dim}d")
            
        except Exception as e:
            logger.error(f"âŒ å›¾æŠ•å½±å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.graph_projector = None
    
    def generate_response(self, unified_input: UnifiedInput) -> LLMResponse:
        """ç”Ÿæˆå“åº”"""
        if not self.is_loaded:
            return self._create_error_response("æ¨¡å‹æœªåŠ è½½", unified_input)
        
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        try:
            # æ›´æ–°ç»Ÿè®¡
            if unified_input.has_multimodal_data():
                self.stats['multimodal_requests'] += 1
            else:
                self.stats['text_only_requests'] += 1
            
            # æ„å»ºprompt
            prompt = self._build_prompt(unified_input)
            
            # å¤„ç†å¤šæ¨¡æ€è¾“å…¥
            processed_input = self._process_multimodal_input(unified_input, prompt)
            
            # ç”Ÿæˆå“åº”
            response_text = self._generate_text(processed_input['text'], processed_input['extra_inputs'])
            
            # åå¤„ç†
            formatted_response = self._post_process_response(response_text, unified_input)
            
            processing_time = time.time() - start_time
            
            # åˆ›å»ºå“åº”å¯¹è±¡
            response = LLMResponse(
                content=formatted_response,
                metadata={
                    'processor_type': unified_input.processor_type,
                    'has_multimodal': unified_input.has_multimodal_data(),
                    'prompt_length': len(prompt),
                    'model_name': self.config.model_config.model_name
                },
                processing_time=processing_time,
                token_usage=self._calculate_token_usage(prompt, response_text)
            )
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['successful_requests'] += 1
            self.stats['total_processing_time'] += processing_time
            self.stats['total_tokens_generated'] += response.token_usage.get('output_tokens', 0)
            
            logger.info(f"âœ… å“åº”ç”ŸæˆæˆåŠŸï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            return response
            
        except Exception as e:
            logger.error(f"âŒ å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
            self.stats['failed_requests'] += 1
            return self._create_error_response(str(e), unified_input)
    
    def _build_prompt(self, unified_input: UnifiedInput) -> str:
        """æ„å»ºprompt"""
        try:
            system_prompt = self.config.system_prompt
            prompt = self.prompt_manager.format_prompt_for_input(unified_input, system_prompt)
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if len(prompt) > self.config.max_input_tokens * 4:  # å¤§æ¦‚ä¼°ç®—
                logger.warning(f"âš ï¸ Promptè¿‡é•¿ï¼Œå¯èƒ½è¢«æˆªæ–­")
            
            return prompt
            
        except Exception as e:
            logger.error(f"âŒ Promptæ„å»ºå¤±è´¥: {str(e)}")
            # ä½¿ç”¨ç®€å•çš„å›é€€prompt
            return f"<|user|>\n{unified_input.query}<|end|>\n<|assistant|>\n"
    
    def _process_multimodal_input(self, unified_input: UnifiedInput, prompt: str) -> Dict[str, Any]:
        """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
        result = {
            'text': prompt,
            'extra_inputs': None
        }
        
        # å¦‚æœæ²¡æœ‰å¤šæ¨¡æ€æ•°æ®æˆ–æŠ•å½±å™¨ï¼Œç›´æ¥è¿”å›æ–‡æœ¬
        if not unified_input.has_multimodal_data() or not self.graph_projector:
            return result
        
        try:
            # è·å–å›¾åµŒå…¥
            graph_embedding = unified_input.get_graph_embedding()
            if not graph_embedding:
                return result
            
            # è½¬æ¢ä¸ºtensor
            graph_tensor = torch.tensor(graph_embedding, dtype=torch.float32).unsqueeze(0)  # [1, graph_dim]
            graph_tensor = graph_tensor.to(self.device)
            
            # æŠ•å½±åˆ°LLMç©ºé—´
            with torch.no_grad():
                projected_embedding = self.graph_projector(graph_tensor)  # [1, llm_dim]
            
            # åŸºäºG-Retrieverçš„èåˆç­–ç•¥
            if self.config.fusion_strategy == "concatenate":
                # åœ¨è¿™é‡Œæˆ‘ä»¬æš‚æ—¶å°†å›¾ä¿¡æ¯ç¼–ç åˆ°æ–‡æœ¬ä¸­
                # å®é™…çš„concatenateèåˆéœ€è¦åœ¨æ¨¡å‹å†…éƒ¨å®ç°
                graph_info = f"\n[å›¾è°±ç‰¹å¾: {len(graph_embedding)}ç»´å‘é‡è¡¨ç¤º]\n"
                result['text'] = prompt.replace('<|assistant|>', graph_info + '<|assistant|>')
                result['extra_inputs'] = {'graph_projection': projected_embedding}
            
            logger.info(f"ğŸ­ å¤šæ¨¡æ€è¾“å…¥å¤„ç†å®Œæˆï¼Œå›¾åµŒå…¥ç»´åº¦: {len(graph_embedding)}")
            
        except Exception as e:
            logger.error(f"âŒ å¤šæ¨¡æ€è¾“å…¥å¤„ç†å¤±è´¥: {str(e)}")
        
        return result
    
    def _generate_text(self, prompt: str, extra_inputs: Optional[Dict[str, Any]] = None) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        try:
            # Tokenizeè¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=self.config.max_input_tokens,
                truncation=True,
                padding=False
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            input_ids = inputs['input_ids'].to(self.device)
            attention_mask = inputs['attention_mask'].to(self.device)
            
            # ç”Ÿæˆé…ç½®
            generation_config = GenerationConfig(
                max_new_tokens=self.config.max_output_tokens,
                temperature=self.config.model_config.temperature,
                top_p=self.config.model_config.top_p,
                top_k=self.config.model_config.top_k,
                do_sample=self.config.model_config.do_sample,
                num_beams=self.config.model_config.num_beams,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1,
                length_penalty=1.0
            )
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    generation_config=generation_config,
                    use_cache=True
                )
            
            # è§£ç å“åº”
            response_ids = outputs[0][input_ids.shape[1]:]  # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            response_text = self.tokenizer.decode(response_ids, skip_special_tokens=True)
            
            return response_text.strip()
            
        except Exception as e:
            logger.error(f"âŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def _post_process_response(self, response_text: str, unified_input: UnifiedInput) -> str:
        """åå¤„ç†å“åº”"""
        try:
            # åŸºæœ¬æ¸…ç†
            response = response_text.strip()
            
            # ç§»é™¤å¯èƒ½çš„é‡å¤æˆ–å¼‚å¸¸å†…å®¹
            if response.count('\n') > 10:  # è¿‡å¤šæ¢è¡Œ
                lines = response.split('\n')
                response = '\n'.join(lines[:10])
            
            # ç¡®ä¿å“åº”ä¸ä¸ºç©º
            if not response:
                response = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•åŸºäºæä¾›çš„ä¿¡æ¯å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
            
            # Markdownæ ¼å¼åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.format_markdown:
                response = self._format_as_markdown(response)
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ å“åº”åå¤„ç†å¤±è´¥: {str(e)}")
            return response_text
    
    def _format_as_markdown(self, text: str) -> str:
        """æ ¼å¼åŒ–ä¸ºMarkdown"""
        # ç®€å•çš„Markdownæ ¼å¼åŒ–
        formatted = text
        
        # æ·»åŠ é€‚å½“çš„é—´è·
        if not formatted.endswith('\n'):
            formatted += '\n'
        
        return formatted
    
    def _calculate_token_usage(self, prompt: str, response: str) -> Dict[str, int]:
        """è®¡ç®—tokenä½¿ç”¨é‡"""
        try:
            prompt_tokens = len(self.tokenizer.encode(prompt))
            response_tokens = len(self.tokenizer.encode(response))
            
            return {
                'input_tokens': prompt_tokens,
                'output_tokens': response_tokens,
                'total_tokens': prompt_tokens + response_tokens
            }
        except:
            return {
                'input_tokens': len(prompt) // 4,  # ç²—ç•¥ä¼°ç®—
                'output_tokens': len(response) // 4,
                'total_tokens': (len(prompt) + len(response)) // 4
            }
    
    def _create_error_response(self, error_message: str, unified_input: UnifiedInput) -> LLMResponse:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return LLMResponse(
            content=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {error_message}",
            metadata={
                'error': True,
                'error_message': error_message,
                'processor_type': unified_input.processor_type
            },
            processing_time=0.0,
            token_usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = max(1, self.stats['total_requests'])
        
        return {
            'model_loaded': self.is_loaded,
            'model_name': self.config.model_config.model_name,
            'device': self.device,
            'multimodal_enabled': self.config.enable_multimodal,
            'graph_projector_enabled': self.graph_projector is not None,
            'requests': {
                'total': self.stats['total_requests'],
                'successful': self.stats['successful_requests'],
                'failed': self.stats['failed_requests'],
                'success_rate': self.stats['successful_requests'] / total_requests
            },
            'performance': {
                'avg_processing_time': self.stats['total_processing_time'] / max(1, self.stats['successful_requests']),
                'total_tokens_generated': self.stats['total_tokens_generated'],
                'avg_tokens_per_request': self.stats['total_tokens_generated'] / max(1, self.stats['successful_requests'])
            },
            'input_distribution': {
                'multimodal_requests': self.stats['multimodal_requests'],
                'text_only_requests': self.stats['text_only_requests'],
                'multimodal_ratio': self.stats['multimodal_requests'] / total_requests
            }
        }
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens_generated': 0,
            'total_processing_time': 0.0,
            'multimodal_requests': 0,
            'text_only_requests': 0
        }

# =============================================================================
# å·¥å‚å‡½æ•°
# =============================================================================

def create_llm_engine(config: Optional[LLMConfig] = None) -> LLMEngine:
    """åˆ›å»ºLLMå¼•æ“"""
    if config is None:
        from .config import get_macos_optimized_config
        config = get_macos_optimized_config()
    
    return LLMEngine(config)

# =============================================================================
# æµ‹è¯•å’Œæ¼”ç¤º
# =============================================================================

if __name__ == "__main__":
    # åˆ›å»ºé…ç½®
    from .config import get_macos_optimized_config
    from .input_router import UnifiedInput
    
    print("ğŸ¤– LLMå¼•æ“æµ‹è¯•")
    
    # åˆ›å»ºå¼•æ“
    config = get_macos_optimized_config()
    engine = create_llm_engine(config)
    
    print(f"å¼•æ“é…ç½®: {config.model_config.model_name}")
    print(f"è®¾å¤‡: {config.model_config.device}")
    print(f"å¤šæ¨¡æ€æ”¯æŒ: {config.enable_multimodal}")
    
    # æ³¨æ„ï¼šå®é™…åŠ è½½æ¨¡å‹éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¿™é‡Œåªåšé…ç½®æµ‹è¯•
    print("âœ… å¼•æ“åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = UnifiedInput(
        query="ç§‘æ¯”æ˜¯è°ï¼Ÿ",
        processor_type="direct",
        text_context="ç§‘æ¯”æ˜¯æ´›æ‰çŸ¶æ¹–äººé˜Ÿçš„ä¼ å¥‡çƒå‘˜ã€‚"
    )
    
    print(f"æµ‹è¯•è¾“å…¥: {test_input}")
    print(f"ç»Ÿè®¡ä¿¡æ¯: {engine.get_stats()}")
