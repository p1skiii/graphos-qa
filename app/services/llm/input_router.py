"""
LLMè¾“å…¥è·¯ç”±å™¨
å®ç°ç»Ÿä¸€è¾“å…¥æ¥å£ï¼Œæ”¯æŒæ¥è‡ªä¸åŒRAGå¤„ç†å™¨çš„è¾“å‡º
åŸºäºG-Retrieverçš„å¤šæ¨¡æ€èåˆæ–¹æ³•
"""
import time
import logging
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass
import json

try:
    import torch
    import numpy as np
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

from app.rag.components.graph_encoder import MultimodalContext

logger = logging.getLogger(__name__)

@dataclass
class LLMInputConfig:
    """LLMè¾“å…¥é…ç½®"""
    processor_type: str  # direct, simple_g, complex_g, comparison, chitchat
    enable_multimodal: bool = True
    max_tokens: int = 3072
    include_metadata: bool = True
    
    # å¤„ç†å™¨ç‰¹å®šé…ç½®
    processor_configs: Dict[str, Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.processor_configs is None:
            self.processor_configs = {}

@dataclass
class UnifiedInput:
    """ç»Ÿä¸€LLMè¾“å…¥æ•°æ®ç»“æ„"""
    # åŸºç¡€ä¿¡æ¯
    query: str
    processor_type: str
    
    # æ–‡æœ¬ä¸Šä¸‹æ–‡
    text_context: Optional[str] = None
    formatted_text: Optional[str] = None
    
    # å¤šæ¨¡æ€ä¿¡æ¯
    multimodal_context: Optional[MultimodalContext] = None
    graph_embedding: Optional[List[float]] = None
    
    # å…ƒæ•°æ®
    metadata: Optional[Dict[str, Any]] = None
    timestamp: float = None
    
    # å¤„ç†å™¨ç‰¹å®šæ•°æ®
    processor_data: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()
        if self.metadata is None:
            self.metadata = {}
    
    def has_multimodal_data(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šæ¨¡æ€æ•°æ®"""
        return (self.multimodal_context is not None or 
                self.graph_embedding is not None)
    
    def get_text_content(self) -> str:
        """è·å–æ–‡æœ¬å†…å®¹"""
        if self.formatted_text:
            return self.formatted_text
        elif self.text_context:
            return self.text_context
        elif self.multimodal_context:
            return self.multimodal_context.text_context
        else:
            return ""
    
    def get_graph_embedding(self) -> Optional[List[float]]:
        """è·å–å›¾åµŒå…¥"""
        if self.graph_embedding:
            return self.graph_embedding
        elif self.multimodal_context and self.multimodal_context.graph_embedding:
            return self.multimodal_context.graph_embedding
        else:
            return None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'query': self.query,
            'processor_type': self.processor_type,
            'text_context': self.text_context,
            'formatted_text': self.formatted_text,
            'has_multimodal': self.has_multimodal_data(),
            'graph_embedding_dim': len(self.get_graph_embedding()) if self.get_graph_embedding() else 0,
            'metadata': self.metadata,
            'timestamp': self.timestamp,
            'processor_data_keys': list(self.processor_data.keys()) if self.processor_data else []
        }
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        multimodal_info = "æœ‰" if self.has_multimodal_data() else "æ— "
        text_len = len(self.get_text_content())
        return f"UnifiedInput(processor={self.processor_type}, text_len={text_len}, multimodal={multimodal_info})"

class InputRouter:
    """LLMè¾“å…¥è·¯ç”±å™¨"""
    
    def __init__(self, config: LLMInputConfig):
        """åˆå§‹åŒ–è¾“å…¥è·¯ç”±å™¨"""
        self.config = config
        self.processor_handlers = {
            'direct': self._handle_direct_processor,
            'simple_g': self._handle_simple_g_processor,
            'complex_g': self._handle_complex_g_processor,
            'comparison': self._handle_comparison_processor,
            'chitchat': self._handle_chitchat_processor
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,
            'by_processor': {},
            'multimodal_count': 0,
            'text_only_count': 0
        }
        
        logger.info(f"ğŸš¦ InputRouteråˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒå¤„ç†å™¨: {list(self.processor_handlers.keys())}")
    
    def route_processor_output(self, processor_output: Dict[str, Any], query: str) -> UnifiedInput:
        """è·¯ç”±å¤„ç†å™¨è¾“å‡ºåˆ°ç»Ÿä¸€è¾“å…¥æ ¼å¼
        
        Args:
            processor_output: RAGå¤„ç†å™¨çš„è¾“å‡º
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            UnifiedInput: ç»Ÿä¸€æ ¼å¼çš„LLMè¾“å…¥
        """
        try:
            # ç¡®å®šå¤„ç†å™¨ç±»å‹
            processor_type = self._determine_processor_type(processor_output)
            
            # é€‰æ‹©å¯¹åº”çš„å¤„ç†å‡½æ•°
            if processor_type in self.processor_handlers:
                handler = self.processor_handlers[processor_type]
                unified_input = handler(processor_output, query)
            else:
                # ä½¿ç”¨é»˜è®¤å¤„ç†å™¨
                logger.warning(f"âš ï¸ æœªè¯†åˆ«çš„å¤„ç†å™¨ç±»å‹: {processor_type}ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†")
                unified_input = self._handle_default_processor(processor_output, query)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self._update_stats(unified_input)
            
            logger.info(f"ğŸ¯ è¾“å…¥è·¯ç”±å®Œæˆ: {unified_input}")
            return unified_input
            
        except Exception as e:
            logger.error(f"âŒ è¾“å…¥è·¯ç”±å¤±è´¥: {str(e)}")
            # åˆ›å»ºé”™è¯¯å›é€€è¾“å…¥
            return self._create_fallback_input(processor_output, query, str(e))
    
    def _determine_processor_type(self, processor_output: Dict[str, Any]) -> str:
        """ç¡®å®šå¤„ç†å™¨ç±»å‹"""
        # æ–¹æ³•1ï¼šä»è¾“å‡ºä¸­ç›´æ¥è·å–
        if 'processor_type' in processor_output:
            return processor_output['processor_type']
        
        # æ–¹æ³•2ï¼šä»å…ƒæ•°æ®ä¸­è·å–
        metadata = processor_output.get('metadata', {})
        if 'processor_name' in metadata:
            processor_name = metadata['processor_name']
            # æ˜ å°„å¤„ç†å™¨åç§°
            if 'direct' in processor_name.lower():
                return 'direct'
            elif 'simple' in processor_name.lower():
                return 'simple_g'
            elif 'complex' in processor_name.lower():
                return 'complex_g'
            elif 'comparison' in processor_name.lower():
                return 'comparison'
            elif 'chitchat' in processor_name.lower():
                return 'chitchat'
        
        # æ–¹æ³•3ï¼šåŸºäºè¾“å‡ºç»“æ„æ¨æ–­
        if 'multimodal_context' in processor_output:
            return 'complex_g'
        elif 'graph' in processor_output and 'textual_context' in processor_output:
            return 'simple_g'
        elif 'comparison_result' in processor_output:
            return 'comparison'
        else:
            return 'direct'
    
    def _handle_direct_processor(self, output: Dict[str, Any], query: str) -> UnifiedInput:
        """å¤„ç†Directå¤„ç†å™¨è¾“å‡º"""
        text_content = ""
        
        # æå–æ–‡æœ¬å†…å®¹
        if 'textual_context' in output:
            if isinstance(output['textual_context'], dict):
                text_content = output['textual_context'].get('formatted_text', '')
            else:
                text_content = str(output['textual_context'])
        
        return UnifiedInput(
            query=query,
            processor_type='direct',
            text_context=text_content,
            formatted_text=text_content,
            metadata={
                'processing_mode': 'direct',
                'has_graph_data': False,
                **output.get('metadata', {})
            },
            processor_data=output
        )
    
    def _handle_simple_g_processor(self, output: Dict[str, Any], query: str) -> UnifiedInput:
        """å¤„ç†Simple Gå¤„ç†å™¨è¾“å‡º"""
        text_content = ""
        
        # æå–æ–‡æœ¬å†…å®¹
        textual_context = output.get('textual_context', {})
        if isinstance(textual_context, dict):
            text_content = textual_context.get('formatted_text', '')
        else:
            text_content = str(textual_context)
        
        return UnifiedInput(
            query=query,
            processor_type='simple_g',
            text_context=text_content,
            formatted_text=text_content,
            metadata={
                'processing_mode': 'simple_graph',
                'has_graph_data': 'graph' in output,
                'graph_nodes': len(output.get('graph', {}).get('nodes', [])),
                **output.get('metadata', {})
            },
            processor_data=output
        )
    
    def _handle_complex_g_processor(self, output: Dict[str, Any], query: str) -> UnifiedInput:
        """å¤„ç†Complex Gå¤„ç†å™¨è¾“å‡º"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¢å¼ºæ¨¡å¼è¾“å‡º
        if 'multimodal_context' in output:
            return self._handle_enhanced_mode_output(output, query)
        else:
            return self._handle_traditional_mode_output(output, query)
    
    def _handle_enhanced_mode_output(self, output: Dict[str, Any], query: str) -> UnifiedInput:
        """å¤„ç†å¢å¼ºæ¨¡å¼è¾“å‡º"""
        multimodal_context = output.get('multimodal_context')
        graph_embedding = None
        
        # æå–å›¾åµŒå…¥
        if 'graph_embedding' in output:
            graph_embed_data = output['graph_embedding']
            if isinstance(graph_embed_data, dict):
                graph_embedding = graph_embed_data.get('embedding')
        
        # æå–æ–‡æœ¬å†…å®¹
        text_content = ""
        if multimodal_context and hasattr(multimodal_context, 'text_context'):
            text_content = multimodal_context.text_context
        elif 'traditional_result' in output:
            traditional = output['traditional_result']
            if 'textual_context' in traditional:
                textual_context = traditional['textual_context']
                if isinstance(textual_context, dict):
                    text_content = textual_context.get('formatted_text', '')
                else:
                    text_content = str(textual_context)
        
        return UnifiedInput(
            query=query,
            processor_type='complex_g',
            text_context=text_content,
            formatted_text=text_content,
            multimodal_context=multimodal_context,
            graph_embedding=graph_embedding,
            metadata={
                'processing_mode': 'enhanced',
                'has_graph_embedding': graph_embedding is not None,
                'multimodal_fusion': True,
                **output.get('metadata', {})
            },
            processor_data=output
        )
    
    def _handle_traditional_mode_output(self, output: Dict[str, Any], query: str) -> UnifiedInput:
        """å¤„ç†ä¼ ç»Ÿæ¨¡å¼è¾“å‡º"""
        text_content = ""
        
        # æå–æ–‡æœ¬å†…å®¹
        textual_context = output.get('textual_context', {})
        if isinstance(textual_context, dict):
            text_content = textual_context.get('formatted_text', '')
        else:
            text_content = str(textual_context)
        
        return UnifiedInput(
            query=query,
            processor_type='complex_g',
            text_context=text_content,
            formatted_text=text_content,
            metadata={
                'processing_mode': 'traditional',
                'has_graph_data': 'graph' in output,
                'multimodal_fusion': False,
                **output.get('metadata', {})
            },
            processor_data=output
        )
    
    def _handle_comparison_processor(self, output: Dict[str, Any], query: str) -> UnifiedInput:
        """å¤„ç†Comparisonå¤„ç†å™¨è¾“å‡º"""
        text_content = ""
        
        # æå–æ¯”è¾ƒç»“æœ
        if 'comparison_result' in output:
            comparison = output['comparison_result']
            if isinstance(comparison, dict):
                text_content = comparison.get('formatted_text', str(comparison))
            else:
                text_content = str(comparison)
        elif 'textual_context' in output:
            textual_context = output['textual_context']
            if isinstance(textual_context, dict):
                text_content = textual_context.get('formatted_text', '')
            else:
                text_content = str(textual_context)
        
        return UnifiedInput(
            query=query,
            processor_type='comparison',
            text_context=text_content,
            formatted_text=text_content,
            metadata={
                'processing_mode': 'comparison',
                'comparison_type': output.get('comparison_type', 'unknown'),
                **output.get('metadata', {})
            },
            processor_data=output
        )
    
    def _handle_chitchat_processor(self, output: Dict[str, Any], query: str) -> UnifiedInput:
        """å¤„ç†Chitchatå¤„ç†å™¨è¾“å‡º"""
        text_content = ""
        
        # æå–é—²èŠå†…å®¹
        if 'response' in output:
            text_content = str(output['response'])
        elif 'textual_context' in output:
            textual_context = output['textual_context']
            if isinstance(textual_context, dict):
                text_content = textual_context.get('formatted_text', '')
            else:
                text_content = str(textual_context)
        
        return UnifiedInput(
            query=query,
            processor_type='chitchat',
            text_context=text_content,
            formatted_text=text_content,
            metadata={
                'processing_mode': 'chitchat',
                'conversational': True,
                **output.get('metadata', {})
            },
            processor_data=output
        )
    
    def _handle_default_processor(self, output: Dict[str, Any], query: str) -> UnifiedInput:
        """é»˜è®¤å¤„ç†å™¨å¤„ç†"""
        # å°è¯•æå–ä»»ä½•å¯ç”¨çš„æ–‡æœ¬å†…å®¹
        text_content = ""
        
        # å°è¯•å¤šç§æ–¹å¼æå–æ–‡æœ¬
        if 'textual_context' in output:
            textual_context = output['textual_context']
            if isinstance(textual_context, dict):
                text_content = textual_context.get('formatted_text', str(textual_context))
            else:
                text_content = str(textual_context)
        elif 'response' in output:
            text_content = str(output['response'])
        elif 'result' in output:
            text_content = str(output['result'])
        else:
            # ä½¿ç”¨æ•´ä¸ªè¾“å‡ºçš„å­—ç¬¦ä¸²è¡¨ç¤º
            text_content = json.dumps(output, ensure_ascii=False, indent=2)
        
        return UnifiedInput(
            query=query,
            processor_type='unknown',
            text_context=text_content,
            formatted_text=text_content,
            metadata={
                'processing_mode': 'default',
                'fallback': True,
                **output.get('metadata', {})
            },
            processor_data=output
        )
    
    def _create_fallback_input(self, output: Dict[str, Any], query: str, error: str) -> UnifiedInput:
        """åˆ›å»ºé”™è¯¯å›é€€è¾“å…¥"""
        return UnifiedInput(
            query=query,
            processor_type='error',
            text_context=f"å¤„ç†é”™è¯¯: {error}",
            formatted_text=f"æ— æ³•å¤„ç†è¾“å…¥æ•°æ®ï¼Œé”™è¯¯ä¿¡æ¯: {error}",
            metadata={
                'processing_mode': 'error',
                'error': error,
                'fallback': True
            },
            processor_data=output
        )
    
    def _update_stats(self, unified_input: UnifiedInput):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['total_processed'] += 1
        
        processor_type = unified_input.processor_type
        if processor_type not in self.stats['by_processor']:
            self.stats['by_processor'][processor_type] = 0
        self.stats['by_processor'][processor_type] += 1
        
        if unified_input.has_multimodal_data():
            self.stats['multimodal_count'] += 1
        else:
            self.stats['text_only_count'] += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_processed': self.stats['total_processed'],
            'processor_distribution': self.stats['by_processor'].copy(),
            'multimodal_ratio': self.stats['multimodal_count'] / max(1, self.stats['total_processed']),
            'text_only_ratio': self.stats['text_only_count'] / max(1, self.stats['total_processed']),
            'supported_processors': list(self.processor_handlers.keys())
        }
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_processed': 0,
            'by_processor': {},
            'multimodal_count': 0,
            'text_only_count': 0
        }

# =============================================================================
# å·¥å‚å‡½æ•°
# =============================================================================

def create_input_router(config: Optional[LLMInputConfig] = None) -> InputRouter:
    """åˆ›å»ºè¾“å…¥è·¯ç”±å™¨"""
    if config is None:
        config = LLMInputConfig(
            processor_type='auto',
            enable_multimodal=True,
            max_tokens=3072,
            include_metadata=True
        )
    
    return InputRouter(config)

# =============================================================================
# æµ‹è¯•å’Œæ¼”ç¤º
# =============================================================================

if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•è·¯ç”±å™¨
    router = create_input_router()
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„å¤„ç†å™¨è¾“å‡º
    test_cases = [
        # Directå¤„ç†å™¨è¾“å‡º
        {
            'processor_output': {
                'textual_context': {'formatted_text': 'ç§‘æ¯”æ˜¯æ´›æ‰çŸ¶æ¹–äººé˜Ÿçš„ä¼ å¥‡çƒå‘˜ã€‚'},
                'metadata': {'processor_name': 'direct_processor'}
            },
            'query': 'ç§‘æ¯”æ˜¯è°ï¼Ÿ'
        },
        
        # Complex Gå¤„ç†å™¨å¢å¼ºæ¨¡å¼è¾“å‡º
        {
            'processor_output': {
                'mode': 'enhanced',
                'multimodal_context': None,  # å®é™…ä½¿ç”¨ä¸­ä¼šæ˜¯MultimodalContextå¯¹è±¡
                'graph_embedding': {'embedding': [0.1, 0.2, 0.3]},
                'metadata': {'processor_name': 'complex_g_processor'}
            },
            'query': 'ç§‘æ¯”å’Œè©¹å§†æ–¯çš„å…³ç³»ï¼Ÿ'
        }
    ]
    
    for i, test_case in enumerate(test_cases):
        print(f"\nğŸ§ª æµ‹è¯•æ¡ˆä¾‹ {i+1}:")
        unified_input = router.route_processor_output(
            test_case['processor_output'], 
            test_case['query']
        )
        print(f"è¾“å…¥ç±»å‹: {unified_input.processor_type}")
        print(f"å¤šæ¨¡æ€æ•°æ®: {'æœ‰' if unified_input.has_multimodal_data() else 'æ— '}")
        print(f"æ–‡æœ¬é•¿åº¦: {len(unified_input.get_text_content())}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š è·¯ç”±å™¨ç»Ÿè®¡: {router.get_stats()}")
