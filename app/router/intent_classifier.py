"""
BERTæ„å›¾åˆ†ç±»å™¨ - äºŒåˆ†ç±»ç®€å•æŸ¥è¯¢å’Œå¤æ‚æŸ¥è¯¢
"""
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, EarlyStoppingCallback
)
from datasets import Dataset
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import json
from pathlib import Path
import logging
from typing import Tuple, Dict, List
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class BERTIntentClassifier:
    """BERTæ„å›¾åˆ†ç±»å™¨"""
    
    def __init__(self, model_name: str = "bert-base-chinese"):
        self.model_name = model_name
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = None
        self.model = None
        
        # æ ‡ç­¾æ˜ å°„
        self.id2label = {0: "simple", 1: "complex"}
        self.label2id = {"simple": 0, "complex": 1}
        
        # è·¯å¾„é…ç½®
        self.model_dir = Path("models")
        self.model_dir.mkdir(exist_ok=True)
        self.results_dir = Path("results/evaluations")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ðŸ’» è®¾å¤‡: {self.device}")
        logger.info(f"ðŸ¤– æ¨¡åž‹: {self.model_name}")
    
    def load_model(self, model_path: str = None):
        """åŠ è½½æ¨¡åž‹"""
        
        if model_path and Path(model_path).exists():
            # åŠ è½½å¾®è°ƒåŽçš„æ¨¡åž‹
            logger.info(f"ðŸ“¥ åŠ è½½å¾®è°ƒæ¨¡åž‹: {model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        else:
            # åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹
            logger.info(f"ðŸ“¥ åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹: {self.model_name}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                num_labels=2,
                id2label=self.id2label,
                label2id=self.label2id
            )
        
        self.model.to(self.device)
        self.model.eval()
        logger.info("âœ… æ¨¡åž‹åŠ è½½å®Œæˆ")
    
    def load_training_data(self, csv_path: str) -> Tuple[Dataset, Dataset]:
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        
        logger.info(f"ðŸ“Š åŠ è½½è®­ç»ƒæ•°æ®: {csv_path}")
        
        # è¯»å–CSVæ•°æ®
        df = pd.read_csv(csv_path)
        
        # æ•°æ®ç»Ÿè®¡
        logger.info(f"ðŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        logger.info(f"   æ€»æ•°æ®é‡: {len(df)}")
        logger.info(f"   SimpleæŸ¥è¯¢: {(df['label'] == 0).sum()}")
        logger.info(f"   ComplexæŸ¥è¯¢: {(df['label'] == 1).sum()}")
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        train_texts, test_texts, train_labels, test_labels = train_test_split(
            df['text'].tolist(),
            df['label'].tolist(),
            test_size=0.2,
            random_state=42,
            stratify=df['label']
        )
        
        # åˆ›å»ºDatasetå¯¹è±¡
        train_dataset = Dataset.from_dict({
            'text': train_texts,
            'labels': train_labels
        })
        
        test_dataset = Dataset.from_dict({
            'text': test_texts,
            'labels': test_labels
        })
        
        logger.info(f"ðŸ“‹ æ•°æ®åˆ’åˆ†:")
        logger.info(f"   è®­ç»ƒé›†: {len(train_dataset)}")
        logger.info(f"   æµ‹è¯•é›†: {len(test_dataset)}")
        
        return train_dataset, test_dataset
    
    def tokenize_function(self, examples):
        """åˆ†è¯å‡½æ•°"""
        return self.tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=128
        )
    
    def compute_metrics(self, eval_pred):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        accuracy = accuracy_score(labels, predictions)
        
        return {"accuracy": accuracy}
    
    def train(self, csv_path: str, epochs: int = 3, batch_size: int = 16, learning_rate: float = 2e-5):
        """è®­ç»ƒæ¨¡åž‹"""
        
        logger.info("ðŸš€ å¼€å§‹BERTå¾®è°ƒè®­ç»ƒ...")
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹
        self.load_model()
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        train_dataset, test_dataset = self.load_training_data(csv_path)
        
        # æ•°æ®é¢„å¤„ç†
        train_dataset = train_dataset.map(self.tokenize_function, batched=True)
        test_dataset = test_dataset.map(self.tokenize_function, batched=True)
        
        # è®­ç»ƒå‚æ•°
        output_dir = self.model_dir / "bert_intent_classifier"
        
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            warmup_steps=50,
            weight_decay=0.01,
            logging_dir=str(self.results_dir / "logs"),
            logging_steps=10,
            eval_strategy="epoch",
            save_strategy="no",  # ç¦ç”¨è‡ªåŠ¨ä¿å­˜
            load_best_model_at_end=False,  # ç¦ç”¨æœ€ä½³æ¨¡åž‹åŠ è½½
            save_total_limit=2,
            seed=42
        )
        
        # è®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=test_dataset,
            compute_metrics=self.compute_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
        )
        
        # å¼€å§‹è®­ç»ƒ
        logger.info("â³ å¼€å§‹è®­ç»ƒ...")
        trainer.train()
        
        # æ‰‹åŠ¨ä¿å­˜æœ€ç»ˆæ¨¡åž‹
        final_model_path = self.model_dir / "bert_intent_classifier_final"
        final_model_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ¨¡åž‹çŠ¶æ€
        self.model.save_pretrained(str(final_model_path))
        self.tokenizer.save_pretrained(str(final_model_path))
        
        logger.info(f"ðŸ’¾ æ¨¡åž‹å·²ä¿å­˜åˆ°: {final_model_path}")
        
        # è¯„ä¼°æ¨¡åž‹
        eval_results = self.evaluate_model(trainer, test_dataset)
        
        return trainer, eval_results
    
    def evaluate_model(self, trainer, test_dataset) -> Dict:
        """è¯¦ç»†è¯„ä¼°æ¨¡åž‹"""
        
        logger.info("ðŸ“Š å¼€å§‹è¯¦ç»†è¯„ä¼°...")
        
        # èŽ·å–é¢„æµ‹ç»“æžœ
        predictions = trainer.predict(test_dataset)
        y_pred = np.argmax(predictions.predictions, axis=1)
        y_true = predictions.label_ids
        y_proba = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred)
        
        # åˆ†ç±»æŠ¥å‘Š
        report = classification_report(
            y_true, y_pred,
            target_names=["Simple", "Complex"],
            output_dict=True
        )
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        
        # è¯¦ç»†ç»“æžœ
        evaluation_results = {
            "accuracy": accuracy,
            "classification_report": report,
            "confusion_matrix": cm.tolist(),
            "predictions": {
                "y_true": y_true.tolist(),
                "y_pred": y_pred.tolist(),
                "y_proba": y_proba.tolist()
            }
        }
        
        # æ‰“å°ç»“æžœ
        logger.info("ðŸ“ˆ è¯„ä¼°ç»“æžœ:")
        logger.info(f"   å‡†ç¡®çŽ‡: {accuracy:.4f}")
        logger.info("ðŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_true, y_pred, target_names=["Simple", "Complex"]))
        logger.info("ðŸ” æ··æ·†çŸ©é˜µ:")
        print("       Simple  Complex")
        print(f"Simple   {cm[0][0]:4d}    {cm[0][1]:4d}")
        print(f"Complex  {cm[1][0]:4d}    {cm[1][1]:4d}")
        
        # ä¿å­˜è¯„ä¼°ç»“æžœ
        results_path = self.results_dir / "evaluation_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ðŸ’¾ è¯„ä¼°ç»“æžœå·²ä¿å­˜: {results_path}")
        
        return evaluation_results
    
    def classify(self, text: str) -> Tuple[str, float]:
        """åˆ†ç±»å•ä¸ªæ–‡æœ¬"""
        
        if self.model is None:
            raise ValueError("æ¨¡åž‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()æˆ–train()")
        
        # é¢„å¤„ç†
        inputs = self.tokenizer(
            text,
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # æŽ¨ç†
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
        
        # è§£æžç»“æžœ
        predicted_id = torch.argmax(probs, dim=-1).item()
        confidence = probs[0][predicted_id].item()
        predicted_label = self.id2label[predicted_id]
        
        return predicted_label, confidence
    
    def batch_classify(self, texts: List[str]) -> List[Tuple[str, float]]:
        """æ‰¹é‡åˆ†ç±»"""
        
        results = []
        for text in texts:
            label, confidence = self.classify(text)
            results.append((label, confidence))
        
        return results
    
    def test_examples(self):
        """æµ‹è¯•ç¤ºä¾‹"""
        
        test_cases = [
            "å§šæ˜Žå¤šå°‘å²ï¼Ÿ",                    # Simple
            "å§šæ˜Žå’Œç§‘æ¯”ä»€ä¹ˆå…³ç³»ï¼Ÿ",              # Complex
            "ç§‘æ¯”èº«é«˜ï¼Ÿ",                      # Simple
            "æ¯”è¾ƒè©¹å§†æ–¯å’Œç§‘æ¯”è°æ›´å¼ºï¼Ÿ",           # Complex
            "æ¹–äººé˜Ÿåœ¨å“ªä¸ªåŸŽå¸‚ï¼Ÿ",                # Simple
            "åˆ†æžæ¹–äººé˜Ÿçš„åŽ†å²æˆå°±",               # Complex
        ]
        
        logger.info("ðŸ§ª æµ‹è¯•ç¤ºä¾‹:")
        for text in test_cases:
            label, confidence = self.classify(text)
            logger.info(f"   '{text}' -> {label} ({confidence:.3f})")

# å…¨å±€å®žä¾‹
intent_classifier = BERTIntentClassifier()

if __name__ == "__main__":
    # è®­ç»ƒç¤ºä¾‹
    csv_path = "data/training/final_training_dataset.csv"
    trainer, results = intent_classifier.train(csv_path, epochs=5)
    intent_classifier.test_examples()
