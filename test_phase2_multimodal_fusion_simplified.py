#!/usr/bin/env python3
"""
Phase 2å¤šæ¨¡æ€èåˆæµ‹è¯•è„šæœ¬ (ç®€åŒ–ç‰ˆ)
æµ‹è¯•GraphEncoderä¸LLMå¼•æ“çš„æ·±åº¦é›†æˆ
éªŒè¯åŸºäºG-Retrieverçš„èåˆæœºåˆ¶
"""
import sys
import os
import time
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_complex_g_processor_enhanced():
    """æµ‹è¯•ComplexGProcessorå¢å¼ºæ¨¡å¼"""
    print("\nğŸ”§ æµ‹è¯•ComplexGProcessorå¢å¼ºæ¨¡å¼...")
    
    try:
        from app.rag.processors.complex_g_processor import create_complex_g_processor
        
        # åˆ›å»ºå¢å¼ºæ¨¡å¼é…ç½®
        config = {
            'processor_name': 'phase2_enhanced_processor',
            'cache_enabled': False,
            'use_enhanced_mode': True,
            'graph_encoder_enabled': True,
            'graph_encoder_config': {
                'model_config': {
                    'input_dim': 768,
                    'hidden_dim': 256,
                    'output_dim': 128
                }
            },
            'enable_multimodal_fusion': True,
            'fusion_strategy': 'concatenate',
            'min_graph_nodes': 2,
            'fallback_to_traditional': True
        }
        
        processor = create_complex_g_processor(config)
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        init_success = processor.initialize()
        print(f"âœ… å¤„ç†å™¨åˆå§‹åŒ–: {'æˆåŠŸ' if init_success else 'å¤±è´¥'}")
        
        # æ£€æŸ¥GraphEncoder
        if processor.graph_encoder:
            print("âœ… GraphEncoderå·²å¯ç”¨")
            
            # æµ‹è¯•GraphEncoderåŠŸèƒ½
            test_result = processor.test_graph_encoder()
            if test_result['success']:
                print("âœ… GraphEncoderæµ‹è¯•é€šè¿‡")
            else:
                print(f"âš ï¸ GraphEncoderæµ‹è¯•å¤±è´¥: {test_result.get('error', 'Unknown error')}")
        else:
            print("âš ï¸ GraphEncoderæœªå¯ç”¨")
        
        print(f"   å¤„ç†å™¨åç§°: {processor.processor_name}")
        print(f"   å½“å‰æ¨¡å¼: {processor.current_mode}")
        print(f"   å¤šæ¨¡æ€èåˆ: {processor.complex_config.enable_multimodal_fusion}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ComplexGProcessorå¢å¼ºæ¨¡å¼æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_graph_semantics_enhancement():
    """æµ‹è¯•å›¾è¯­ä¹‰å¢å¼ºåŠŸèƒ½"""
    print("\nğŸ”§ æµ‹è¯•å›¾è¯­ä¹‰å¢å¼º...")
    
    try:
        from app.rag.processors.complex_g_processor import create_complex_g_processor
        
        # åˆ›å»ºå¤„ç†å™¨
        config = {
            'processor_name': 'semantic_test_processor',
            'use_enhanced_mode': True,
            'graph_encoder_enabled': True,
            'graph_encoder_config': {
                'model_config': {
                    'input_dim': 768,
                    'hidden_dim': 256,
                    'output_dim': 128
                }
            },
            'enable_multimodal_fusion': True,
            'fusion_strategy': 'concatenate'
        }
        
        processor = create_complex_g_processor(config)
        
        # åˆ›å»ºæµ‹è¯•å›¾æ•°æ®
        test_graph_data = {
            'nodes': [
                {'id': 'kobe', 'label': 'Player', 'name': 'ç§‘æ¯”'},
                {'id': 'lebron', 'label': 'Player', 'name': 'è©¹å§†æ–¯'},
                {'id': 'lakers', 'label': 'Team', 'name': 'æ¹–äººé˜Ÿ'},
                {'id': 'championship', 'label': 'Achievement', 'name': 'æ€»å† å†›'}
            ],
            'edges': [
                {'source': 'kobe', 'target': 'lakers', 'relation': 'plays_for'},
                {'source': 'lebron', 'target': 'lakers', 'relation': 'plays_for'},
                {'source': 'kobe', 'target': 'championship', 'relation': 'won'},
                {'source': 'lebron', 'target': 'championship', 'relation': 'won'}
            ]
        }
        
        test_query = "ç§‘æ¯”å’Œè©¹å§†æ–¯çš„èŒä¸šæˆå°±å¯¹æ¯”"
        
        # æµ‹è¯•å›¾æ‘˜è¦ç”Ÿæˆ
        summary = processor._generate_graph_summary(test_graph_data, test_query)
        print(f"âœ… å›¾æ‘˜è¦ç”Ÿæˆ: {summary}")
        
        # æµ‹è¯•å®ä½“å…³ç³»åˆ†æ
        entity_analysis = processor._analyze_entities_and_relations(test_graph_data)
        print(f"âœ… å®ä½“åˆ†æ: {entity_analysis.get('entity_count', 0)}ä¸ªå®ä½“ï¼Œ{entity_analysis.get('relation_count', 0)}ä¸ªå…³ç³»")
        print(f"   å®ä½“ç±»å‹: {entity_analysis.get('entity_types', [])}")
        print(f"   å…³ç³»ç±»å‹: {entity_analysis.get('relation_types', [])}")
        
        # æµ‹è¯•æŸ¥è¯¢ç›¸å…³æ€§åˆ†æ
        relevance_info = processor._analyze_query_relevance(test_graph_data, test_query)
        print(f"âœ… æŸ¥è¯¢ç›¸å…³æ€§: {relevance_info.get('relevance_score', 0.0):.2f}")
        print(f"   åŒ¹é…èŠ‚ç‚¹: {relevance_info.get('matched_nodes', [])}")
        
        # æµ‹è¯•è¯­ä¹‰å¢å¼º
        mock_graph_embedding = {
            'embedding': [0.1, 0.2, -0.3] * 42 + [0.1, 0.2],  # 128ç»´æ¨¡æ‹ŸåµŒå…¥
            'encoding_success': True
        }
        
        enhanced_info = processor._enhance_graph_semantics(
            mock_graph_embedding, 
            test_graph_data, 
            test_query
        )
        
        print(f"âœ… è¯­ä¹‰å¢å¼ºå®Œæˆ")
        if 'semantic_summary' in enhanced_info:
            print(f"   è¯­ä¹‰æ‘˜è¦: {enhanced_info['semantic_summary']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å›¾è¯­ä¹‰å¢å¼ºæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_multimodal_context_creation():
    """æµ‹è¯•å¤šæ¨¡æ€ä¸Šä¸‹æ–‡åˆ›å»º"""
    print("\nğŸ”§ æµ‹è¯•å¤šæ¨¡æ€ä¸Šä¸‹æ–‡åˆ›å»º...")
    
    try:
        from app.rag.processors.complex_g_processor import create_complex_g_processor
        from app.rag.components.graph_encoder import MultimodalContext
        
        # åˆ›å»ºå¤„ç†å™¨
        processor = create_complex_g_processor({
            'processor_name': 'multimodal_test_processor',
            'use_enhanced_mode': True,
            'enable_multimodal_fusion': True,
            'fusion_strategy': 'concatenate'
        })
        
        # æµ‹è¯•æ•°æ®
        textual_context = {
            'formatted_text': 'ç§‘æ¯”å’Œè©¹å§†æ–¯éƒ½æ˜¯NBAå†å²ä¸Šçš„ä¼Ÿå¤§çƒå‘˜ã€‚ç§‘æ¯”è·å¾—äº†5æ¬¡æ€»å† å†›ï¼Œè©¹å§†æ–¯è·å¾—äº†4æ¬¡æ€»å† å†›ã€‚'
        }
        
        enhanced_graph_embedding = {
            'embedding': [0.15, -0.22, 0.33] * 42 + [0.11, 0.07],  # 128ç»´
            'encoding_success': True,
            'semantic_summary': 'ç¯®çƒå›¾è°±åŒ…å«2ä¸ªçƒå‘˜å®ä½“å’Œå¤šä¸ªæˆå°±å…³ç³»',
            'entity_analysis': {'entity_count': 4, 'relation_count': 6},
            'query_relevance': {'relevance_score': 0.9, 'matched_nodes': ['kobe', 'lebron']}
        }
        
        query = "æ¯”è¾ƒç§‘æ¯”å’Œè©¹å§†æ–¯çš„èŒä¸šæˆå°±"
        
        # åˆ›å»ºå¤šæ¨¡æ€ä¸Šä¸‹æ–‡
        multimodal_context = processor._create_multimodal_context(
            textual_context,
            enhanced_graph_embedding,
            query
        )
        
        print(f"âœ… å¤šæ¨¡æ€ä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ")
        print(f"   æ–‡æœ¬é•¿åº¦: {len(multimodal_context.text_context)}")
        print(f"   å›¾åµŒå…¥ç»´åº¦: {len(multimodal_context.graph_embedding) if multimodal_context.graph_embedding else 0}")
        print(f"   å…ƒæ•°æ®é”®: {list(multimodal_context.metadata.keys())}")
        
        # æ£€æŸ¥å¢å¼ºçš„å…ƒæ•°æ®
        metadata = multimodal_context.metadata
        if 'graph_summary' in metadata:
            print(f"   å›¾æ‘˜è¦: {metadata['graph_summary']}")
        if 'entity_analysis' in metadata:
            print(f"   å®ä½“åˆ†æ: {metadata['entity_analysis']}")
        if 'query_relevance' in metadata:
            print(f"   æŸ¥è¯¢ç›¸å…³æ€§: {metadata['query_relevance']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¤šæ¨¡æ€ä¸Šä¸‹æ–‡åˆ›å»ºæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_llm_system_creation():
    """æµ‹è¯•LLMç³»ç»Ÿåˆ›å»º"""
    print("\nğŸ”§ æµ‹è¯•LLMç³»ç»Ÿåˆ›å»º...")
    
    try:
        from app.llm.factory import LLMFactory
        
        # åˆ›å»ºLLMç³»ç»Ÿ
        factory = LLMFactory()
        llm_system = factory.create_system('macos_optimized')
        
        print(f"âœ… LLMç³»ç»Ÿåˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å‹: {llm_system.config.llm_config.model_config.model_name}")
        print(f"   èåˆç­–ç•¥: {llm_system.config.llm_config.fusion_strategy}")
        print(f"   å¤šæ¨¡æ€æ”¯æŒ: {llm_system.config.input_config.enable_multimodal}")
        
        # æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–
        init_success = llm_system.initialize()
        print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–: {'æˆåŠŸ' if init_success else 'å¤±è´¥'}")
        
        if init_success:
            status = llm_system.get_system_status()
            components = status['components']
            print(f"   ç»„ä»¶çŠ¶æ€:")
            print(f"     å¼•æ“: {'âœ…' if components['engine'] else 'âŒ'}")
            print(f"     è·¯ç”±å™¨: {'âœ…' if components['input_router'] else 'âŒ'}")
            print(f"     æ¨¡æ¿ç®¡ç†å™¨: {'âœ…' if components['prompt_manager'] else 'âŒ'}")
            print(f"     å“åº”æ ¼å¼åŒ–å™¨: {'âœ…' if components['response_formatter'] else 'âŒ'}")
        
        return True
        
    except Exception as e:
        print(f"âŒ LLMç³»ç»Ÿåˆ›å»ºæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_fusion_text_building():
    """æµ‹è¯•èåˆæ–‡æœ¬æ„å»º"""
    print("\nğŸ”§ æµ‹è¯•èåˆæ–‡æœ¬æ„å»º...")
    
    try:
        from app.llm.factory import LLMFactory
        from app.llm.input_router import UnifiedInput
        from app.rag.components.graph_encoder import MultimodalContext
        
        # åˆ›å»ºLLMç³»ç»Ÿ
        factory = LLMFactory()
        llm_system = factory.create_system('macos_optimized')
        
        if not llm_system.initialize():
            print("âš ï¸ LLMç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
            return True
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        multimodal_context = MultimodalContext(
            text_context="ç§‘æ¯”å’Œè©¹å§†æ–¯éƒ½æ˜¯NBAå†å²ä¸Šçš„ä¼Ÿå¤§çƒå‘˜ã€‚ç§‘æ¯”è·å¾—äº†5æ¬¡æ€»å† å†›ï¼Œè©¹å§†æ–¯è·å¾—äº†4æ¬¡æ€»å† å†›ã€‚",
            graph_embedding=[0.1, 0.2, -0.3] * 42 + [0.1, 0.2],  # 128ç»´
            metadata={
                'graph_summary': 'ç¯®çƒå›¾è°±åŒ…å«2ä¸ªçƒå‘˜å®ä½“å’Œå¤šä¸ªæˆå°±å…³ç³»',
                'entity_analysis': {'entity_count': 4, 'relation_count': 6},
                'query_relevance': {'relevance_score': 0.9}
            }
        )
        
        unified_input = UnifiedInput(
            query="æ¯”è¾ƒç§‘æ¯”å’Œè©¹å§†æ–¯çš„èŒä¸šæˆå°±",
            processor_type='complex_g',
            text_context="ç§‘æ¯”å’Œè©¹å§†æ–¯éƒ½æ˜¯NBAå†å²ä¸Šçš„ä¼Ÿå¤§çƒå‘˜ã€‚",
            multimodal_context=multimodal_context,
            graph_embedding=[0.1, 0.2, -0.3] * 42 + [0.1, 0.2]
        )
        
        # æµ‹è¯•ä¸åŒèåˆç­–ç•¥
        strategies = ['concatenate', 'weighted', 'attention']
        
        for strategy in strategies:
            print(f"\n   æµ‹è¯•{strategy}èåˆç­–ç•¥:")
            
            # æ›´æ–°ç³»ç»Ÿé…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…ä¸­éœ€è¦é‡æ–°åˆ›å»ºç³»ç»Ÿï¼‰
            llm_system.config.llm_config.fusion_strategy = strategy
            
            # æ„å»ºprompt
            if llm_system.engine:
                prompt = llm_system.engine._build_prompt(unified_input)
                processed_input = llm_system.engine._process_multimodal_input(unified_input, prompt)
                
                fusion_metadata = processed_input.get('fusion_metadata', {})
                print(f"      èåˆåº”ç”¨: {fusion_metadata.get('fusion_applied', False)}")
                print(f"      èåˆç­–ç•¥: {fusion_metadata.get('fusion_strategy', 'N/A')}")
                
                if fusion_metadata.get('fusion_applied'):
                    print(f"      å›¾åµŒå…¥ç»´åº¦: {fusion_metadata.get('graph_embedding_dim', 0)}")
                    
                    # æ£€æŸ¥èåˆæ–‡æœ¬
                    fusion_text = processed_input.get('text', '')
                    if '[å›¾è°±åˆ†æ]' in fusion_text:
                        print(f"      âœ… å›¾è°±ä¿¡æ¯å·²èå…¥æ–‡æœ¬")
                    else:
                        print(f"      âš ï¸ å›¾è°±ä¿¡æ¯æœªæ­£ç¡®èå…¥")
            else:
                print(f"      âš ï¸ LLMå¼•æ“æœªåˆ›å»º")
        
        print("âœ… èåˆæ–‡æœ¬æ„å»ºæµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ èåˆæ–‡æœ¬æ„å»ºæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_end_to_end_integration():
    """æµ‹è¯•ç«¯åˆ°ç«¯é›†æˆ"""
    print("\nğŸ”§ æµ‹è¯•ç«¯åˆ°ç«¯é›†æˆ...")
    
    try:
        from app.rag.processors.complex_g_processor import create_complex_g_processor
        from app.llm.factory import LLMFactory
        from app.llm.input_router import create_input_router
        
        # 1. åˆ›å»ºæ‰€æœ‰ç»„ä»¶
        processor = create_complex_g_processor({
            'processor_name': 'e2e_integration_processor',
            'use_enhanced_mode': True,
            'graph_encoder_enabled': True,
            'graph_encoder_config': {
                'model_config': {
                    'input_dim': 768,
                    'hidden_dim': 256,
                    'output_dim': 128
                }
            },
            'enable_multimodal_fusion': True,
            'fusion_strategy': 'concatenate'
        })
        
        factory = LLMFactory()
        llm_system = factory.create_system('macos_optimized')
        input_router = create_input_router()
        
        print("âœ… æ‰€æœ‰ç»„ä»¶åˆ›å»ºæˆåŠŸ")
        
        # 2. æ¨¡æ‹ŸRAGå¤„ç†å™¨è¾“å‡º
        mock_output = {
            'success': True,
            'mode': 'enhanced',
            'query': 'åˆ†æç§‘æ¯”å’Œè©¹å§†æ–¯çš„ç¯®çƒæˆå°±',
            'traditional_result': {
                'textual_context': {
                    'formatted_text': 'ç§‘æ¯”å¸ƒè±æ©ç‰¹å’Œå‹’å¸ƒæœ—è©¹å§†æ–¯éƒ½æ˜¯NBAå†å²ä¸Šæœ€ä¼Ÿå¤§çš„çƒå‘˜ä¹‹ä¸€ã€‚ç§‘æ¯”åœ¨æ¹–äººé˜Ÿæ•ˆåŠ›20å¹´ï¼Œè·å¾—5æ¬¡æ€»å† å†›ã€‚è©¹å§†æ–¯åœ¨å¤šæ”¯çƒé˜Ÿæ•ˆåŠ›ï¼Œè·å¾—4æ¬¡æ€»å† å†›ï¼Œ4æ¬¡FMVPã€‚'
                }
            },
            'graph_embedding': {
                'embedding': [0.15, -0.22, 0.33] * 42 + [0.11, 0.07],  # 128ç»´
                'encoding_success': True,
                'semantic_summary': 'ç¯®çƒå›¾è°±åŒ…å«2ä¸ªçƒå‘˜å®ä½“ã€2ä¸ªçƒé˜Ÿå®ä½“å’Œ8ä¸ªæˆå°±å…³ç³»',
                'entity_analysis': {'entity_count': 4, 'relation_count': 8},
                'query_relevance': {'relevance_score': 0.95, 'matched_nodes': ['kobe', 'lebron']}
            },
            'multimodal_context': None,  # å°†ç”±è·¯ç”±å™¨åˆ›å»º
            'enhanced_metadata': {
                'fusion_strategy': 'concatenate',
                'llm_ready': True
            }
        }
        
        # 3. è·¯ç”±å¤„ç†
        unified_input = input_router.route_processor_output(
            mock_output, 
            'åˆ†æç§‘æ¯”å’Œè©¹å§†æ–¯çš„ç¯®çƒæˆå°±'
        )
        
        print(f"âœ… è¾“å…¥è·¯ç”±å®Œæˆ")
        print(f"   å¤„ç†å™¨ç±»å‹: {unified_input.processor_type}")
        print(f"   å¤šæ¨¡æ€æ•°æ®: {'æœ‰' if unified_input.has_multimodal_data() else 'æ— '}")
        
        graph_embedding = unified_input.get_graph_embedding()
        print(f"   å›¾åµŒå…¥ç»´åº¦: {len(graph_embedding) if graph_embedding else 0}")
        
        # 4. LLMç³»ç»Ÿå¤„ç†
        if llm_system.initialize():
            print("âœ… LLMç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            
            if llm_system.engine:
                # æµ‹è¯•è¾“å…¥å¤„ç†
                prompt = llm_system.engine._build_prompt(unified_input)
                processed_input = llm_system.engine._process_multimodal_input(unified_input, prompt)
                
                print(f"âœ… å¤šæ¨¡æ€è¾“å…¥å¤„ç†å®Œæˆ")
                
                fusion_metadata = processed_input.get('fusion_metadata', {})
                if fusion_metadata.get('fusion_applied'):
                    print(f"   èåˆç­–ç•¥: {fusion_metadata.get('fusion_strategy', 'N/A')}")
                    print(f"   å›¾åµŒå…¥ç»´åº¦: {fusion_metadata.get('graph_embedding_dim', 0)}")
                    print(f"   æŠ•å½±ç»´åº¦: {fusion_metadata.get('projected_dim', 0)}")
                else:
                    print(f"   âš ï¸ èåˆæœªåº”ç”¨: {fusion_metadata}")
        else:
            print("âš ï¸ LLMç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        
        print("âœ… ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ Phase 2: å¤šæ¨¡æ€èåˆæ·±åº¦é›†æˆæµ‹è¯• (ç®€åŒ–ç‰ˆ)")
    print("=" * 70)
    
    test_results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("ComplexGProcessorå¢å¼ºæ¨¡å¼æµ‹è¯•", test_complex_g_processor_enhanced),
        ("å›¾è¯­ä¹‰å¢å¼ºæµ‹è¯•", test_graph_semantics_enhancement),
        ("å¤šæ¨¡æ€ä¸Šä¸‹æ–‡åˆ›å»ºæµ‹è¯•", test_multimodal_context_creation),
        ("LLMç³»ç»Ÿåˆ›å»ºæµ‹è¯•", test_llm_system_creation),
        ("èåˆæ–‡æœ¬æ„å»ºæµ‹è¯•", test_fusion_text_building),
        ("ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•", test_end_to_end_integration)
    ]
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        
        start_time = time.time()
        try:
            result = test_func()
            test_results.append((test_name, result))
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            elapsed = time.time() - start_time
            print(f"\n{status} - {test_name} (è€—æ—¶: {elapsed:.2f}ç§’)")
        except Exception as e:
            test_results.append((test_name, False))
            print(f"\nâŒ å¤±è´¥ - {test_name}: {str(e)}")
    
    # æ€»ç»“
    print(f"\n{'='*70}")
    print("ğŸ¯ Phase 2æµ‹è¯•æ€»ç»“:")
    
    passed = sum(1 for _, result in test_results if result)
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ…" if result else "âŒ"
        print(f"   {status} {test_name}")
    
    print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ Phase 2å¤šæ¨¡æ€èåˆå®ç°å®Œæˆï¼")
        print("\nğŸš€ Phase 2å…³é”®æˆæœ:")
        print("   âœ… å¢å¼ºå›¾è¯­ä¹‰ç†è§£(æ‘˜è¦ç”Ÿæˆã€å®ä½“åˆ†æã€ç›¸å…³æ€§åˆ†æ)")
        print("   âœ… å¤šæ¨¡æ€ä¸Šä¸‹æ–‡åˆ›å»ºä¸å…ƒæ•°æ®å¢å¼º")
        print("   âœ… å¤šç§èåˆç­–ç•¥æ”¯æŒ(concatenate/weighted/attention)")
        print("   âœ… ComplexGProcessoræ·±åº¦é›†æˆå¢å¼º")
        print("   âœ… LLMå¤šæ¨¡æ€è¾“å…¥å¤„ç†æµæ°´çº¿")
        print("   âœ… ç«¯åˆ°ç«¯å¤šæ¨¡æ€èåˆéªŒè¯")
        print("\nâ¡ï¸  Phase 2å®Œæˆï¼Œå¯ä»¥å¼€å§‹Phase 3: å¾®è°ƒä¼˜åŒ–")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤åæ‰èƒ½è¿›å…¥Phase 3")
        failed_tests = [name for name, result in test_results if not result]
        print(f"   å¤±è´¥çš„æµ‹è¯•: {', '.join(failed_tests)}")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
